Transformer-in-Vision[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

A paper list of some recent Transformer-based CV works. If you find some ignored papers, please open issues or pull requests.

The list is too long and unreadable. A version update will be made as soon as possible in the future.

**Last updated: 2024/03/23

## Table of Contents

- [Survey](main/survey.md)
- [Recent Papers](#recent-papers)
  - [Action](#action)
  - [Active Learning](#active-learning)
  - [Adversarial Attacks](#adversarial-attacks)
  - [Anomaly Detection](#anomaly-detection)
  - [Assessment](#assessment)
  - [Augmentation](#augmentation)
  - [Audio](#audio)
  - [Bird's-Eye-View](#birds-eye-view)
  - [Captioning](#captioning)
  - [Change Detection](#change-detection)
  - [Classification (Backbone)](#classification-backbone)
  - [Clustering](#clustering)
  - [Completion](#completion)
  - [Compression](#compression)
  - [Cross-view](#cross-view)
  - [Crowd](#crowd)
  - [Deblurring](#deblurring)
  - [Depth](#depth)
  - [Deepfake Detection](#deepfake-detection)
  - [Dehazing](#dehazing)
  - [Deraining](#deraining)
  - [Denoising](#denoising)
  - [Detection](#detection)
  - [Diffusion](#diffusion)
  - [Edge](#edge)
  - [Enhancement](#enhancement)
  - [Face](#face)
  - [Federated Learning](#federated-learning)
  - [Few-shot Learning](#few-shot-learning)
  - [Fusion](#fusion)
  - [Gait](#gait)
  - [Gaze](#gaze)
  - [Generative Model](#generative-model)
  - [Graph](#graph)
  - [Hand Gesture](#hand-gesture)
  - [High Dynamic Range Imaging](#high-dynamic-range-imaging)
  - [HOI](#hoi)
  - [Hyperspectral](#hyperspectral)
  - [Illumination](#illumination)
  - [Incremental Learning](#incremental-learning)
  - [In-painting](#in-painting)
  - [Instance Segmentation](#instance-segmentation)
  - [Knowledge Distillation](#knowledge-distillation)
  - [Lane](#lane)
  - [Layout](#layout)
  - [Lighting](#lighting)
  - [LLM](#llmlvm)
  - [Matching](#matching)
  - [Matting](#matting)
  - [Medical](#medical)
  - [Mesh](#mesh)
  - [Metric learning](#metric-learning)
  - [Motion](#motion)
  - [Multi-label](#multi-label)
  - [Multi-task/modal](#multi-taskmodal)
  - [Multi-view Stereo](#multi-view-stereo)
  - [NAS](#nas)
  - [Navigation](#navigation)
  - [Neural Rendering](#neural-rendering)
  - [OCR](#ocr)
  - [Octree](#octree)
  - [Open World](#open-world)
  - [Optical Flow](#optical-flow)
  - [Panoptic Segmentation](#panoptic-segmentation)
  - [Point Cloud](#point-cloud)
  - [Pose](#pose)
  - [Planning](#planning)
  - [Pruning & Quantization](#pruning--quantization)
  - [Recognition](#recognition)
  - [Reconstruction](#reconstruction)
  - [Referring](#referring)
  - [Registration](#registration)
  - [Re-identification](#re-identification)
  - [Remote Sensing](#remote-sensing)
  - [Restoration](#restoration)
  - [Retrieval](#retrieval)
  - [Robotic](#robotic)
  - [Salient Detection](#salient-detection)
  - [Scene](#scene)
  - [Self-supervised Learning](#self-supervised-learning)
  - [Semantic Segmentation](#semantic-segmentation)
  - [Shape](#shape)
  - [SLAM](#slam)
  - [SNN](#snn)
  - [Style Transfer](#style-transfer)
  - [Super-Resolution](#super-resolution)
  - [Synthesis](#synthesis)
  - [Text-to-Image/Video](#text-to-imagevideo)
  - [Texture](#texture)
  - [Time Series](#time-series)
  - [Tracking](#tracking)
  - [Traffic](#traffic)
  - [Transfer learning](#transfer-learning)
  - [Translation](#translation)
  - [Unsupervised learning](#unsupervised-learning)
  - [UAV](#uav)
  - [Video](#video)
  - [Visual Grounding](#visual-grounding)
  - [Visual Question Answering](#visual-question-answering)
  - [Visual Reasoning](#visual-reasoning)
  - [Visual Relationship Detection](#visual-relationship-detection)
  - [Voxel](#voxel)
  - [Weakly Supervised Learning](#weakly-supervised-learning)
  - [Zero-Shot Learning](#zero-shot-learning)
  - [Others](#others)
- [Contact & Feedback](#contact--feedback)

  
## Survey: 
- (arXiv 2024.03) A Survey of Vision Transformers in Autonomous Driving: Current Trends and Future Directions. [[Paper]](https://arxiv.org/pdf/2403.07542.pdf)

- (arXiv 2024.02) A Survey on Transformer Compression. [[Paper]](https://arxiv.org/pdf/2402.05964.pdf)

- (arXiv 2024.02) Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey. [[Paper]](https://arxiv.org/pdf/2402.02941.pdf)

- (arXiv 2024.01) Transformer for Object Re-Identification: A Survey. [[Paper]](https://arxiv.org/pdf/2401.06960.pdf)

- (arXiv 2023.12) A Comprehensive Study of Vision Transformers in Image Classification Tasks. [[Paper]](https://arxiv.org/pdf/2312.01232.pdf)

- (arXiv 2023.12) A Recent Survey of Vision Transformers for Medical Image Segmentation. [[Paper]](https://arxiv.org/pdf/2312.00634.pdf)

- (arXiv 2023.11) Explainability of Vision Transformers: A Comprehensive Review and New Perspectives. [[Paper]](https://arxiv.org/pdf/2311.06786.pdf)

- (arXiv 2023.10) Understanding Video Transformers for Segmentation: A Survey of Application and Interpretability. [[Paper]](https://arxiv.org/pdf/2310.12296.pdf)

- (arXiv 2023.10) Unsupervised Object Localization in the Era of Self-Supervised ViTs: A Survey. [[Paper]](https://arxiv.org/pdf/2310.12904.pdf), [[Awesome]](https://github.com/valeoai/Awesome-Unsupervised-Object-Localization)

- (arXiv 2023.09) Transformers in Small Object Detection: A Benchmark and Survey of State-of-the-Art. [[Paper]](https://arxiv.org/pdf/2309.04902.pdf)

- (arXiv 2023.09) A survey on efficient vision transformers: algorithms, techniques, and performance benchmarking. [[Paper]](https://arxiv.org/pdf/2309.02031.pdf)

- (arXiv 2023.07) A Survey of Techniques for Optimizing Transformer Inference. [[Paper]](https://arxiv.org/pdf/2307.07982.pdf)

- (arXiv 2023.07) Transformers in Reinforcement Learning: A Survey. [[Paper]](https://arxiv.org/pdf/2307.05979.pdf)

- (arXiv 2023.07) Vision Language Transformers: A Survey. [[Paper]](https://arxiv.org/pdf/2307.03254.pdf)

- (arXiv 2023.06) 2D Object Detection with Transformers: A Review. [[Paper]](https://arxiv.org/pdf/2306.04670.pdf), [[Awesome]](https://github.com/mindgarage-shan/trans_object_detection_survey)

- (arXiv 2023.05) Vision Transformers for Mobile Applications: A Short Survey. [[Paper]](https://arxiv.org/pdf/2305.19365.pdf)

- (arXiv 2023.05) A survey of the Vision Transformers and its CNN-Transformer based Variants. [[Paper]](https://arxiv.org/pdf/2305.09880.pdf)

- (arXiv 2023.05) Semantic Segmentation using Vision Transformers: A survey. [[Paper]](https://arxiv.org/pdf/2305.03273.pdf)

- (arXiv 2023.04) Transformer-based models and hardware acceleration analysis in autonomous driving: A survey. [[Paper]](https://arxiv.org/pdf/2304.10891.pdf)

- (arXiv 2023.04) Transformer-Based Visual Segmentation: A Survey. [[Paper]](https://arxiv.org/pdf/2304.09854.pdf), [[Awesome]](https://github.com/lxtGH/Awesome-Segmenation-With-Transformer)

- (arXiv 2023.02) Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey. [[Paper]](https://arxiv.org/pdf/2302.08641.pdf)

- (arXiv 2023.02) A Survey on Efficient Training of Transformers. [[Paper]](https://arxiv.org/pdf/2302.01107.pdf)

- (arXiv 2023.01) Advances in Medical Image Analysis with Vision Transformers: A Comprehensive Review. [[Paper]](https://arxiv.org/pdf/2301.03505.pdf), [[Awesome]](https://github.com/mindflow-institue/Awesome-Transformer)

- (arXiv 2022.11) Vision Transformers in Medical Imaging: A Review. [[Paper]](https://arxiv.org/pdf/2211.10043.pdf)

- (arXiv 2022.11) A Comprehensive Survey of Transformers for Computer Vision. [[Paper]](https://arxiv.org/pdf/2211.06004.pdf)

- (arXiv 2022.09) A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective. [[Paper]](https://arxiv.org/pdf/2209.13232.pdf)

- (arXiv 2022.09) Vision Transformers for Action Recognition: A Survey. [[Paper]](https://arxiv.org/pdf/2209.05700.pdf)

- (arXiv 2022.09) Transformers in Remote Sensing: A Survey. [[Paper]](https://arxiv.org/pdf/2209.01206.pdf), [[Awesome]](https://github.com/VIROBO-15/Transformer-in-Remote-Sensing)

- (arXiv 2022.08) Medical image analysis based on transformer: A Review. [[Paper]](https://arxiv.org/pdf/2208.06643.pdf)

- (arXiv 2022.08) 3D Vision with Transformers: A Survey. [[Paper]](https://arxiv.org/pdf/2208.04309.pdf), [[Awesome]](https://github.com/lahoud/3d-vision-transformers)

- (arXiv 2022.05) Multimodal Learning with Transformers: A Survey. [[Paper]](https://arxiv.org/pdf/2206.06488.pdf)

- (arXiv 2022.05) Transformers in 3D Point Clouds: A Survey. [[Paper]](https://arxiv.org/pdf/2205.07417.pdf)

- (arXiv 2022.03) Vision Transformers in Medical Computer Vision - A Contemplative Retrospection. [[Paper]](https://arxiv.org/pdf/2203.15269.pdf)

- (arXiv 2022.03) Transformers Meet Visual Learning Understanding: A Comprehensive Review. [[Paper]](https://arxiv.org/pdf/2203.12944.pdf)

- (arXiv 2022.03) Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work. [[Paper]](https://arxiv.org/pdf/2203.01536.pdf)

- (arXiv 2022.02) Transformers in Medical Image Analysis: A Review. [[Paper]](https://arxiv.org/pdf/2202.12165.pdf)

- (arXiv 2022.01) Transformers in Medical Imaging: A Survey. [[Paper]](https://arxiv.org/pdf/2201.09873.pdf), [[Awesome]](https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging)

- (arXiv 2022.01) A Comprehensive Study of Vision Transformers on Dense Prediction Tasks. [[Paper]](https://arxiv.org/pdf/2201.08683.pdf)

- (arXiv 2022.01) Video Transformers: A Survey. [[Paper]](https://arxiv.org/pdf/2201.05991.pdf)

- (arXiv 2021.11) A Survey of Visual Transformers. [[Paper]](https://arxiv.org/pdf/2111.06091.pdf)

- (arXiv 2021.09) Survey: Transformer based Video-Language Pre-training. [[Paper]](https://arxiv.org/pdf/2109.09920.pdf)

- (arXiv 2021.03) Multi-modal Motion Prediction with Stacked Transformers. [[Paper]](https://arxiv.org/pdf/2103.11624.pdf), [[Code]](https://github.com/decisionforce/mmTransformer)

- (arXiv 2021.03) Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with Language and Vision. [[Paper]](https://arxiv.org/pdf/2103.04037.pdf)

- (arXiv 2020.09) Efficient Transformers: A Survey. [[Paper]](https://arxiv.org/pdf/2009.06732.pdf)

- (arXiv 2020.01) Transformers in Vision: A Survey. [[Paper]](https://arxiv.org/pdf/2101.01169.pdf)

  
## Recent Papers

### Action
- (CVPR'20) Speech2Action: Cross-modal Supervision for Action Recognition, [[Paper]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Nagrani_Speech2Action_Cross-Modal_Supervision_for_Action_Recognition_CVPR_2020_paper.pdf)
- (arXiv 2021.01) Trear: Transformer-based RGB-D Egocentric Action Recognition, [[Paper]](https://arxiv.org/pdf/2101.03904.pdf)
- (arXiv 2021.02) Relaxed Transformer Decoders for Direct Action Proposal Generation, [[Paper]](https://arxiv.org/pdf/2102.01894.pdf), [[Code]](https://github.com/MCG-NJU/RTD-Action)
- (arXiv 2021.04) TubeR: Tube-Transformer for Action Detection, [[Paper]](https://arxiv.org/pdf/2104.00969.pdf)
- (arXiv 2021.04) Few-Shot Transformation of Common Actions into Time and Space, [[Paper]](https://arxiv.org/pdf/2104.02439.pdf)
- (arXiv 2021.05) Temporal Action Proposal Generation with Transformers, [[Paper]](https://arxiv.org/pdf/2105.12043.pdf)
- (arXiv 2021.06) End-to-end Temporal Action Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2106.10271.pdf), [[Code]](https://github.com/xlliu7/TadTR)
- (arXiv 2021.06) OadTR: Online Action Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2106.11149.pdf), [[Code]](https://github.com/wangxiang1230/OadTR)
- (arXiv 2021.07) Action Transformer: A Self-Attention Model for Short-Time Human Action Recognition, [[Paper]](https://arxiv.org/pdf/2107.00606.pdf)
- (arXiv 2021.07) VideoLightFormer: Lightweight Action Recognition using Transformers, [[Paper]](https://arxiv.org/pdf/2107.00451.pdf)
- (arXiv 2021.07) Long Short-Term Transformer for Online Action Detection, [[Paper]](https://arxiv.org/pdf/2107.03377.pdf)
- (arXiv 2021.07) STAR: Sparse Transformer-based Action Recognition, [[Paper]](https://arxiv.org/pdf/2107.07089.pdf), [[Code]](https://github.com/imj2185/STAR)
- (arXiv 2021.08) Shifted Chunk Transformer for Spatio-Temporal Representational Learning, [[Paper]](https://arxiv.org/pdf/2108.11575.pdf)
- (arXiv 2021.08) GroupFormer: Group Activity Recognition with Clustered Spatial-Temporal Transformer, [[Paper]](https://arxiv.org/pdf/2108.12630.pdf), [[Code]](https://github.com/xueyee/GroupFormer)
- (arXiv 2021.09) GCsT: Graph Convolutional Skeleton Transformer for Action Recognition, [[Paper]](https://arxiv.org/pdf/2109.02860.pdf), [[Code]](https://github.com/xueyee/GroupFormer)
- (arXiv 2021.10) Lightweight Transformer in Federated Setting for Human Activity Recognition, [[Paper]](https://arxiv.org/pdf/2110.00244.pdf)
- (arXiv 2021.10) ASFormer: Transformer for Action Segmentation, [[Paper]](https://arxiv.org/pdf/2110.08568.pdf), [[Code]](https://github.com/ChinaYi/ASFormer)
- (arXiv 2021.10) Few-Shot Temporal Action Localization with Query Adaptive Transformer, [[Paper]](https://arxiv.org/pdf/2110.10552.pdf), [[Code]](https://github.com/sauradip/fewshotQAT)
- (arXiv 2021.10) IIP-Transformer: Intra-Inter-Part Transformer for Skeleton-Based Action Recognition, [[Paper]](https://arxiv.org/pdf/2110.13385.pdf), [[Code]](https://github.com/qtwang0035/IIP-Transformer)
- (arXiv 2021.11) Evaluating Transformers for Lightweight Action Recognition, [[Paper]](https://arxiv.org/pdf/2111.09641.pdf)
- (arXiv 2021.12) MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection, [[Paper]](https://arxiv.org/pdf/2112.03902.pdf)
- (arXiv 2021.12) Co-training Transformer with Videos and Images Improves Action Recognition, [[Paper]](https://arxiv.org/pdf/2112.07175.pdf)
- (arXiv 2021.12) Temporal Transformer Networks with Self-Supervision for Action Recognition, [[Paper]](https://arxiv.org/pdf/2112.07338.pdf)
- (arXiv 2022.01) Spatio-Temporal Tuples Transformer for Skeleton-Based Action Recognition, [[Paper]](https://arxiv.org/pdf/2201.02849.pdf), [[Code]](https://github.com/heleiqiu/STTFormer)
- (arXiv 2022.01) Transformers in Action:Weakly Supervised Action Segmentation, [[Paper]](https://arxiv.org/pdf/2201.05675.pdf)
- (arXiv 2022.02) ActionFormer: Localizing Moments of Actions with Transformers, [[Paper]](https://arxiv.org/pdf/2202.07925.pdf), [[Code]](https://github.com/happyharrycn/actionformer_release)
- (arXiv 2022.03) Multi-View Fusion Transformer for Sensor-Based Human Activity Recognition, [[Paper]](https://arxiv.org/pdf/2202.12949.pdf)
- (arXiv 2022.03) TransDARC: Transformer-based Driver Activity Recognition with Latent Space Feature Calibration, [[Paper]](https://arxiv.org/pdf/2203.00927.pdf), [[Code]](https://github.com/KPeng9510/TransDARC)
- (arXiv 2022.03) Zero-Shot Action Recognition with Transformer-based Video Semantic Embedding, [[Paper]](https://arxiv.org/pdf/2203.05156.pdf)
- (arXiv 2022.03) LocATe: End-to-end Localization of Actions in 3D with Transformers, [[Paper]](https://arxiv.org/pdf/2203.10719.pdf)
- (arXiv 2022.03) DirecFormer: A Directed Attention in Transformer Approach to Robust Action Recognition, [[Paper]](https://arxiv.org/pdf/2203.10233.pdf), [[Code]](https://github.com/uark-cviu/DirecFormer)
- (arXiv 2022.03) Multi-label Transformer for Action Unit Detection, [[Paper]](https://arxiv.org/pdf/2203.12531.pdf)
- (arXiv 2022.04) Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition, [[Paper]](https://arxiv.org/pdf/2204.00452.pdf)
- (arXiv 2022.04) TALLFormer: Temporal Action Localization with Long-memory Transformer, [[Paper]](https://arxiv.org/pdf/2204.01680.pdf), [[Code]](https://github.com/klauscc/TALLFormer)
- (arXiv 2022.04) TransRAC: Encoding Multi-scale Temporal Correlation with Transformers for Repetitive Action Counting, [[Paper]](https://arxiv.org/pdf/2204.01018.pdf), [[Code]](https://github.com/SvipRepetitionCounting/TransRAC)
- (arXiv 2022.04) Detector-Free Weakly Supervised Group Activity Recognition, [[Paper]](https://arxiv.org/pdf/2204.02139.pdf), [[Code]](https://cvlab.postech.ac.kr/research/DFWSGAR/)
- (arXiv 2022.05) Cross-modal Representation Learning for Zero-shot Action Recognition, [[Paper]](https://arxiv.org/pdf/2205.01657.pdf), [[Code]](https://github.com/microsoft/ResT)
- (arXiv 2022.05) Entity-aware and Motion-aware Transformers for Language-driven Action Localization in Videos, [[Paper]](https://arxiv.org/pdf/2205.05854.pdf), [[Code]](https://github.com/shuoyang129/EAMAT)
- (arXiv 2022.05) Cross-subject Action Unit Detection with Meta Learning and Transformer-based Relation Modeling, [[Paper]](https://arxiv.org/pdf/2205.08787.pdf)
- (arXiv 2022.05) Cross-Enhancement Transformer for Action Segmentation, [[Paper]](https://arxiv.org/pdf/2205.09445.pdf)
- (arXiv 2022.05) Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation, [[Paper]](https://arxiv.org/pdf/2205.13425.pdf)
- (arXiv 2022.05) Future Transformer for Long-term Action Anticipation, [[Paper]](https://arxiv.org/pdf/2205.14022.pdf), [[Code]](http://cvlab.postech.ac.kr/research/FUTR)
- (arXiv 2022.06) One-stage Action Detection Transformer, [[Paper]](https://arxiv.org/pdf/2206.10080.pdf)
- (arXiv 2022.06) Spatial Transformer Network with Transfer Learning for Small-scale Fine-grained Skeleton-based Tai Chi Action Recognition, [[Paper]](https://arxiv.org/pdf/2206.15002.pdf)
- (arXiv 2022.07) Hunting Group Clues with Transformers for Social Group Activity Recognition, [[Paper]](https://arxiv.org/pdf/2207.05254.pdf)
- (arXiv 2022.07) Global-local Motion Transformer for Unsupervised Skeleton-based Action Learning, [[Paper]](https://arxiv.org/pdf/2207.06101.pdf),[[Code]](https://github.com/Boeun-Kim/GL-Transformer)
- (arXiv 2022.07) Entry-Flipped Transformer for Inference and Prediction of Participant Behavior, [[Paper]](https://arxiv.org/pdf/2207.06235.pdf),[[Code]](https://github.com/Boeun-Kim/GL-Transformer)
- (arXiv 2022.07) Action Quality Assessment with Temporal Parsing Transformer, [[Paper]](https://arxiv.org/pdf/2207.09270.pdf)
- (arXiv 2022.07) HTNet: Anchor-free Temporal Action Localization with Hierarchical Transformers, [[Paper]](https://arxiv.org/pdf/2207.09662.pdf)
- (arXiv 2022.07) An Efficient Spatio-Temporal Pyramid Transformer for Action Detection, [[Paper]](https://arxiv.org/pdf/2207.10448.pdf)
- (arXiv 2022.07) Action Quality Assessment using Transformers, [[Paper]](https://arxiv.org/pdf/2207.12318.pdf)
- (arXiv 2022.07) Unsupervised Domain Adaptation for Video Transformers in Action Recognition, [[Paper]](https://arxiv.org/pdf/2207.12842.pdf),[[Code]](https://github.com/vturrisi/UDAVT)
- (arXiv 2022.07) Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition, [[Paper]](https://arxiv.org/pdf/2207.13259.pdf),[[Code]](https://github.com/MartinXM/TPS)
- (arXiv 2022.08) Combined CNN Transformer Encoder for Enhanced Fine-grained Human Action Recognition, [[Paper]](https://arxiv.org/pdf/2208.01897.pdf)
- (arXiv 2022.08) ViT-ReT: Vision and Recurrent Transformer Neural Networks for Human Activity Recognition in Videos, [[Paper]](https://arxiv.org/pdf/2208.07929.pdf),[[Code]](https://github.com/JamesWensel/TranformerActivityRecognition)
- (arXiv 2022.08) Adaptive Perception Transformer for Temporal Action Localization, [[Paper]](https://arxiv.org/pdf/2208.11908.pdf),[[Code]](https://github.com/SouperO/AdaPerFormer)
- (arXiv 2022.08) A Circular Window-based Cascade Transformer for Online Action Detection, [[Paper]](https://arxiv.org/pdf/2208.14209.pdf)
- (arXiv 2022.09) Self-Supervised Multimodal Fusion Transformer for Passive Activity Recognition, [[Paper]](https://arxiv.org/pdf/2209.03765.pdf)
- (arXiv 2022.09) TASKED: Transformer-based Adversarial learning for human activity recognition using wearable sensors via Self-KnowledgE Distillation, [[Paper]](https://arxiv.org/pdf/2209.09092.pdf)
- (arXiv 2022.09) Exploring Modulated Detection Transformer as a Tool for Action Recognition in Videos, [[Paper]](https://arxiv.org/pdf/2209.10126.pdf)
- (arXiv 2022.09) Lightweight Transformers for Human Activity Recognition on Mobile Devices, [[Paper]](https://arxiv.org/pdf/2209.11750.pdf)
- (arXiv 2022.09) Multi-dataset Training of Transformers for Robust Action Recognition, [[Paper]](https://arxiv.org/pdf/2209.12362.pdf),[[Code]](https://github.com/JunweiLiang/MultiTrain)
- (arXiv 2022.10) Focal and Global Spatial-Temporal Transformer for Skeleton-based Action Recognition, [[Paper]](https://arxiv.org/pdf/2210.02693.pdf),[[Code]](https://github.com/JunweiLiang/MultiTrain)
- (arXiv 2022.10) STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition, [[Paper]](https://arxiv.org/pdf/2210.07503.pdf)
- (arXiv 2022.10) Transformer-based Action recognition in hand-object interacting scenarios, [[Paper]](https://arxiv.org/pdf/2210.11387.pdf)
- (arXiv 2022.10) Anticipative Feature Fusion Transformer for Multi-Modal Action Anticipation, [[Paper]](https://arxiv.org/pdf/2210.12649.pdf)
- (arXiv 2022.10) Holistic Interaction Transformer Network for Action Detection, [[Paper]](https://arxiv.org/pdf/2210.12686.pdf),[[Code]](https://github.com/joslefaure/HIT)
- (arXiv 2022.10) GliTr: Glimpse Transformers with Spatiotemporal Consistency for Online Action Prediction, [[Paper]](https://arxiv.org/pdf/2210.13605.pdf)
- (arXiv 2022.10) Hypergraph Transformer for Skeleton-based Action Recognition, [[Paper]](https://arxiv.org/pdf/2211.09590.pdf)
- (arXiv 2022.11) SVFormer: Semi-supervised Video Transformer for Action Recognition, [[Paper]](https://arxiv.org/pdf/2211.13222.pdf),[[Code]](https://github.com/ChenHsing/SVFormer)
- (arXiv 2022.11) Interaction Visual Transformer for Egocentric Action Anticipation, [[Paper]](https://arxiv.org/pdf/2211.14154.pdf),[[Code]](https://github.com/ChenHsing/SVFormer)
- (arXiv 2023.02) Transformers in Action Recognition: A Review on Temporal Modeling, [[Paper]](https://arxiv.org/pdf/2302.01921.pdf)
- (arXiv 2023.02) Video Action Recognition Collaborative Learning with Dynamics via PSO-ConvNet Transformer, [[Paper]](https://arxiv.org/pdf/2302.09187.pdf),[[Code]](https://github.com/leonlha/Video-Action-Recognition-via-PSO-ConvNet-Transformer-Collaborative-Learning-with-Dynamics)
- (arXiv 2023.02) Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition, [[Paper]](https://arxiv.org/pdf/2302.13434.pdf)
- (arXiv 2023.02) Temporal Segment Transformer for Action Segmentation, [[Paper]](https://arxiv.org/pdf/2302.13074.pdf)
- (arXiv 2023.03) EgoViT: Pyramid Video Transformer for Egocentric Action Recognition, [[Paper]](https://arxiv.org/pdf/2303.08920.pdf)
- (arXiv 2023.03) Vision Transformer for Action Units Detection, [[Paper]](https://arxiv.org/pdf/2303.09917.pdf)
- (arXiv 2023.03) Group Activity Recognition using Self-supervised Approach of Spatiotemporal Transformers, [[Paper]](https://arxiv.org/pdf/2303.12149.pdf)
- (arXiv 2023.03) 3Mformer: Multi-order Multi-mode Transformer for Skeletal Action Recognition, [[Paper]](https://arxiv.org/pdf/2303.14474.pdf)
- (arXiv 2023.04) STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition, [[Paper]](https://arxiv.org/pdf/2303.18177.pdf),[[Code]](https://github.com/zgzxy001/STMT)
- (arXiv 2023.04) End-to-End Spatio-Temporal Action Localisation with Video Transformers, [[Paper]](https://arxiv.org/pdf/2304.12160.pdf)
- (arXiv 2023.05) Distilled Mid-Fusion Transformer Networks for Multi-Modal Human Activity Recognition, [[Paper]](https://arxiv.org/pdf/2305.03810.pdf)
- (arXiv 2023.05) Multi-View Multi-Scale Driver Action Recognition with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.08877.pdf),[[Code]](https://github.com/PurdueDigitalTwin/M2DAR)
- (arXiv 2023.05) Enhancing Transformer Backbone for Egocentric Video Action Segmentation, [[Paper]](https://arxiv.org/pdf/2305.11365.pdf),[[Code]](https://www.sail-nu.com/dxformer)
- (arXiv 2023.05) A Multi-Modal Transformer Network for Action Detection, [[Paper]](https://arxiv.org/pdf/2305.19624.pdf)
- (arXiv 2023.06) Optimizing ViViT Training: Time and Memory Reduction for Action Recognition, [[Paper]](https://arxiv.org/pdf/2306.04822.pdf)
- (arXiv 2023.06) SpATr: MoCap 3D Human Action Recognition based on Spiral Auto-encoder and Transformer Network, [[Paper]](https://arxiv.org/pdf/2306.17574.pdf)
- (arXiv 2023.07) Task-Specific Alignment and Multiple Level Transformer for Few-Shot Action Recognition, [[Paper]](https://arxiv.org/pdf/2307.01985.pdf),[[Code]](https://github.com/cofly2014/tsa-mlt.git)
- (arXiv 2023.07) VS-TransGRU: A Novel Transformer-GRU-based Framework Enhanced by Visual-Semantic Fusion for Egocentric Action Anticipation, [[Paper]](https://arxiv.org/pdf/2307.03918.pdf)
- (arXiv 2023.07) Multimodal Distillation for Egocentric Action Recognition, [[Paper]](https://arxiv.org/pdf/2307.07483.pdf)
- (arXiv 2023.07) Human Action Recognition in Still Images Using ConViT, [[Paper]](https://arxiv.org/pdf/2307.08994.pdf)
- (arXiv 2023.07) MSQNet: Actor-agnostic Action Recognition with Multi-modal Query, [[Paper]](https://arxiv.org/pdf/2307.10763.pdf), [[Code]](https://github.com/mondalanindya/MSQNet)
- (arXiv 2023.07) Event-based Vision for Early Prediction of Manipulation Actions, [[Paper]](https://arxiv.org/pdf/2307.14332.pdf)
- (arXiv 2023.08) PAT: Position-Aware Transformer for Dense Multi-Label Action Detection, [[Paper]](https://arxiv.org/pdf/2308.05051.pdf)
- (arXiv 2023.08) Seeing in Flowing: Adapting CLIP for Action Recognition with Motion Prompts Learning, [[Paper]](https://arxiv.org/pdf/2308.04828.pdf)
- (arXiv 2023.08) MAiVAR-T: Multimodal Audio-image and Video Action Recognizer using Transformers, [[Paper]](https://arxiv.org/pdf/2308.03741.pdf)
- (arXiv 2023.08) Memory-and-Anticipation Transformer for Online Action Understanding, [[Paper]](https://arxiv.org/pdf/2308.07893.pdf), [[Code]](https://github.com/Echo0125/)
- (arXiv 2023.08) Self-Feedback DETR for Temporal Action Detection, [[Paper]](https://arxiv.org/pdf/2308.10570.pdf), [[Code]](https://github.com/Echo0125/)
- (arXiv 2023.08) EventTransAct: A video transformer-based framework for Event-camera based action recognition, [[Paper]](https://arxiv.org/pdf/2308.13711.pdf), [[Code]](https://tristandb8.github.io/EventTransAct_webpage/)
- (arXiv 2023.08) Topology-aware MLP for Skeleton-based Action Recognition, [[Paper]](https://arxiv.org/pdf/2308.16018.pdf), [[Code]](https://github.com/BUPTSJZhang/Ta-MLP)
- (arXiv 2023.08) Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation, [[Paper]](https://arxiv.org/pdf/2308.16552.pdf)
- (arXiv 2023.09) COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers, [[Paper]](https://arxiv.org/pdf/2309.01270.pdf), [[Code]](https://github.com/juliendenize/eztorch)
- (arXiv 2023.09) Unified Contrastive Fusion Transformer for Multimodal Human Action Recognition, [[Paper]](https://arxiv.org/pdf/2309.05032.pdf)
- (arXiv 2023.09) SkeleTR: Towrads Skeleton-based Action Recognition in the Wild, [[Paper]](https://arxiv.org/pdf/2309.11445.pdf)
- (arXiv 2023.09) Egocentric RGB+Depth Action Recognition in Industry-Like Settings, [[Paper]](https://arxiv.org/pdf/2309.13962.pdf)
- (arXiv 2023.10) POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization, [[Paper]](https://arxiv.org/pdf/2310.13585.pdf)
- (arXiv 2023.11) Distilling Knowledge from CNN-Transformer Models for Enhanced Human Action Recognition, [[Paper]](https://arxiv.org/pdf/2311.01283.pdf)
- (arXiv 2023.11) Act-VIT: A Representationally Robust Attention Architecture for Skeleton Based Action Recognition Using Vision Transformer, [[Paper]](https://arxiv.org/pdf/2311.08094.pdf)
- (arXiv 2023.11) SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action Segmentation, [[Paper]](https://arxiv.org/pdf/2311.17428.pdf), [[Code]](https://github.com/LIUQI-creat/SigFormer)
- (arXiv 2023.11) GeoDeformer: Geometric Deformable Transformer for Action Recognition, [[Paper]](https://arxiv.org/pdf/2311.17975.pdf)
- (arXiv 2023.12) REACT: Recognize Every Action Everywhere All At Once, [[Paper]](https://arxiv.org/pdf/2312.00188.pdf)
- (arXiv 2023.12) Adapting Short-Term Transformers for Action Detection in Untrimmed Videos, [[Paper]](https://arxiv.org/pdf/2312.00188.pdf)
- (arXiv 2023.12) STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition, [[Paper]](https://arxiv.org/pdf/2312.03288.pdf),[[Code]](https://github.com/maclong01/STEP-CATFormer)
- (arXiv 2024.01) Multi-view Distillation based on Multi-modal Fusion for Few-shot Action Recognition, [[Paper]](https://arxiv.org/pdf/2401.08345.pdf),[[Code]](https://github.com/cofly2014/MDMF)
- (arXiv 2024.03) SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition, [[Paper]](https://arxiv.org/pdf/2403.09508.pdf),[[Code]](https://jeonghyeokdo.github.io/SkateFormer_site/)

### Active Learning
- (arXiv 2022.06) Visual Transformer for Task-aware Active Learning, [[Paper]](https://arxiv.org/pdf/2206.06761.pdf), [[Code]](https://github.com/razvancaramalau/Visual-Transformer-for-Task-aware-Active-Learning)

### Adversarial Attacks
- (arXiv 2022.06) Exploring Adversarial Attacks and Defenses in Vision Transformers trained with DINO, [[Paper]](https://arxiv.org/pdf/2206.06761.pdf), [[Code]](https://github.com/thobauma/AADefDINO)
- (arXiv 2022.06) Backdoor Attacks on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.08477.pdf), [[Code]](https://github.com/UCDvision/backdoor_transformer.git)
- (arXiv 2022.06) Defending Backdoor Attacks on Vision Transformer via Patch Processing, [[Paper]](https://arxiv.org/pdf/2206.12381.pdf)
- (arXiv 2022.07) Towards Efficient Adversarial Training on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.10498.pdf)
- (arXiv 2022.08) Understanding Adversarial Robustness of Vision Transformers via Cauchy Problem, [[Paper]](https://arxiv.org/pdf/2208.00906.pdf), [[Code]](https://github.com/TrustAI/ODE4RobustViT)
- (arXiv 2022.08) Analyzing Adversarial Robustness of Vision Transformers against Spatial and Spectral Attacks, [[Paper]](https://arxiv.org/pdf/2208.09602.pdf)
- (arXiv 2023.01) Inference Time Evidences of Adversarial Attacks for Forensic on Transformers, [[Paper]](https://arxiv.org/pdf/2301.13356.pdf)
- (arXiv 2023.03) Transferable Adversarial Attacks on Vision Transformers with Token Gradient Regularization, [[Paper]](https://arxiv.org/pdf/2303.15754.pdf)
- (arXiv 2023.05) On enhancing the robustness of Vision Transformers: Defensive Diffusion, [[Paper]](https://arxiv.org/pdf/2305.08031.pdf), [[Code]](https://github.com/Muhammad-Huzaifaa/Defensive_Diffusion)
- (arXiv 2023.06) Pre-trained transformer for adversarial purification, [[Paper]](https://arxiv.org/pdf/2306.01762.pdf)
- (arXiv 2023.07) Random Position Adversarial Patch for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.04066.pdf)
- (arXiv 2023.07) Enhanced Security against Adversarial Examples Using a Random Ensemble of Encrypted Vision Transformer Models, [[Paper]](https://arxiv.org/pdf/2307.13985.pdf)
- (arXiv 2023.09) Exploring Non-additive Randomness on ViT against Query-Based Black-Box Attacks, [[Paper]](https://arxiv.org/pdf/2309.06438.pdf)
- (arXiv 2023.09) RBFormer: Improve Adversarial Robustness of Transformer by Robust Bias, [[Paper]](https://arxiv.org/pdf/2309.13245.pdf)
- (arXiv 2023.10) Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models, [[Paper]](https://arxiv.org/pdf/2310.04655.pdf)
- (arXiv 2023.10) ConViViT -- A Deep Neural Network Combining Convolutions and Factorized Self-Attention for Human Activity Recognition, [[Paper]](https://arxiv.org/pdf/2310.14416.pdf)
- (arXiv 2023.10) Blacksmith: Fast Adversarial Training of Vision Transformers via a Mixture of Single-step and Multi-step Methods, [[Paper]](https://arxiv.org/pdf/2310.18975.pdf)
- (arXiv 2023.11) DialMAT: Dialogue-Enabled Transformer with Moment-Based Adversarial Training, [[Paper]](https://arxiv.org/pdf/2311.06855.pdf)
- (arXiv 2023.11) Attention Deficit is Ordered! Fooling Deformable Vision Transformers with Collaborative Adversarial Patches, [[Paper]](https://arxiv.org/pdf/2311.12914.pdf)
- (arXiv 2023.12) MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness, [[Paper]](https://arxiv.org/pdf/2312.04960.pdf), [[Code]](https://github.com/xiaoyunxxy/MIMIR)
- (arXiv 2024.01) FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.01752.pdf)
- (arXiv 2024.02) DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2402.02554.pdf)
- (arXiv 2024.03) Attacking Transformers with Feature Diversity Adversarial Perturbation, [[Paper]](https://arxiv.org/pdf/2403.07942.pdf)
- (arXiv 2024.03) Approximate Nullspace Augmented Finetuning for Robust Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.10476.pdf)

### Anomaly Detection
- (arXiv 2021.04) VT-ADL: A Vision Transformer Network for Image Anomaly Detection and Localization, [[Paper]](https://arxiv.org/pdf/2104.10036.pdf)
- (arXiv 2021.04) Inpainting Transformer for Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2104.13897.pdf)
- (arXiv 2022.03) AnoViT: Unsupervised Anomaly Detection and Localization with Vision Transformer-based Encoder-Decoder, [[Paper]](https://arxiv.org/pdf/2203.10808.pdf)
- (arXiv 2022.06) Anomaly detection in surveillance videos using transformer based attention model, [[Paper]](https://arxiv.org/pdf/2206.01524.pdf), [[Code]](https://github.com/kapildeshpande/Anomaly-Detection-in-Surveillance-Videos)
- (arXiv 2022.06) Multi-Contextual Predictions with Vision Transformer for Video Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2206.08568.pdf)
- (arXiv 2022.08) HaloAE: An HaloNet based Local Transformer Auto-Encoder for Anomaly Detection and Localization, [[Paper]](https://arxiv.org/pdf/2208.03486.pdf), [[Code]](https://anonymous.4open.science/r/HaloAE-E27B/README.md)
- (arXiv 2022.08) ADTR: Anomaly Detection Transformer with Feature Reconstruction, [[Paper]](https://arxiv.org/pdf/2209.01816.pdf)
- (arXiv 2022.09) Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2209.12148.pdf), [[Code]](https://github.com/ristea/ssmctb)
- (arXiv 2022.09) Anomaly Detection in Aerial Videos with Transformers, [[Paper]](https://arxiv.org/pdf/2209.13363.pdf), [[Code]](https://github.com/jin-pu/drone-anomaly)
- (arXiv 2022.10) Masked Transformer for image Anomaly Localization, [[Paper]](https://arxiv.org/pdf/2210.15540.pdf)
- (arXiv 2022.11) Generalizable Industrial Visual Anomaly Detection with Self-Induction Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.12311.pdf)
- (arXiv 2023.03) Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization, [[Paper]](https://arxiv.org/pdf/2303.17354.pdf)
- (arXiv 2023.03) Unsupervised Anomaly Detection with Local-Sensitive VQVAE and Global-Sensitive Transformers, [[Paper]](https://arxiv.org/pdf/2303.17505.pdf)
- (arXiv 2023.03) Visual Anomaly Detection via Dual-Attention Transformer and Discriminative Flow, [[Paper]](https://arxiv.org/pdf/2303.17882.pdf)
- (arXiv 2023.05) Multiresolution Feature Guidance Based Transformer for Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2305.14880.pdf)
- (arXiv 2023.06) Efficient Anomaly Detection with Budget Annotation Using Semi-Supervised Residual Transformer, [[Paper]](https://arxiv.org/pdf/2306.03492.pdf), [[Code]](https://github.com/BeJane/Semi_REST)
- (arXiv 2023.07) SelFormaly: Towards Task-Agnostic Unified Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2307.12540.pdf)
- (arXiv 2023.08) Patch-wise Auto-Encoder for Visual Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2308.00429.pdf)
- (arXiv 2023.09) Mask2Anomaly: Mask Transformer for Universal Open-set Segmentation, [[Paper]](https://arxiv.org/pdf/2309.04579.pdf)
- (arXiv 2023.10) Hierarchical Vector Quantized Transformer for Multi-class Unsupervised Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2310.14228.pdf), [[Code]](https://github.com/RuiyingLu/HVQ-Trans)
- (arXiv 2023.12) Intelligent Anomaly Detection for Lane Rendering Using Transformer with Self-Supervised Pre-Training and Customized Fine-Tuning, [[Paper]](https://arxiv.org/pdf/2312.04398.pdf)
- (arXiv 2023.12) Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2312.07495.pdf), [[Code]](https://zhangzjn.github.io/projects/ViTAD)

### Assessment
- (arXiv 2021.01) Transformer for Image Quality Assessment, [[Paper]](https://arxiv.org/abs/2101.01097), [[Code]](https://github.com/junyongyou/triq)
- (arXiv 2021.04) Perceptual Image Quality Assessment with Transformers, [[Paper]](https://arxiv.org/abs/2104.14730), [[Code]](https://github.com/manricheon/IQT)
- (arXiv 2021.08) No-Reference Image Quality Assessment via Transformers, Relative Ranking, and Self-Consistency, [[Paper]](https://arxiv.org/pdf/2108.06858.pdf), [[Code]](https://github.com/isalirezag/TReS)
- (arXiv 2021.08) MUSIQ: Multi-scale Image Quality Transformer, [[Paper]](https://arxiv.org/pdf/2108.05997.pdf), [[Code]](https://github.com/google-research/google-research/tree/master/musiq)
- (arXiv 2021.10) VTAMIQ: Transformers for Attention Modulated Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2110.01655.pdf)
- (arXiv 2021.12) Learning Transformer Features for Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2112.00485.pdf)
- (arXiv 2022.03) Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment, [[Paper]](https://arxiv.org/pdf/2203.14557.pdf)
- (arXiv 2022.04) Multi-Scale Features and Parallel Transformers Based Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2204.09779.pdf), [[Code]](https://github.com/KomalPal9610/IQA)
- (arXiv 2022.05) SwinIQA: Learned Swin Distance for Compressed Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2205.04264.pdf)
- (arXiv 2022.05) MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion, [[Paper]](https://arxiv.org/pdf/2205.10101.pdf)
- (arXiv 2022.08) DAHiTrA: Damage Assessment Using a Novel Hierarchical Transformer Architecture, [[Paper]](https://arxiv.org/pdf/2208.02205.pdf)
- (arXiv 2022.10) DCVQE: A Hierarchical Transformer for Video Quality Assessment, [[Paper]](https://arxiv.org/pdf/2210.04377.pdf)
- (arXiv 2023.03) ST360IQ: No-Reference Omnidirectional Image Quality Assessment with Spherical Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.06907.pdf), [[Code]](https://github.com/Nafiseh-Tofighi/ST360IQ)
- (arXiv 2023.03) MRET: Multi-resolution Transformer for Video Quality Assessment, [[Paper]](https://arxiv.org/pdf/2303.07489.pdf)
- (arXiv 2023.05) Blind Image Quality Assessment via Transformer Predicted Error Map and Perceptual Quality Token, [[Paper]](https://arxiv.org/pdf/2305.09353.pdf), [[Code]](https://github.com/Srache/TempQT)
- (arXiv 2023.08) Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2308.12001.pdf)
- (arXiv 2023.12) Activating Frequency and ViT for 3D Point Cloud Quality Assessment without Reference, [[Paper]](https://arxiv.org/pdf/2312.05972.pdf), [[Code]](https://github.com/o-messai/3D-PCQA)
- (arXiv 2024.01) Video Quality Assessment Based on Swin TransformerV2 and Coarse to Fine Strategy, [[Paper]](https://arxiv.org/pdf/2401.08522.pdf), [[Code]](https://github.com/o-messai/3D-PCQA)

### Augmentation 
- (arXiv 2022.10) TokenMixup: Efficient Attention-guided Token-level Data Augmentation for Transformers, [[Paper]](https://arxiv.org/pdf/2210.07562.pdf), [[Code]](https://github.com/mlvlab/TokenMixup)
- (arXiv 2022.12) SMMix: Self-Motivated Image Mixing for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.12977.pdf), [[Code]](https://github.com/ChenMnZ/SMMix)
- (arXiv 2023.05) Transformer-based Sequence Labeling for Audio Classification based on MFCCs, [[Paper]](https://arxiv.org/pdf/2305.00417.pdf)

### Audio 
- (arXiv 2022.11) ASiT: Audio Spectrogram vIsion Transformer for General Audio Representation, [[Paper]](https://arxiv.org/pdf/2211.13189.pdf)
- (arXiv 2023.03) Multiscale Audio Spectrogram Transformer for Efficient Audio Classification, [[Paper]](https://arxiv.org/pdf/2303.10757.pdf)
- (arXiv 2023.03) ModEFormer: Modality-Preserving Embedding for Audio-Video Synchronization using Transformers, [[Paper]](https://arxiv.org/pdf/2303.11551.pdf)
- (arXiv 2023.07) AVSegFormer: Audio-Visual Segmentation with Transformer, [[Paper]](https://arxiv.org/pdf/2307.01146.pdf), [[Code]](https://github.com/vvvb-github/AVSegFormer)
- (arXiv 2023.11) Rethink Cross-Modal Fusion in Weakly-Supervised Audio-Visual Video Parsing, [[Paper]](https://arxiv.org/pdf/2311.08151.pdf)
- (arXiv 2023.12) Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling, [[Paper]](https://arxiv.org/pdf/2312.01017.pdf)
- (arXiv 2024.01) Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video Classificationg, [[Paper]](https://arxiv.org/pdf/2312.01017.pdf)

### Bird's-Eye-View
- (arXiv 2022.03) BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers, [[Paper]](https://arxiv.org/pdf/2203.17270.pdf), [[Code]](https://github.com/zhiqi-li/BEVFormer)
- (arXiv 2022.05) ViT-BEVSeg: A Hierarchical Transformer Network for Monocular Birds-Eye-View Segmentation, [[Paper]](https://arxiv.org/pdf/2205.15667.pdf), [[Code]](https://github.com/robotvisionmu/ViT-BEVSeg)
- (arXiv 2022.06) PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images, [[Paper]](https://arxiv.org/pdf/2206.01256.pdf)
- (arXiv 2022.06) Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer, [[Paper]](https://arxiv.org/pdf/2206.04584.pdf), [[Code]](https://github.com/hustvl/GKT)
- (arXiv 2022.06) PolarFormer: Multi-camera 3D Object Detection with Polar Transformer, [[Paper]](https://arxiv.org/pdf/2206.15398.pdf), [[Code]](https://github.com/fudan-zvg/PolarFormer)
- (arXiv 2022.07) CoBEVT: Cooperative Bird's Eye View ation with Sparse Transformers, [[Paper]](https://arxiv.org/pdf/2207.02202.pdf)
- (arXiv 2022.07) UniFormer: Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View, [[Paper]](https://arxiv.org/pdf/2207.08536.pdf)
- (arXiv 2022.09) A Dual-Cycled Cross-View Transformer Network for Unified Road Layout Estimation and 3D Object Detection in the Bird's-Eye-View, [[Paper]](https://arxiv.org/pdf/2209.08844.pdf)
- (arXiv 2022.09) BEV-LGKD: A Unified LiDAR-Guided Knowledge Distillation Framework for BEV 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2212.00623.pdf)
- (arXiv 2023.02) DA-BEV: Depth Aware BEV Transformer for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2302.13002.pdf)
- (arXiv 2023.03) TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2303.09998.pdf), [[Code]](https://github.com/MediaBrain-SJTU/TBP-Former)
- (arXiv 2023.04) VoxelFormer: Bird's-Eye-View Feature Generation based on Dual-view Attention for Multi-view 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2304.01054.pdf), [[Code]](https://github.com/Lizhuoling/VoxelFormer-public.git)
- (arXiv 2023.04) FedBEVT: Federated Learning Bird's Eye View Perception Transformer in Road Traffic Systems, [[Paper]](https://arxiv.org/pdf/2304.01534.pdf)
- (arXiv 2023.04) A Cross-Scale Hierarchical Transformer with Correspondence-Augmented Attention for inferring Bird's-Eye-View ation, [[Paper]](https://arxiv.org/pdf/2304.03650.pdf)
- (arXiv 2023.06) OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2306.01738.pdf)
- (arXiv 2023.06) An Efficient Transformer for Simultaneous Learning of BEV and Lane Representations in 3D Lane Detection, [[Paper]](https://arxiv.org/pdf/2306.04927.pdf)
- (arXiv 2023.07) HeightFormer: Explicit Height Modeling without Extra Data for Camera-only 3D Object Detection in Bird鈥檚 Eye View, [[Paper]](https://arxiv.org/pdf/2307.13510.pdf)
- (arXiv 2023.08) UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation, [[Paper]](https://arxiv.org/pdf/2308.07732.pdf), [[Code]](https://github.com/Haiyang-W/UniTR)
- (arXiv 2023.09) FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal Consistent Transformer for 3D Objection, [[Paper]](https://arxiv.org/pdf/2309.05257.pdf)
- (arXiv 2023.10) Towards Generalizable Multi-Camera 3D Object Detection via Perspective Debiasing, [[Paper]](https://arxiv.org/pdf/2310.11346.pdf)
- (arXiv 2023.12) Towards Efficient 3D Object Detection in Bird's-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach, [[Paper]](https://arxiv.org/pdf/2312.00633.pdf)
- (arXiv 2023.12) BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2312.01696.pdf)
- (arXiv 2023.12) COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction, [[Paper]](https://arxiv.org/pdf/2312.01919.pdf)
- (arXiv 2023.12) Learned Fusion: 3D Object Detection using Calibration-Free Transformer Feature Fusion, [[Paper]](https://arxiv.org/pdf/2312.09082.pdf)
- (arXiv 2023.12) Diffusion-Based Particle-DETR for BEV Perception, [[Paper]](https://arxiv.org/pdf/2312.11578.pdf)
- (arXiv 2023.12) Lift-Attend-Splat: Bird's-eye-view camera-lidar fusion using transformers, [[Paper]](https://arxiv.org/pdf/2312.14919.pdf)
- (arXiv 2024.01) WidthFormer: Toward Efficient Transformer-based BEV View Transformation, [[Paper]](https://arxiv.org/pdf/2401.03836.pdf), [[Code]](https://github.com/ChenhongyiYang/WidthFormer)
- (arXiv 2024.02) OccTransformer: Improving BEVFormer for 3D camera-only occupancy prediction, [[Paper]](https://arxiv.org/pdf/2402.18140.pdf)
- (arXiv 2024.03) CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow,  [[Paper]](https://arxiv.org/pdf/2403.08919.pdf)

### Captioning
- (arXiv 2021.01)  CPTR: Full Transformer Network for Image Captioning, [[Paper]](https://arxiv.org/pdf/2101.10804.pdf)
- (arXiv 2021.01) Dual-Level Collaborative Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2101.06462.pdf)
- (arXiv.2021.02) VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining, [[Paper]](https://arxiv.org/pdf/2102.10407.pdf), [[Code]](https://github.com/Vision-CAIR/VisualGPT)
- (arXiv 2021.06) Semi-Autoregressive Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2106.09436.pdf), [[Code]](https://github.com/YuanEZhou/satic)
- (arXiv 2021.08) Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers, [[Paper]](https://arxiv.org/pdf/2108.02147.pdf)
- (arXiv 2021.08) Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning, [[Paper]](https://arxiv.org/pdf/2108.02366.pdf), [[Code]](https://github.com/Unbear430/DGCN-for-image-captioning)
- (arXiv 2021.09) Bornon: Bengali Image Captioning with Transformer-based Deep learning approach, [[Paper]](https://arxiv.org/pdf/2109.05218.pdf)
- (arXiv 2021.09) Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning, [[Paper]](https://arxiv.org/pdf/2109.07799.pdf), [[Code]](https://github.com/shikha-gist/Image-Captioning/) 
- (arXiv 2021.09) Geometry-Entangled Visual Semantic Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2109.14137.pdf)
- (arXiv 2021.10) Geometry Attention Transformer with Position-aware LSTMs for Image Captioning, [[Paper]](https://arxiv.org/pdf/2110.00335.pdf)
- (arXiv 2021.10) Bangla Image Caption Generation through CNN-Transformer based Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2110.12442.pdf)
- (arXiv 2021.11) SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning, [[Paper]](https://arxiv.org/pdf/2111.13196.pdf)
- (arXiv 2021.12) Injecting Semantic Concepts into End-to-End Image Captioning, [[Paper]](https://arxiv.org/pdf/2112.05230.pdf)
- (arXiv 2022.01) Compact Bidirectional Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2201.01984.pdf), [[Code]](https://github.com/YuanEZhou/CBTrans) 
- (arXiv 2022.02) ACORT: A Compact Object Relation Transformer for Parameter Efficient Image Captioning, [[Paper]](https://arxiv.org/pdf/2202.05451.pdf), [[Code]](https://github.com/jiahuei/sparse-image-captioning) 
- (arXiv 2022.02) Deep soccer captioning with transformer: dataset, semantics-related losses, and multi-level evaluation, [[Paper]](https://arxiv.org/pdf/2202.05728.pdf), [[Code]](https://sites.google.com/view/soccercaptioning) 
- (arXiv 2022.03) X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning, [[Paper]](https://arxiv.org/pdf/2203.00843.pdf)
- (arXiv 2022.03) End-to-End Transformer Based Model for Image Captioning, [[Paper]](https://arxiv.org/pdf/2203.15350.pdf)
- (arXiv 2022.03) Quantifying Societal Bias Amplification in Image Captioning, [[Paper]](https://arxiv.org/pdf/2203.15395.pdf)
- (arXiv 2022.04) Image Captioning In the Transformer Age, [[Paper]](https://arxiv.org/pdf/2204.07374.pdf)
- (arXiv 2022.05) Dual-Level Decoupled Transformer for Video Captioning, [[Paper]](https://arxiv.org/pdf/2205.03039.pdf)
- (arXiv 2022.05) Variational Transformer: A Framework Beyond the Trade-off between Accuracy and Diversity for Image Captioning, [[Paper]](https://arxiv.org/pdf/2205.14458.pdf), [[Code]](https://github.com/kaelsunkiller/VaT)
- (arXiv 2022.06) Transformer-Based Multi-modal Proposal and Re-Rank for Wikipedia Image-Caption Matching, [[Paper]](https://arxiv.org/pdf/2206.10436.pdf), [[Code]](https://github.com/mesnico/Wiki-Image-Caption-Matching)
- (arXiv 2022.07) ExpansionNet: exploring the sequence length bottleneck in the Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2207.03327.pdf), [[Code]](https://github.com/jchenghu/ExpansionNet)
- (arXiv 2022.07) GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features, [[Paper]](https://arxiv.org/pdf/2207.09666.pdf), [[Code]](https://github.com/davidnvq/grit)
- (arXiv 2022.07) Retrieval-Augmented Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2207.13162.pdf)
- (arXiv 2022.09) vieCap4H-VLSP 2021: Vietnamese Image Captioning for Healthcare Domain using Swin Transformer and Attention-based LSTM, [[Paper]](https://arxiv.org/pdf/2209.01304.pdf), [[Code]](https://git.io/JDdJm)
- (arXiv 2022.11) VieCap4H - VLSP 2021: ObjectAoA -- Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning, [[Paper]](https://arxiv.org/pdf/2211.05405.pdf)
- (arXiv 2022.11) VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning, [[Paper]](https://arxiv.org/pdf/2211.15103.pdf), [[Code]](https://github.com/UARK-AICV/VLTinT)
- (arXiv 2022.11) GRiT: A Generative Region-to-text Transformer for Object Understanding, [[Paper]](https://arxiv.org/pdf/2212.00280.pdf), [[Code]](https://github.com/JialianW/GRiT)
- (arXiv 2023.01) End-to-End 3D Dense Captioning with Vote2Cap-DETR, [[Paper]](https://arxiv.org/pdf/2301.02508.pdf), [[Code]](https://github.com/ch3cook-fdu/Vote2Cap-DETR)
- (arXiv 2023.02) ADAPT: Action-aware Driving Caption Transformer, [[Paper]](https://arxiv.org/pdf/2302.00673.pdf), [[Code]](https://github.com/jxbbb/ADAPT)
- (arXiv 2023.02) DEVICE: DEpth and VIsual ConcEpts Aware Transformer for TextCaps, [[Paper]](https://arxiv.org/pdf/2302.01540.pdf)
- (arXiv 2023.03) Neighborhood Contrastive Transformer for Change Captioning, [[Paper]](https://arxiv.org/pdf/2303.03171.pdf), [[Code]](https://github.com/tuyunbin/NCT)
- (arXiv 2023.03) Comparative study of Transformer and LSTM Network with attention mechanism on Image Captioning, [[Paper]](https://arxiv.org/pdf/2303.02648.pdf)
- (arXiv 2023.03) Text with Knowledge Graph Augmented Transformer for Video Captioning, [[Paper]](https://arxiv.org/pdf/2303.12423.pdf)
- (arXiv 2023.05) Transforming Visual Scene Graphs to Image Captions, [[Paper]](https://arxiv.org/pdf/2305.02177.pdf)
- (arXiv 2023.07) Embedded Heterogeneous Attention Transformer for Cross-lingual Image Captioning, [[Paper]](https://arxiv.org/pdf/2307.09915.pdf)
- (arXiv 2023.08) RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension, [[Paper]](https://arxiv.org/pdf/2308.02299.pdf), [[Code]](https://github.com/mightyzau/RegionBLIP)
- (arXiv 2023.08) Enhancing image captioning with depth information using a Transformer-based framework, [[Paper]](https://arxiv.org/pdf/2308.03767.pdf)
- (arXiv 2023.09) Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning, [[Paper]](https://arxiv.org/pdf/2309.02999.pdf), [[Code]](https://github.com/ch3cook-fdu/Vote2Cap-DETR)
- (arXiv 2023.09) Collaborative Three-Stream Transformers for Video Captioning, [[Paper]](https://arxiv.org/pdf/2309.09611.pdf), [[Code]](https://github.com/ch3cook-fdu/Vote2Cap-DETR)
- (arXiv 2023.09) Accurate and Fast Compressed Video Captioning, [[Paper]](https://arxiv.org/pdf/2309.12867.pdf), [[Code]](https://github.com/acherstyx/CoCap)

### Change Detection
- (arXiv 2022.01) A Transformer-Based Siamese Network for Change Detection, [[Paper]](https://arxiv.org/pdf/2201.01293.pdf), [[Code]](https://github.com/wgcban/ChangeFormer)
- (arXiv 2022.07) IDET: Iterative Difference-Enhanced Transformers for High-Quality Change Detection, [[Paper]](https://arxiv.org/pdf/2207.09240.pdf)
- (arXiv 2023.08) UCDFormer: Unsupervised Change Detection Using a Transformer-driven Image Translation, [[Paper]](https://arxiv.org/pdf/2308.01146.pdf), [[Code]](https://github.com/zhu-xlab/UCDFormer)
- (arXiv 2023.09) Changes-Aware Transformer: Learning Generalized Changes Representation, [[Paper]](https://arxiv.org/pdf/2309.13619.pdf)
- (arXiv 2023.10) Transformer-based Multimodal Change Detection with Multitask Consistency Constraints, [[Paper]](https://arxiv.org/pdf/2310.09276.pdf)
- (arXiv 2023.10) TransY-Net:Learning Fully Transformer Networks for Change Detection of Remote Sensing Images, [[Paper]](https://arxiv.org/pdf/2310.14214.pdf), [[Code]](https://github.com/Drchip61/TransYNet)
- (arXiv 2023.11) MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations, [[Paper]](https://arxiv.org/pdf/2311.09726.pdf), [[Code]](https://github.com/guanyuezhen/MS-Former)
- (arXiv 2023.12) Adapting Vision Transformer for Efficient Change Detection, [[Paper]](https://arxiv.org/pdf/2312.04869.pdf)

### Classification (Backbone)
- (ICLR'21) MODELING LONG-RANGE INTERACTIONS WITHOUT ATTENTION, [[Paper]](https://openreview.net/pdf?id=xTJEN-ggl1b), [[Code]](https://github.com/lucidrains/lambda-networks)
- (CVPR'20) Feature Pyramid Transformer, [[Paper]](https://arxiv.org/pdf/2007.09451), [[Code]](https://github.com/ZHANGDONG-NJUST/FPT)
- (ICLR'21) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, [[Paper]](https://arxiv.org/pdf/2010.11929), [[Code]](https://github.com/google-research/vision_transformer)
- (arXiv 2020.06) Visual Transformers: Token-based Image Representation and Processing for Computer Vision, [[Paper]](https://arxiv.org/pdf/2006.03677)
- (arXiv 2020.12) Training data-efficient image transformers & distillation through attention, [[Paper]](https://arxiv.org/abs/2012.12877), [[Code]](https://github.com/facebookresearch/deit)
- (arXiv 2021.01) Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, [[Paper]](https://arxiv.org/pdf/2101.11986.pdf), [[Code]](https://github.com/yitu-opensource/T2T-ViT)
- (arXiv 2021.01) Bottleneck Transformers for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2101.11605.pdf) , [[Code]](https://github.com/leaderj1001/BottleneckTransformers)
- (arXiv.2021.02) Conditional Positional Encodings for Vision Transformers, [[Paper]](https://arxiv.org/abs/2102.10882), [[Code]](https://github.com/Meituan-AutoML/CPVT)
- (arXiv.2021.02) Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12122.pdf), [[Code]](https://github.com/whai362/PVT)
- (arXiv 2021.03) Transformer in Transformer, [[Paper]](https://arxiv.org/pdf/2103.00112.pdf), [[Code]](https://github.com/huawei-noah/noah-research/tree/master/TNT) 
- (arXiv 2021.03) ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases, [[Paper]](https://arxiv.org/pdf/2103.10697.pdf), [[Code]](https://github.com/facebookresearch/convit)
- (arXiv 2021.03) Scalable Visual Transformers with Hierarchical Pooling, [[Paper]](https://arxiv.org/pdf/2103.10619.pdf)
- (arXiv 2021.03) Incorporating Convolution Designs into Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.11816.pdf)
- (arXiv 2021.03) DeepViT: Towards Deeper Vision Transformer, [[Paper]](https://arxiv.org/pdf/2103.11886.pdf), [[Code]](https://github.com/zhoudaquan/dvit_repo)
- (arXiv 2021.03) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, [[Paper]](https://arxiv.org/pdf/2103.14030.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer) 
- (arXiv 2021.03) Understanding Robustness of Transformers for Image Classification, [[Paper]](https://arxiv.org/abs/2103.14586)
- (arXiv 2021.03) Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding, [[Paper]](https://arxiv.org/abs/2103.15358)
- (arXiv 2021.03) CvT: Introducing Convolutions to Vision Transformers, [[Paper]](https://arxiv.org/abs/2103.15808), [[Code]](https://github.com/leoxiaobin/CvT)
- (arXiv 2021.03) Rethinking Spatial Dimensions of Vision Transformers, [[Paper]](https://arxiv.org/abs/2103.16302), [[Code]](https://github.com/naver-ai/pit)
- (arXiv 2021.03) Going deeper with Image Transformers, [[Paper]](https://arxiv.org/abs/2103.17239)
- (arXiv 2021.04) LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference, [[Paper]](https://arxiv.org/abs/2104.01136)
- (arXiv 2021.04) On the Robustness of Vision Transformers to Adversarial Examples, [[Paper]](https://arxiv.org/abs/2104.02610)
- (arXiv 2021.04) LocalViT: Bringing Locality to Vision Transformers, [[Paper]](https://arxiv.org/abs/2104.05704), [[Code]](https://github.com/ofsoundof/LocalViT)
- (arXiv 2021.04) Escaping the Big Data Paradigm with Compact Transformers, [[Paper]](https://arxiv.org/abs/2104.05707), [[Code]](https://github.com/SHI-Labs/Compact-Transformers)
- (arXiv 2021.04) Co-Scale Conv-Attentional Image Transformers, [[Paper]](https://arxiv.org/abs/2104.06399), [[Code]](https://github.com/mlpc-ucsd/CoaT)
- (arXiv 2021.04) Token Labeling: Training a 85.5% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet, [[Paper]](https://arxiv.org/pdf/2104.10858.pdf), [[Code]](https://github.com/zihangJiang/TokenLabeling)
- (arXiv 2021.04) So-ViT: Mind Visual Tokens for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2104.10935.pdf)
- (arXiv 2021.04) Multiscale Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.11227.pdf), [[Code]](https://github.com/facebookresearch/SlowFast)
- (arXiv 2021.04) Visformer: The Vision-friendly Transformer, [[Paper]](https://arxiv.org/pdf/2104.12533.pdf), [[Code]](https://github.com/danczs/Visformer)
- (arXiv 2021.04) Improve Vision Transformers Training by Suppressing Over-smoothing, [[Paper]](https://arxiv.org/pdf/2104.12753.pdf), [[Code]](https://github.com/ChengyueGongR/PatchVisionTransformer)
- (arXiv 2021.04) Twins: Revisiting the Design of Spatial Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.13840.pdf), [[Code]](https://github.com/Meituan-AutoML/Twins)
- (arXiv 2021.04) ConTNet: Why not use convolution and transformer at the same time, [[Paper]](https://arxiv.org/pdf/2104.13497.pdf), [[Code]](https://github.com/yanhao-tian/ConTNet)
- (arXiv 2021.05) Rethinking the Design Principles of Robust Vision Transformer, [[Paper]](https://arxiv.org/pdf/2105.07926.pdf), [[Code]](https://github.com/vtddggg/Robust-Vision-Transformer)
- (arXiv 2021.05) Vision Transformers are Robust Learners, [[Paper]](https://arxiv.org/pdf/2105.07581.pdf), [[Code]](https://git.io/J3VO0)
- (arXiv 2021.05) Rethinking Skip Connection with Layer Normalization in Transformers and ResNets, [[Paper]](https://arxiv.org/pdf/2105.07581.pdf), [[Code]](https://git.io/J3VO0)
- (arXiv 2021.05) Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead, [[Paper]](https://arxiv.org/pdf/2105.09121.pdf)
- (arXiv 2021.05) Intriguing Properties of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2105.10497.pdf), [[Code]](https://git.io/Js15X)
- (arXiv 2021.05) Aggregating Nested Transformers, [[Paper]](https://arxiv.org/pdf/2105.12723.pdf)
- (arXiv 2021.05) ResT: An Efficient Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2105.13677.pdf), [[Code]](https://github.com/wofmanaf/ResT)
- (arXiv 2021.06) DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification, [[Paper]](https://arxiv.org/pdf/2106.02034.pdf), [[Code]](https://github.com/raoyongming/DynamicViT)
- (arXiv 2021.06) When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations, [[Paper]](https://arxiv.org/pdf/2106.01548.pdf)
- (arXiv 2021.06) Container: Context Aggregation Network, [[Paper]](https://arxiv.org/pdf/2106.01401.pdf)
- (arXiv 2021.06) TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classication, [[Paper]](https://arxiv.org/pdf/2106.00908.pdf)
- (arXiv 2021.06) KVT: k-NN Attention for Boosting Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.00515.pdf)
- (arXiv 2021.06) MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens, [[Paper]](https://arxiv.org/pdf/2105.15168.pdf), [[Code]](https://github.com/hustvl/MSG-Transformer) 
- (arXiv 2021.06) Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length, [[Paper]](https://arxiv.org/pdf/2105.15075.pdf)
- (arXiv 2021.06) Less is More: Pay Less Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2105.14217.pdf)
- (arXiv 2021.06) FoveaTer: Foveated Transformer for Image Classification, [[Paper]](https://arxiv.org/pdf/2105.14173.pdf)
- (arXiv 2021.06) An Attention Free Transformer, [[Paper]](https://arxiv.org/pdf/2105.14103.pdf)
- (arXiv 2021.06) Glance-and-Gaze Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.02277.pdf), [[Code]](https://github.com/yucornetto/GG-Transformer)
- (arXiv 2021.06) RegionViT: Regional-to-Local Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.02689.pdf)
- (arXiv 2021.06) Chasing Sparsity in Vision Transformers: An End-to-End Exploration, [[Paper]](https://arxiv.org/pdf/2106.04533.pdf), [[Code]](https://github.com/VITA-Group/SViTE)
- (arXiv 2021.06) Scaling Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.04560.pdf)
- (arXiv 2021.06) CAT: Cross Attention in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.05786.pdf), [[Code]](https://github.com/linhezheng19/CAT)
- (arXiv 2021.06) On Improving Adversarial Transferability of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.04169.pdf), [[Code]](https://git.io/JZmG3)
- (arXiv 2021.06) Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight, [[Paper]](https://arxiv.org/pdf/2106.04263.pdf)
- (arXiv 2021.06) Patch Slimming for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.02852.pdf)
- (arXiv 2021.06) Transformer in Convolutional Neural Networks, [[Paper]](https://arxiv.org/pdf/2106.03180.pdf), [[Code]](https://github.com/yun-liu/TransCNN)
- (arXiv 2021.06) ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias, [[Paper]](https://arxiv.org/pdf/2106.03348.pdf), [[Code]](https://github.com/Annbless/ViTAE)
- (arXiv 2021.06) Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.03650.pdf)
- (arXiv 2021.06) Refiner: Refining Self-attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.03714.pdf)
- (arXiv 2021.06) Reveal of Vision Transformers Robustness against Adversarial Attacks, [[Paper]](https://arxiv.org/pdf/2106.03734.pdf)
- (arXiv 2021.06) Efficient Training of Visual Transformers with Small-Size Datasets, [[Paper]](https://arxiv.org/pdf/2106.03746.pdf)
- (arXiv 2021.06) Delving Deep into the Generalization of Vision Transformers under Distribution Shifts, [[Paper]](https://arxiv.org/pdf/2106.07617.pdf)
- (arXiv 2021.06) BEIT: BERT Pre-Training of Image Transformers, [[Paper]](https://arxiv.org/pdf/2106.08254.pdf), [[Code]](https://aka.ms/beit)
- (arXiv 2021.06) XCiT: Cross-Covariance Image Transformers, [[Paper]](https://arxiv.org/pdf/2106.09681.pdf), [[Code]](https://github.com/facebookresearch/xcit)
- (arXiv 2021.06) How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.10270.pdf), [[Code1]](https://github.com/google-research/vision_transformer), [[Code2]](https://github.com/rwightman/pytorch-image-models)
- (arXiv 2021.06) Exploring Vision Transformers for Fine-grained Classification, [[Paper]](https://arxiv.org/pdf/2106.10587.pdf), [[Code]](https://github.com/mv-lab/ViT-FGVC8)
- (arXiv 2021.06) TokenLearner: What Can 8 Learned Tokens Do for Images and Videos, [[Paper]](https://arxiv.org/pdf/2106.11297.pdf)
- (arXiv 2021.06) Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers, [[Paper]](https://arxiv.org/pdf/2106.13122.pdf), [[Code]](https://github.com/katelyn98/CorruptionRobustness)
- (arXiv 2021.06) VOLO: Vision Outlooker for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2106.13112.pdf), [[Code]](https://github.com/sail-sg/volo)
- (arXiv 2021.06) IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.12620.pdf), [[Project]](http://people.csail.mit.edu/bpan/ia-red/)
- (arXiv 2021.06) PVTv2: Improved Baselines with Pyramid Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.13797.pdf), [[Code]](https://github.com/whai362/PVT)
- (arXiv 2021.06) Early Convolutions Help Transformers See Better, [[Paper]](https://arxiv.org/pdf/2106.14881.pdf)
- (arXiv 2021.06) Multi-Exit Vision Transformer for Dynamic Inference, [[Paper]](https://arxiv.org/pdf/2106.15183.pdf)
- (arXiv 2021.07) Augmented Shortcuts for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.15941.pdf)
- (arXiv 2021.07) Improving the Efficiency of Transformers for Resource-Constrained Devices, [[Paper]](https://arxiv.org/pdf/2106.16006.pdf)
- (arXiv 2021.07) CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows, [[Paper]](https://arxiv.org/pdf/2107.00652.pdf), [[Code]](https://github.com/microsoft/CSWin-Transformer)
- (arXiv 2021.07) Focal Self-attention for Local-Global Interactions in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.00641.pdf)
- (arXiv 2021.07) Cross-view Geo-localization with Evolving Transformer, [[Paper]](https://arxiv.org/pdf/2107.00842.pdf)
- (arXiv 2021.07) What Makes for Hierarchical Vision Transformer, [[Paper]](https://arxiv.org/pdf/2107.02174.pdf)
- (arXiv 2021.07) Efficient Vision Transformers via Fine-Grained Manifold Distillation, [[Paper]](https://arxiv.org/pdf/2107.01378.pdf)
- (arXiv 2021.07) Vision Xformers: Efficient Attention for Image Classification, [[Paper]](https://arxiv.org/pdf/2107.02239.pdf)
- (arXiv 2021.07) Long-Short Transformer: Efficient Transformers for Language and Vision, [[Paper]](https://arxiv.org/pdf/2107.02192.pdf)
- (arXiv 2021.07) Feature Fusion Vision Transformer for Fine-Grained Visual Categorization, [[Paper]](https://arxiv.org/pdf/2107.02341.pdf)
- (arXiv 2021.07) Local-to-Global Self-Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.04735.pdf), [[Code]](https://github.com/ljpadam/LG-Transformer)
- (arXiv 2021.07) Visual Parser: Representing Part-whole Hierarchies with Transformers, [[Paper]](https://arxiv.org/pdf/2107.05790.pdf), [[Code]](https://github.com/kevin-ssy/ViP)
- (arXiv 2021.07) CMT: Convolutional Neural Networks Meet Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.06263.pdf)
- (arXiv 2021.07) Combiner: Full Attention Transformer with Sparse Computation Cost, [[Paper]](https://arxiv.org/pdf/2107.05768.pdf)
- (arXiv 2021.07) A Comparison of Deep Learning Classification Methods on Small-scale Image Data set: from Convolutional Neural Networks to Visual Transformers, [[Paper]](https://arxiv.org/pdf/2107.07699.pdf)
- (arXiv 2021.07) Contextual Transformer Networks for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2107.12292.pdf), [[Code]](https://github.com/JDAI-CV/CoTNet)
- (arXiv 2021.07) Rethinking and Improving Relative Position Encoding for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2107.14222.pdf), [[Code]](https://github.com/microsoft/AutoML/tree/main/iRPE)
- (arXiv 2021.08) CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention, [[Paper]](https://arxiv.org/pdf/2108.00154.pdf), [[Code]](https://github.com/cheerss/CrossFormer)
- (arXiv 2021.08) Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer, [[Paper]](https://arxiv.org/pdf/2108.01390.pdf)
- (arXiv 2021.08) Vision Transformer with Progressive Sampling, [[Paper]](https://arxiv.org/pdf/2108.01684.pdf), [[Code]](https://github.com/yuexy/PS-ViT)
- (arXiv 2021.08) Armour: Generalizable Compact Self-Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.01778.pdf)
- (arXiv 2021.08) ConvNets vs. Transformers: Whose Visual Representations are More Transferable, [[Paper]](https://arxiv.org/pdf/2108.05305.pdf)
- (arXiv 2021.08) Mobile-Former: Bridging MobileNet and Transformer, [[Paper]](https://arxiv.org/pdf/2108.05895.pdf)
- (arXiv 2021.08) Do Vision Transformers See Like Convolutional Neural Networks, [[Paper]](https://arxiv.org/pdf/2108.08810.pdf)
- (arXiv 2021.08) Exploring and Improving Mobile Level Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.13015.pdf)
- (arXiv 2021.08) A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP, [[Paper]](https://arxiv.org/pdf/2108.13002.pdf)
- (arXiv 2021.08) Scaled ReLU Matters for Training Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.03810.pdf)
- (arXiv 2021.09) Towards Transferable Adversarial Attacks on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.04176.pdf)
- (arXiv 2021.09) DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Transformers, [[Paper]](https://arxiv.org/pdf/2109.10060.pdf), [[Code]](https://github.com/changlin31/DS-Net)
- (arXiv 2021.09) Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers, [[Paper]](https://arxiv.org/pdf/2109.10686.pdf)
- (arXiv 2021.09) Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models, [[Paper]](https://arxiv.org/pdf/2109.13925.pdf)
- (arXiv 2021.09) UFO-ViT: High Performance Linear Vision Transformer without Softmax, [[Paper]](https://arxiv.org/pdf/2109.14382.pdf)
- (arXiv 2021.10) MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer, [[Paper]](https://arxiv.org/pdf/2110.02178.pdf)
- (arXiv 2021.10) Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs, [[Paper]](https://arxiv.org/pdf/2110.02797.pdf), [[Code]](https://github.com/phibenz/robustness_comparison_vit_mlp-mixer_cnn)
- (arXiv 2021.10) Token Pooling in Visual Transformers, [[Paper]](https://arxiv.org/pdf/2110.03860.pdf)
- (arXiv 2021.10) NViT: Vision Transformer Compression and Parameter Redistribution, [[Paper]](https://arxiv.org/pdf/2110.04869.pdf)
- (arXiv 2021.10) Adversarial Token Attacks on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2110.04337.pdf)
- (arXiv 2021.10) Certified Patch Robustness via Smoothed Vision Transformers, [[Paper]](https://arxiv.org/pdf/2110.07719.pdf), [[Code]](https://github.com/MadryLab/smoothed-vit)
- (arXiv 2021.10) Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation, [[Paper]](https://arxiv.org/pdf/2110.07858.pdf)
- (arXiv 2021.10) SOFT: Softmax-free Transformer with Linear Complexity, [[Paper]](https://arxiv.org/pdf/2110.11945.pdf), [[Code]](https://github.com/fudan-zvg/SOFT)
- (arXiv 2021.10) Blending Anti-Aliasing into Vision Transformer, [[Paper]](https://arxiv.org/pdf/2110.15156.pdf), [[Code]](https://github.com/amazon-research/anti-aliasing-transformer)
- (arXiv 2021.11) Can Vision Transformers Perform Convolution, [[Paper]](https://arxiv.org/pdf/2111.01353.pdf)
- (arXiv 2021.11) Sliced Recursive Transformer, [[Paper]](https://arxiv.org/pdf/2111.05297.pdf), [[Code]](https://github.com/szq0214/SReT)
- (arXiv 2021.11) Hybrid BYOL-ViT: Efficient approach to deal with small Datasets, [[Paper]](https://arxiv.org/pdf/2111.04845.pdf)
- (arXiv 2021.11) Are Transformers More Robust Than CNNs, [[Paper]](https://arxiv.org/pdf/2111.05464.pdf), [[Code]](https://github.com/ytongbai/ViTs-vs-CNNs)
- (arXiv 2021.11) iBOT: Image BERT Pre-Training with Online Tokenizer, [[Paper]](https://arxiv.org/pdf/2111.07832.pdf)
- (arXiv 2021.11) Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding, [[Paper]](https://arxiv.org/pdf/2111.08413.pdf)
- (arXiv 2021.11) TransMix: Attend to Mix for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.09833.pdf), [[Code]](https://github.com/Beckschen/TransMix)
- (arXiv 2021.11) Swin Transformer V2: Scaling Up Capacity and Resolution, [[Paper]](https://arxiv.org/pdf/2111.09883.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer)
- (arXiv 2021.11) Are Vision Transformers Robust to Patch Perturbations, [[Paper]](https://arxiv.org/pdf/2111.10659.pdf)
- (arXiv 2021.11) Discrete Representations Strengthen Vision Transformer Robustness, [[Paper]](https://arxiv.org/pdf/2111.10493.pdf)
- (arXiv 2021.11) Zero-Shot Certified Defense against Adversarial Patches with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.10481.pdf)
- (arXiv 2021.11) MetaFormer is Actually What You Need for Vision, [[Paper]](https://arxiv.org/pdf/2111.11418.pdf), [[Code]](https://github.com/sail-sg/poolformer)
- (arXiv 2021.11) DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion, [[Paper]](https://arxiv.org/pdf/2111.11326.pdf), [[Code]](https://github.com/arthurdouillard/dytox)
- (arXiv 2021.11) Mesa: A Memory-saving Training Framework for Transformers, [[Paper]](https://arxiv.org/pdf/2111.11124.pdf), [[Code]](https://github.com/zhuang-group/Mesa)
- (arXiv 2021.11) Semi-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.11067.pdf)
- (arXiv 2021.11) DBIA: Data-free Backdoor Injection Attack against Transformer Networks, [[Paper]](https://arxiv.org/pdf/2111.11870.pdf), [[Code]](https://anonymous.4open.science/r/DBIA-825D)
- (arXiv 2021.11) Self-slimmed Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.12624.pdf)
- (arXiv 2021.11) PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12710.pdf), [[Code]](https://github.com/microsoft/PeCo)
- (arXiv 2021.11) SWAT: Spatial Structure Within and Among Tokens, [[Paper]](https://arxiv.org/pdf/2111.13677.pdf)
- (arXiv 2021.11) NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2111.12994.pdf), [[Code]](https://github.com/NomMer1125/NomMer)
- (arXiv 2021.11) Global Interaction Modelling in Vision Transformer via Super Tokens, [[Paper]](https://arxiv.org/pdf/2111.13156.pdf)
- (arXiv 2021.11) ATS: Adaptive Token Sampling For Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.15667.pdf)
- (arXiv 2021.11) Pyramid Adversarial Training Improves ViT Performance, [[Paper]](https://arxiv.org/pdf/2111.15121.pdf)
- (arXiv 2021.12) Improved Multiscale Vision Transformers for Classification and Detection, [[Paper]](https://arxiv.org/pdf/2112.01526.pdf)
- (arXiv 2021.12) Make A Long Image Short: Adaptive Token Length for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.01686.pdf)
- (arXiv 2021.12) Dynamic Token Normalization Improves Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.02624.pdf), [[Code]](https://github.com/wqshao126/DTN)
- (arXiv 2021.12) Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training, [[Paper]](https://arxiv.org/pdf/2112.03552.pdf)
- (arXiv 2021.12) Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal, [[Paper]](https://arxiv.org/pdf/2112.03492.pdf), [[Code]](https://github.com/shiyuchengtju/par)
- (arXiv 2021.12) Visual Transformers with Primal Object Queries for Multi-Label Image Classification, [[Paper]](https://arxiv.org/pdf/2112.05485.pdf)
- (arXiv 2021.12) Couplformer:Rethinking Vision Transformer with Coupling Attention Map, [[Paper]](https://arxiv.org/pdf/2112.05425.pdf)
- (arXiv 2021.12) AdaViT: Adaptive Tokens for Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.07658.pdf)
- (arXiv 2021.12) Lite Vision Transformer with Enhanced Self-Attention, [[Paper]](https://arxiv.org/pdf/2112.10809.pdf), [[Code]](https://github.com/Chenglin-Yang/LVT)
- (arXiv 2021.12) Learned Queries for Efficient Local Attention, [[Paper]](https://arxiv.org/pdf/2112.11435.pdf), [[Code]](https://github.com/moabarar/qna)
- (arXiv 2021.12) MPViT: Multi-Path Vision Transformer for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2112.11010.pdf), [[Code]](https://github.com/youngwanLEE/MPViT)
- (arXiv 2021.12) MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation, [[Paper]](https://arxiv.org/pdf/2112.11542.pdf)
- (arXiv 2021.12) ELSA: Enhanced Local Self-Attention for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.12786.pdf), [[Code]](https://github.com/damo-cv/ELSA)
- (arXiv 2021.12) SimViT: Exploring a Simple Vision Transformer with sliding windows, [[Paper]](https://arxiv.org/pdf/2112.13085.pdf), [[Code]](https://github.com/ucasligang/SimViT)
- (arXiv 2021.12) Vision Transformer for Small-Size Datasets, [[Paper]](https://arxiv.org/pdf/2112.13492.pdf)
- (arXiv 2021.12) ViR: the Vision Reservoir, [[Paper]](https://arxiv.org/pdf/2112.13545.pdf)
- (arXiv 2021.12) Augmenting Convolutional networks with attention-based aggregation, [[Paper]](https://arxiv.org/pdf/2112.13692.pdf)
- (arXiv 2021.12) Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention, [[Paper]](https://arxiv.org/pdf/2112.14000.pdf), [[Code]](https://github.com/BR-IDL/PaddleViT)
- (arXiv 2021.12) SPViT: Enabling Faster Vision Transformers via Soft Token Pruning, [[Paper]](https://arxiv.org/pdf/2112.13890.pdf)
- (arXiv 2021.12) Stochastic Layers in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.15111.pdf)
- (arXiv 2022.01) Vision Transformer with Deformable Attention, [[Paper]](https://arxiv.org/pdf/2201.00520.pdf), [[Code]](https://github.com/LeapLabTHU/DAT)
- (arXiv 2022.01) PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture, [[Paper]](https://arxiv.org/pdf/2201.00978.pdf), [[Code]](https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch)
- (arXiv 2022.01) QuadTree Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2201.02767.pdf), [[Code]](https://github.com/Tangshitao/QuadtreeAttention)
- (arXiv 2022.01) TerViT: An Efficient Ternary Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.08050.pdf)
- (arXiv 2022.01) UniFormer: Unifying Convolution and Self-attention for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2201.09450.pdf), [[Code]](https://github.com/Sense-X/UniFormer)
- (arXiv 2022.01) Patches Are All You Need?, [[Paper]](https://arxiv.org/pdf/2201.09792.pdf), [[Code]](https://github.com/locuslab/convmixer)
- (arXiv 2022.01) Convolutional Xformers for Vision, [[Paper]](https://arxiv.org/pdf/2201.10271.pdf), [[Code]](https://github.com/pranavphoenix/CXV)
- (arXiv 2022.01) When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism, [[Paper]](https://arxiv.org/pdf/2201.10801.pdf), [[Code]](https://github.com/microsoft/SPACH)
- (arXiv 2022.01) Training Vision Transformers with Only 2040 Images, [[Paper]](https://arxiv.org/pdf/2201.10728.pdf)
- (arXiv 2022.01) O-ViT: Orthogonal Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.12133.pdf)
- (arXiv 2022.01) Aggregating Global Features into Local Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.12903.pdf),[[Code]](https://github.com/krushi1992/MOA-transformer)
- (arXiv 2022.01) BOAT: Bilateral Local Attention Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.13027.pdf)
- (arXiv 2022.02) BViT: Broad Attention based Vision Transformer, [[Paper]](https://arxiv.org/pdf/2202.06268.pdf),[[Code]](https://github.com/DRL-CASIA/Broad_ViT)
- (arXiv 2022.02) How Do Vision Transformers Work, [[Paper]](https://arxiv.org/pdf/2202.06709.pdf),[[Code]](https://github.com/xxxnell/how-do-vits-work)
- (arXiv 2022.02) Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations, [[Paper]](https://arxiv.org/pdf/2202.07800.pdf),[[Code]](https://github.com/youweiliang/evit)
- (arXiv 2022.02) ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond, [[Paper]](https://arxiv.org/pdf/2202.10108.pdf)
- (arXiv 2022.02) Learning to Merge Tokens in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2202.12015.pdf)
- (arXiv 2022.02) Auto-scaling Vision Transformers without Training, [[Paper]](https://arxiv.org/pdf/2202.11921.pdf),[[Code]](https://github.com/VITA-Group/AsViT)
- (arXiv 2022.03) Aggregated Pyramid Vision Transformer: Split-transform-merge Strategy for Image Recognition without Convolutions, [[Paper]](https://arxiv.org/pdf/2203.00960.pdf)
- (arXiv 2022.03) D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale Attention, [[Paper]](https://arxiv.org/pdf/2203.00860.pdf)
- (arXiv 2022.03) BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning, [[Paper]](https://arxiv.org/pdf/2203.01522.pdf)
- (arXiv 2022.03) Multi-Tailed Vision Transformer for Efficient Inference, [[Paper]](https://arxiv.org/pdf/2203.01587.pdf)
- (arXiv 2022.03) ViT-P: Rethinking Data-efficient Vision Transformers from Locality, [[Paper]](https://arxiv.org/pdf/2203.02358.pdf)
- (arXiv 2022.03) Coarse-to-Fine Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.03821.pdf),[[Code]](https://github.com/ChenMnZ/CF-ViT)
- (arXiv 2022.03) Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention, [[Paper]](https://arxiv.org/pdf/2203.03937.pdf)
- (arXiv 2022.03) EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.03952.pdf)
- (arXiv 2022.03) WaveMix: Resource-efficient Token Mixing for Images, [[Paper]](https://arxiv.org/pdf/2203.03689.pdf), [[Code]](https://github.com/pranavphoenix/WaveMix)
- (arXiv 2022.03) Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice, [[Paper]](https://arxiv.org/pdf/2203.05962.pdf), [[Code]](https://github.com/VITA-Group/ViT-Anti-Oversmoothing)
- (arXiv 2022.03) Visualizing and Understanding Patch Interactions in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.05922.pdf)
- (arXiv 2022.03) EIT: Efficiently Lead Inductive Biases to ViT, [[Paper]](https://arxiv.org/pdf/2203.07116.pdf), [[Code]](https://github.com/MrHaiPi/EIT)
- (arXiv 2022.03) The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy, [[Paper]](https://arxiv.org/pdf/2203.06345.pdf), [[Code]](https://github.com/VITA-Group/Diverse-ViT)
- (arXiv 2022.03) Towards Practical Certifiable Patch Defense with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.08519.pdf)
- (arXiv 2022.03) Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations, [[Paper]](https://arxiv.org/pdf/2203.08392.pdf), [[Code]](https://github.com/RICE-EIC/Patch-Fool)
- (arXiv 2022.03) Are Vision Transformers Robust to Spurious Correlations, [[Paper]](https://arxiv.org/pdf/2203.09125.pdf), [[Code]](https://github.com/deeplearning-wisc/vit-spurious-robustness)
- (arXiv 2022.03) Three things everyone should know about Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.09125.pdf)
- (arXiv 2022.03) ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.10790.pdf)
- (arXiv 2022.03) GradViT: Gradient Inversion of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.11894.pdf), [[Code]](https://gradvit.github.io/)
- (arXiv 2022.03) Learning Patch-to-Cluster Attention in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.11987.pdf)
- (arXiv 2022.03) Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization, [[Paper]](https://arxiv.org/pdf/2203.13167.pdf)
- (arXiv 2022.03) Beyond Fixation: Dynamic Window Visual Transformer, [[Paper]](https://arxiv.org/pdf/2203.12856.pdf), [[Code]](https://github.com/pzhren/DW-ViT)
- (arXiv 2022.03) Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness, [[Paper]](https://arxiv.org/pdf/2203.13639.pdf)
- (arXiv 2022.03) Automated Progressive Learning for Efficient Training of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.14509.pdf), [[Code]](https://github.com/dvlab-research/Stratified-Transformer)
- (arXiv 2022.03) Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.14313.pdf), [[Code]](https://github.com/sunsmarterjie/beyond_masking)
- (arXiv 2022.03) CaCo: Both Positive and Negative Samples are Directly Learnable via Cooperative-adversarial Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2203.14370.pdf), [[Code]](https://github.com/maple-research-lab/caco)
- (arXiv 2022.03) SepViT: Separable Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.15380.pdf)
- (arXiv 2022.03) Fine-tuning Image Transformers using Learnable Memory, [[Paper]](https://arxiv.org/pdf/2203.15243.pdf)
- (arXiv 2022.03) Parameter-efficient Fine-tuning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.16329.pdf)
- (arXiv 2022.03) MaxViT: Multi-Axis Vision Transformer, [[Paper]](https://arxiv.org/pdf/2204.0169.pdf)
- (arXiv 2022.04) BatchFormerV2: Exploring Sample Relationships for Dense Representation Learning, [[Paper]](https://arxiv.org/pdf/2204.01254.pdf)
- (arXiv 2022.04) Improving Vision Transformers by Revisiting High-frequency Components, [[Paper]](https://arxiv.org/pdf/2204.00993.pdf)
- (arXiv 2022.04) MixFormer: Mixing Features across Windows and Dimensions, [[Paper]](https://arxiv.org/pdf/2204.02557.pdf), [[Code]](https://github.com/PaddlePaddle/PaddleClas)
- (arXiv 2022.04) DaViT: Dual Attention Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.03645.pdf), [[Code]](https://github.com/dingmyu/davit)
- (arXiv 2022.04) Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels, [[Paper]](https://arxiv.org/pdf/2204.04905.pdf)
- (arXiv 2022.04) MiniViT: Compressing Vision Transformers with Weight Multiplexing, [[Paper]](https://arxiv.org/pdf/2204.07154.pdf)
- (arXiv 2022.04) DeiT III: Revenge of the ViT, [[Paper]](https://arxiv.org/pdf/2204.07118.pdf)
- (arXiv 2022.04) Neighborhood Attention Transformer, [[Paper]](https://arxiv.org/pdf/2204.07143.pdf), [[Code]](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)
- (arXiv 2022.04) ResT V2: Simpler, Faster and Stronger, [[Paper]](https://arxiv.org/pdf/2204.07366.pdf), [[Code]](https://github.com/wofmanaf/ResT)
- (arXiv 2022.04) VSA: Learning Varied-Size Window Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.08446.pdf), [[Code]](https://github.com/ViTAE-Transformer/ViTAE-VSA)
- (arXiv 2022.04) OCFormer: One-Class Transformer Network for Image Classification, [[Paper]](https://arxiv.org/pdf/2204.11449.pdf)
- (arXiv 2022.04) Adaptive Split-Fusion Transformer, [[Paper]](https://arxiv.org/pdf/2204.11449.pdf), [[Code]](https://github.com/szx503045266/ASF-former)
- (arXiv 2022.04) Understanding The Robustness in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.12451.pdf), [[Code]](https://github.com/NVlabs/FAN)
- (arXiv 2022.05) Better plain ViT baselines for ImageNet-1k, [[Paper]](https://arxiv.org/pdf/2205.01580.pdf), [[Code]](https://github.com/google-research/big_vision)
- (arXiv 2022.05) EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.03436.pdf), [[Code]](https://github.com/google-research/big_vision)
- (arXiv 2022.05) ConvMAE: Masked Convolution Meets Masked Autoencoders, [[Paper]](https://arxiv.org/pdf/2205.03892.pdf), [[Code]](https://github.com/Alpha-VL/ConvMAE)
- (arXiv 2022.05) Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.08078.pdf)
- (arXiv 2022.05) TRT-ViT: TensorRT-oriented Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.09579.pdf)
- (arXiv 2022.05) Super Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.11397.pdf), [[Code]](https://github.com/lmbxmu/SuperViT)
- (arXiv 2022.05) Deeper vs Wider: A Revisit of Transformer Configuration, [[Paper]](https://arxiv.org/pdf/2205.10505.pdf)
- (arXiv 2022.05) Vision Transformers in 2022: An Update on Tiny ImageNet, [[Paper]](https://arxiv.org/pdf/2205.10660.pdf), [[Code]](https://github.com/ehuynh1106/TinyImageNet-Transformers)
- (arXiv 2022.05) Privacy-Preserving Image Classification Using Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.12041.pdf)
- (arXiv 2022.05) Inception Transformer, [[Paper]](https://arxiv.org/pdf/2205.12956.pdf), [[Code]](https://github.com/sail-sg/iFormer)
- (arXiv 2022.05) MoCoViT: Mobile Convolutional Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.12635.pdf), [[Code]](https://github.com/sail-sg/iFormer)
- (arXiv 2022.05) Breaking the Chain of Gradient Leakage in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.12551.pdf), [[Code]](https://github.com/sail-sg/iFormer)
- (arXiv 2022.05) Hierarchical Vision Transformer for Masked Image Modeling, [[Paper]](https://arxiv.org/pdf/2205.13515.pdf), [[Code]](https://github.com/LayneH/GreenMIM)
- (arXiv 2022.05) Fast Vision Transformers with HiLo Attention, [[Paper]](https://arxiv.org/pdf/2205.13213.pdf), [[Code]](https://github.com/zip-group/LITv2)
- (arXiv 2022.05) AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition, [[Paper]](https://arxiv.org/pdf/2205.13535.pdf), [[Code]](http://www.shoufachen.com/adaptformer-page)
- (arXiv 2022.05) X-ViT: High Performance Linear Vision Transformer without Softmax, [[Paper]](https://arxiv.org/pdf/2205.13805.pdf)
- (arXiv 2022.05) Architecture-Agnostic Masked Image Modeling 鈥? From ViT back to CNN, [[Paper]](https://arxiv.org/pdf/2205.13805.pdf), [[Code]](http://www.shoufachen.com/adaptformer-page)
- (arXiv 2022.05) HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling, [[Paper]](https://arxiv.org/pdf/2205.14949.pdf)
- (arXiv 2022.05) EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition, [[Paper]](https://arxiv.org/pdf/2205.14756.pdf), [[Code]](https://tinyml.mit.edu/)
- (arXiv 2022.06) EfficientFormer: Vision Transformers at MobileNet Speed, [[Paper]](https://arxiv.org/pdf/2206.01191.pdf), [[Code]](https://github.com/snap-research/EfficientFormer)
- (arXiv 2022.06) Optimizing Relevance Maps of Vision Transformers Improves Robustness, [[Paper]](https://arxiv.org/pdf/2206.01161.pdf), [[Code]](https://github.com/hila-chefer/RobustViT)
- (arXiv 2022.06) Separable Self-attention for Mobile Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.02680.pdf), [[Code]](https://github.com/apple/ml-cvnets)
- (arXiv 2022.06) Spatial Entropy Regularization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.04636.pdf)
- (arXiv 2022.06) Peripheral Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.06801.pdf), [[Code]](http://cvlab.postech.ac.kr/research/PerViT/)
- (arXiv 2022.06) SP-ViT: Learning 2D Spatial Priors for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.07662.pdf)
- (arXiv 2022.06) FIT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification, [[Paper]](https://arxiv.org/pdf/2206.08671.pdf), [[Code]](https://github.com/cambridge-mlg/fit)
- (arXiv 2022.06) SimA: Simple Softmax-free Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.08898.pdf), [[Code]](https://github.com/UCDvision/sima)
- (arXiv 2022.06) Vicinity Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.10552.pdf), [[Code]](https://github.com/OpenNLPLab/Vicinity-Vision-Transformer)
- (arXiv 2022.06) EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications, [[Paper]](https://arxiv.org/pdf/2206.10589.pdf), [[Code]](https://t.ly/_Vu9)
- (arXiv 2022.06) Global Context Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.09959.pdf), [[Code]](https://github.com/NVlabs/GCViT)
- (arXiv 2022.06) EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm, [[Paper]](https://arxiv.org/pdf/2206.09325.pdf), [[Code]](https://https://github.com/zhangzjn/EATFormer)
- (arXiv 2022.06) A Unified and Biologically-Plausible Relational Graph Representation of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.11073.pdf)
- (arXiv 2022.06) Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment, [[Paper]](https://arxiv.org/pdf/2206.13951.pdf), [[Code]](https://github.com/kojima-takeshi188/CFA)
- (arXiv 2022.06) Continual Learning with Transformers for Image Classification, [[Paper]](https://arxiv.org/pdf/2206.14085.pdf)
- (arXiv 2022.07) Visual Transformer Meets CutMix for Improved Accuracy, Communication Efficiency, and Data Privacy in Split Learning, [[Paper]](https://arxiv.org/pdf/2207.00234.pdf)
- (arXiv 2022.07) Rethinking Query-Key Pairwise Interactions in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.00188.pdf)
- (arXiv 2022.07) Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks, [[Paper]](https://arxiv.org/pdf/2207.01580.pdf), [[Code]](https://github.com/raoyongming/DynamicViT)
- (arXiv 2022.07) Softmax-free Linear Transformers, [[Paper]](https://arxiv.org/pdf/2207.03341.pdf), [[Code]](https://github.com/fudan-zvg/SOFT)
- (arXiv 2022.07) MaiT: Leverage Attention Masks for More Efficient Image Transformers, [[Paper]](https://arxiv.org/pdf/2207.03006.pdf)
- (arXiv 2022.07) Dual Vision Transformer, [[Paper]](https://arxiv.org/pdf/2207.04976.pdf), [[Code]](https://github.com/YehLi/ImageNetModel)
- (arXiv 2022.07) Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning, [[Paper]](https://arxiv.org/pdf/2207.04978.pdf), [[Code]](https://github.com/YehLi/ImageNetModel)
- (arXiv 2022.07) Horizontal and Vertical Attention in Transformers, [[Paper]](https://arxiv.org/pdf/2207.04399.pdf)
- (arXiv 2022.07) LightViT: Towards Light-Weight Convolution-Free Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.05557.pdf), [[Code]](https://github.com/hunto/LightViT)
- (arXiv 2022.07) Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios, [[Paper]](https://arxiv.org/pdf/2207.05501.pdf)
- (arXiv 2022.07) Image and Model Transformation with Secret Key for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2207.05366.pdf)
- (arXiv 2022.07) Convolutional Bypasses Are Better Vision Transformer Adapters, [[Paper]](https://arxiv.org/pdf/2207.07039.pdf)
- (arXiv 2022.07) Lightweight Vision Transformer with Cross Feature Attention, [[Paper]](https://arxiv.org/pdf/2207.07268.pdf)
- (arXiv 2022.07) Multi-manifold Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.08569.pdf)
- (arXiv 2022.07) TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.08409.pdf), [[Code]](https://github.com/Sense-X/TokenMix)
- (arXiv 2022.07) Locality Guidance for Improving Vision Transformers on Tiny Datasets, [[Paper]](https://arxiv.org/pdf/2207.10026.pdf), [[Code]](https://github.com/lkhl/tiny-transformers)
- (arXiv 2022.07) TinyViT: Fast Pretraining Distillation for Small Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.10666.pdf), [[Code]](https://github.com/microsoft/Cream/tree/main/TinyViT)
- (arXiv 2022.07) Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2207.11971.pdf), [[Code]](https://yingyichen-cyy.github.io/Jigsaw-ViT/)
- (arXiv 2022.07) An Impartial Take to the CNN vs Transformer Robustness Contest, [[Paper]](https://arxiv.org/pdf/2207.11347.pdf)
- (arXiv 2022.07) Pro-tuning: Unified Prompt Tuning for Vision Tasks, [[Paper]](https://arxiv.org/pdf/2207.14381.pdf)
- (arXiv 2022.08) Semi-supervised Vision Transformers at Scale, [[Paper]](https://arxiv.org/pdf/2208.05688.pdf)
- (arXiv 2022.08) BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers, [[Paper]](https://arxiv.org/pdf/2208.06366.pdf), [[Code]](https://aka.ms/beit)
- (arXiv 2022.08) Accelerating Vision Transformer Training via a Patch Sampling Schedule, [[Paper]](https://arxiv.org/pdf/2208.09520.pdf), [[Code]](https://github.com/BradMcDanel/pss)
- (arXiv 2022.08) ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition, [[Paper]](https://arxiv.org/pdf/2208.10431.pdf), [[Code]](https://github.com/zju-vipa/ProtoPFormer)
- (arXiv 2022.08) FocusFormer: Focusing on What We Need via Architecture Sampler, [[Paper]](https://arxiv.org/pdf/2208.10861.pdf)
- (arXiv 2022.08) gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window, [[Paper]](https://arxiv.org/pdf/2208.11718.pdf)
- (arXiv 2022.08) Video Mobile-Former: Video Recognition with Efficient Global Spatial-temporal Modeling, [[Paper]](https://arxiv.org/pdf/2208.12257.pdf)
- (arXiv 2022.08) ClusTR: Exploring Efficient Self-attention via Clustering for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2208.13138.pdf)
- (arXiv 2022.09) MAFormer: A Transformer Network with Multi-scale Attention Fusion for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2209.01620.pdf)
- (arXiv 2022.09) A Light Recipe to Train Robust Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.07399.pdf), [[Code]](https://github.com/dedeswim/vits-robustness-torch)
- (arXiv 2022.09) ConvFormer: Closing the Gap Between CNN and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.07738.pdf), [[Code]](https://github.com/dedeswim/vits-robustness-torch)
- (arXiv 2022.09) Axially Expanded Windows for Local-Global Interaction in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.08726.pdf)
- (arXiv 2022.09) Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention, [[Paper]](https://arxiv.org/pdf/2209.13802.pdf)
- (arXiv 2022.09) Effective Vision Transformer Training: A Data-Centric Perspective, [[Paper]](https://arxiv.org/pdf/2209.15006.pdf)
- (arXiv 2022.09) Dilated Neighborhood Attention Transformer, [[Paper]](https://arxiv.org/pdf/2209.15001.pdf), [[Code]](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)
- (arXiv 2022.10) MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features, [[Paper]](https://arxiv.org/pdf/2209.15159.pdf), [[Code]](https://github.com/micronDLA/MobileViTv3)
- (arXiv 2022.10) Fast-ParC: Position Aware Global Kernel for ConvNets and ViTs, [[Paper]](https://arxiv.org/pdf/2210.04020.pdf)
- (arXiv 2022.10) Strong Gravitational Lensing Parameter Estimation with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.04143.pdf), [[Code]](https://github.com/kuanweih/strong_lensing_vit_resnet)
- (arXiv 2022.10) Token-Label Alignment for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.06455.pdf), [[Code]](https://github.com/Euphoria16/TL-Align)
- (arXiv 2022.10) Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets, [[Paper]](https://arxiv.org/pdf/2210.05958.pdf), [[Code]](https://github.com/ArieSeirack/DHVT)
- (arXiv 2022.10) Prompt Generation Networks for Efficient Adaptation of Frozen Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.06466.pdf), [[Code]](https://github.com/jochemloedeman/PGN)
- (arXiv 2022.10) Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers, [[Paper]](https://arxiv.org/pdf/2210.06313.pdf)
- (arXiv 2022.10) Curved Representation Space of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.05742.pdf)
- (arXiv 2022.10) How to Train Vision Transformer on Small-scale Datasets, [[Paper]](https://arxiv.org/pdf/2210.07240.pdf), [[Code]](https://github.com/hananshafi/vits-for-small-scale-datasets)
- (arXiv 2022.10) Vision Transformer Visualization: What Neurons Tell and How Neurons Behave, [[Paper]](https://arxiv.org/pdf/2210.07646.pdf), [[Code]](https://github.com/bym1902/vit_visualization)
- (arXiv 2022.10) When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture, [[Paper]](https://arxiv.org/pdf/2210.07540.pdf), [[Code]](https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers)
- (arXiv 2022.10) Vision Transformers provably learn spatial structure, [[Paper]](https://arxiv.org/pdf/2210.09221.pdf)
- (arXiv 2022.10) Scratching Visual Transformer's Back with Uniform Attention, [[Paper]](https://arxiv.org/pdf/2210.08457.pdf)
- (arXiv 2022.10) Token Merging: Your ViT But Faster, [[Paper]](https://arxiv.org/pdf/2210.09461.pdf), [[Code]](https://github.com/facebookresearch/ToMe)
- (arXiv 2022.10) Accumulated Trivial Attention Matters in Vision Transformers on Small Datasets, [[Paper]](https://arxiv.org/pdf/2210.12333.pdf), [[Code]](https://github.com/xiangyu8/SATA)
- (arXiv 2022.10) MetaFormer Baselines for Vision, [[Paper]](https://arxiv.org/pdf/2210.13452.pdf)
- (arXiv 2022.10) Learning Explicit Object-Centric Representations with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.14139.pdf)
- (arXiv 2022.10) Explicitly Increasing Input Information Density for Vision Transformers on Small Datasets, [[Paper]](https://arxiv.org/pdf/2210.14319.pdf), [[Code]](https://github.com/xiangyu8/DenseVT)
- (arXiv 2022.10) Grafting Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.15943.pdf)
- (arXiv 2022.10) Differentially Private CutMix for Split Learning with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.15986.pdf)
- (arXiv 2022.10) ViT-LSLA: Vision Transformer with Light Self-Limited-Attention, [[Paper]](https://arxiv.org/pdf/2210.17115.pdf)
- (arXiv 2022.11) Rethinking Hierarchicies in Pre-trained Plain Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.01785.pdf), [[Code]](https://github.com/ViTAE-Transformer/HPViT)
- (arXiv 2022.11) The Lottery Ticket Hypothesis for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.01484.pdf)
- (arXiv 2022.11) ViT-CX: Causal Explanation of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.03064.pdf)
- (arXiv 2022.11) ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention, [[Paper]](https://arxiv.org/pdf/2211.05109.pdf)
- (arXiv 2022.11) Training a Vision Transformer from scratch in less than 24 hours with 1 GPU, [[Paper]](https://arxiv.org/pdf/2211.05187.pdf), [[Code]](https://github.com/BorealisAI/efficient-vit-training)
- (arXiv 2022.11) Demystify Transformers & Convolutions in Modern Image Deep Networks, [[Paper]](https://arxiv.org/pdf/2211.05781.pdf), [[Code]](https://github.com/OpenGVLab/STM-Evaluation)
- (arXiv 2022.11) Token Transformer: Can class token help window-based transformer build better long-range interactions, [[Paper]](https://arxiv.org/pdf/2211.06083.pdf)
- (arXiv 2022.11) CabViT: Cross Attention among Blocks for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.07198.pdf), [[Code]](https://github.com/hkzhang91/CabViT)
- (arXiv 2022.11) BiViT: Extremely Compressed Binary Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.07091.pdf), [[Code]](https://github.com/hkzhang91/CabViT)
- (arXiv 2022.11) HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.08024.pdf)
- (arXiv 2022.11) Vision Transformer with Super Token Sampling, [[Paper]](https://arxiv.org/pdf/2211.11167.pdf), [[Code]](https://github.com/hhb072/SViT)
- (arXiv 2022.11) Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention During Vision Transformer Inference, [[Paper]](https://arxiv.org/pdf/2211.10526.pdf)
- (arXiv 2022.11) Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2211.11943.pdf)
- (arXiv 2022.11) TranViT: An Integrated Vision Transformer Framework for Discrete Transit Travel Time Range Prediction, [[Paper]](https://arxiv.org/pdf/2211.12322.pdf)
- (arXiv 2022.11) Gated Class-Attention with Cascaded Feature Drift Compensation for Exemplar-free Continual Learning of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.12292.pdf), [[Code]](https://github.com/OcraM17/GCAB-CFDC)
- (arXiv 2022.11) Data Augmentation Vision Transformer for Fine-grained Image Classification, [[Paper]](https://arxiv.org/pdf/2211.12879.pdf)
- (arXiv 2022.11) Integrally Pre-Trained Transformer Pyramid Networks, [[Paper]](https://arxiv.org/pdf/2211.12735.pdf), [[Code]](https://github.com/sunsmarterjie/iTPN)
- (arXiv 2022.11) Explanation on Pretraining Bias of Finetuned Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.15428.pdf)
- (arXiv 2022.11) Adaptive Attention Link-based Regularization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.13852.pdf)
- (arXiv 2022.11) Semantic-Aware Local-Global Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.14705.pdf)
- (arXiv 2022.11) Pattern Attention Transformer with Doughnut Kernel, [[Paper]](https://arxiv.org/pdf/2211.16961.pdf)
- (arXiv 2022.11) ResFormer: Scaling ViTs with Multi-Resolution Training, [[Paper]](https://arxiv.org/pdf/2212.00776.pdf)
- (arXiv 2022.12) Teaching Matters: Investigating the Role of Supervision in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.03862.pdf), [[Code]](https://github.com/mwalmer-umd/vit_analysis)
- (arXiv 2022.12) Group Generalized Mean Pooling for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2212.04114.pdf)
- (arXiv 2022.12) OAMixer: Object-aware Mixing Layer for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.06595.pdf), [[Code]](https://github.com/alinlab/OAMixer)
- (arXiv 2022.12) What do Vision Transformers Learn? A Visual Exploration, [[Paper]](https://arxiv.org/pdf/2212.06727.pdf)
- (arXiv 2022.12) GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation, [[Paper]](https://arxiv.org/pdf/2212.06795.pdf), [[Code]](https://github.com/ChenhongyiYang/GPViT)
- (arXiv 2022.12) FlexiViT: One Model for All Patch Sizes, [[Paper]](https://arxiv.org/pdf/2212.08013.pdf), [[Code]](https://github.com/google-research/big_vision)
- (arXiv 2022.12) Rethinking Vision Transformers for MobileNet Size and Speed, [[Paper]](https://arxiv.org/pdf/2212.08059.pdf), [[Code]](https://github.com/snap-research/EfficientFormer)
- (arXiv 2022.12) Rethinking Cooking State Recognition with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.08586.pdf)
- (arXiv 2022.12) What Makes for Good Tokenizers in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2212.11115.pdf)
- (arXiv 2022.12) Local Learning on Transformers via Feature Reconstruction, [[Paper]](https://arxiv.org/pdf/2212.14215.pdf)
- (arXiv 2022.12) Exploring Transformer Backbones for Image Diffusion Models, [[Paper]](https://arxiv.org/pdf/2212.14678.pdf)
- (arXiv 2023.01) TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models, [[Paper]](https://arxiv.org/pdf/2301.01296.pdf), [[Code]](https://github.com/OliverRensu/TinyMIM)
- (arXiv 2023.01) Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.01431.pdf)
- (arXiv 2023.01) Skip-Attention: Improving Vision Transformers by Paying Less Attention, [[Paper]](https://arxiv.org/pdf/2301.02240.pdf)
- (arXiv 2023.01) Dynamic Grained Encoder for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.03831.pdf), [[Code]](https://github.com/StevenGrove/vtpack)
- (arXiv 2023.01) Image Memorability Prediction with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.08647.pdf)
- (arXiv 2023.01) Holistically Explainable Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.08669.pdf)
- (arXiv 2023.02) DilateFormer: Multi-Scale Dilated Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2302.01791.pdf), [[Code]](https://github.com/JIAOJIAYUASD/dilateformer)
- (arXiv 2023.02) KDEformer: Accelerating Transformers via Kernel Density Estimation, [[Paper]](https://arxiv.org/pdf/2302.02451.pdf), [[Code]](https://github.com/majid-daliri/kdeformer)
- (arXiv 2023.02) Reversible Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.04869.pdf), [[Code]](https://github.com/facebookresearch/slowfast)
- (arXiv 2023.02) TFormer: A Transmission-Friendly ViT Model for IoT Devices, [[Paper]](https://arxiv.org/pdf/2302.07734.pdf)
- (arXiv 2023.02) Efficiency 360: Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.08374.pdf)
- (arXiv 2023.02) ViTA: A Vision Transformer Inference Accelerator for Edge Applications, [[Paper]](https://arxiv.org/pdf/2302.09108.pdf)
- (arXiv 2023.02) CertViT: Certified Robustness of Pre-Trained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.10287.pdf), [[Code]](https://github.com/sagarverma/transformer-lipschitz)
- (arXiv 2023.03) Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves, [[Paper]](https://arxiv.org/pdf/2303.01112.pdf), [[Code]](https://masora1030.github.io/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves/)
- (arXiv 2023.03) Data-Efficient Training of CNNs and Transformers with Coresets: A Stability Perspective, [[Paper]](https://arxiv.org/pdf/2303.02095.pdf), [[Code]](https://github.com/transmuteAI/Data-Efficient-Transformers)
- (arXiv 2023.03) A Fast Training-Free Compression Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.02331.pdf), [[Code]](https://github.com/johnheo/fast-compress-vit)
- (arXiv 2023.03) FFT-based Dynamic Token Mixer for Vision, [[Paper]](https://arxiv.org/pdf/2303.03932.pdf), [[Code]](https://github.com/okojoalg/dfformer)
- (arXiv 2023.03) Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models, [[Paper]](https://arxiv.org/pdf/2303.04143.pdf), [[Code]](https://github.com/SamsungSAILMontreal/ghn3)
- (arXiv 2023.03) X-Pruner: eXplainable Pruning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.04935.pdf)
- (arXiv 2023.03) CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale Attention, [[Paper]](https://arxiv.org/pdf/2303.06908.pdf), [[Code]](https://github.com/cheerss/CrossFormer)
- (arXiv 2023.03) Stabilizing Transformer Training by Preventing Attention Entropy Collapse, [[Paper]](https://arxiv.org/pdf/2303.06296.pdf)
- (arXiv 2023.03) Making Vision Transformers Efficient from A Token Sparsification View, [[Paper]](https://arxiv.org/pdf/2303.08685.pdf)
- (arXiv 2023.03) BiFormer: Vision Transformer with Bi-Level Routing Attention, [[Paper]](https://arxiv.org/pdf/2303.08810.pdf), [[Code]](https://github.com/rayleizhu/BiFormer)
- (arXiv 2023.03) ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices, [[Paper]](https://arxiv.org/pdf/2303.09730.pdf)
- (arXiv 2023.03) Robustifying Token Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.11126.pdf)
- (arXiv 2023.03) FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization, [[Paper]](https://arxiv.org/pdf/2303.14189.pdf)
- (arXiv 2023.03) Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.13755.pdf)
- (arXiv 2023.03) How Does Attention Work in Vision Transformers? A Visual Analytics Attempt, [[Paper]](https://arxiv.org/pdf/2303.13731.pdf)
- (arXiv 2023.03) SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications, [[Paper]](https://arxiv.org/pdf/2303.15446.pdf), [[Code]](https://tinyurl.com/5ft8v46w)
- (arXiv 2023.03) Vision Transformer with Quadrangle Attention, [[Paper]](https://arxiv.org/pdf/2303.15105.pdf), [[Code]](https://github.com/ViTAE-Transformer/QFormer)
- (arXiv 2023.04) LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.18013.pdf)
- (arXiv 2023.04) Rethinking Local Perception in Lightweight Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.17803.pdf)
- (arXiv 2023.04) Vision Transformers with Mixed-Resolution Tokenization, [[Paper]](https://arxiv.org/pdf/2304.00287.pdf), [[Code]](https://github.com/TomerRonen34/mixed-resolution-vit)
- (arXiv 2023.04) Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention, [[Paper]](https://arxiv.org/pdf/2304.03282.pdf), [[Code]](https://github.com/dingmyu/DependencyViT)
- (arXiv 2023.04) PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift, [[Paper]](https://arxiv.org/pdf/2304.03481.pdf), [[Code]](https://isee-ai.cn/wugaojie/PSLT.html)
- (arXiv 2023.04) SparseFormer: Sparse Visual Recognition via Limited Latent Tokens, [[Paper]](https://arxiv.org/pdf/2304.03768.pdf), [[Code]](https://github.com/showlab/sparseformer)
- (arXiv 2023.04) ViT-Calibrator: Decision Stream Calibration for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2304.04354.pdf)
- (arXiv 2023.04) Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention, [[Paper]](https://arxiv.org/pdf/2304.04237.pdf), [[Code]](https://github.com/LeapLabTHU/Slide-Transformer)
- (arXiv 2023.04) Life Regression based Patch Slimming for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2304.04926.pdf)
- (arXiv 2023.04) RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer, [[Paper]](https://arxiv.org/pdf/2304.05659.pdf), [[Code]](https://techmonsterwang.github.io/RIFormer/)
- (arXiv 2023.04) SpectFormer: Frequency and Attention is what you need in a Vision Transformer, [[Paper]](https://arxiv.org/pdf/2304.06446.pdf), [[Code]](https://badripatro.github.io/SpectFormers/)
- (arXiv 2023.04) VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking, [[Paper]](https://arxiv.org/pdf/2304.06391.pdf), [[Code]](https://github.com/AngelosNal/Vision-DiffMask)
- (arXiv 2023.04) RSIR Transformer: Hierarchical Vision Transformer using Random Sampling Windows and Important Region Windows, [[Paper]](https://arxiv.org/pdf/2304.06250.pdf)
- (arXiv 2023.04) Dynamic Mobile-Former: Strengthening Dynamic Convolution with Attention and Residual Connection in Kernel Space, [[Paper]](https://arxiv.org/pdf/2304.07254.pdf), [[Code]](https://github.com/ysj9909/DMF)
- (arXiv 2023.04) LipsFormer: Introducing Lipschitz Continuity to Vision Transformers, [[Paper]](https://arxiv.org/pdf/2304.09856.pdf), [[Code]](https://github.com/IDEA-Research/LipsFormer)
- (arXiv 2023.04) Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2304.10716.pdf), [[Code]](https://github.com/megvii-research/TPS-CVPR2023)
- (arXiv 2023.04) MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2304.12043.pdf), [[Code]](https://github.com/fistyee/MixPro)
- (arXiv 2023.04) Vision Conformer: Incorporating Convolutions into Vision Transformer Layers, [[Paper]](https://arxiv.org/pdf/2304.13991.pdf)
- (arXiv 2023.05) Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT, [[Paper]](https://arxiv.org/pdf/2305.00201.pdf)
- (arXiv 2023.05) MMViT: Multiscale Multiview Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.00104.pdf)
- (arXiv 2023.05) AxWin Transformer: A Context-Aware Vision Transformer Backbone with Axial Windows, [[Paper]](https://arxiv.org/pdf/2305.01280.pdf)
- (arXiv 2023.05) Understanding Gaussian Attention Bias of Vision Transformers Using Effective Receptive Fields, [[Paper]](https://arxiv.org/pdf/2305.04722.pdf)
- (arXiv 2023.05) EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention, [[Paper]](https://arxiv.org/pdf/2305.07027.pdf), [[Code]](https://github.com/microsoft/Cream/tree/main/EfficientViT)
- (arXiv 2023.05) GSB: Group Superposition Binarization for Vision Transformer with Limited Training Samples, [[Paper]](https://arxiv.org/pdf/2305.07931.pdf)
- (arXiv 2023.05) Enhancing Performance of Vision Transformers on Small Datasets through Local Inductive Bias Incorporation, [[Paper]](https://arxiv.org/pdf/2305.08551.pdf)
- (arXiv 2023.05) CageViT: Convolutional Activation Guided Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.09924.pdf)
- (arXiv 2023.05) Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design, [[Paper]](https://arxiv.org/pdf/2305.13035.pdf)
- (arXiv 2023.05) Predicting Token Impact Towards Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.14840.pdf)
- (arXiv 2023.05) Dual Path Transformer with Partition Attention, [[Paper]](https://arxiv.org/pdf/2305.14768.pdf)
- (arXiv 2023.05) BinaryViT: Towards Efficient and Accurate Binary Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.14730.pdf)
- (arXiv 2023.05) Making Vision Transformers Truly Shift-Equivariant, [[Paper]](https://arxiv.org/pdf/2305.16316.pdf)
- (arXiv 2023.05) Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability, [[Paper]](https://arxiv.org/pdf/2305.15775.pdf), [[Code]](https://github.com/jyhong0304/concept_centric_transformers)
- (arXiv 2023.05) DiffRate : Differentiable Compression Rate for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.17997.pdf), [[Code]](https://github.com/OpenGVLab/DiffRate)
- (arXiv 2023.06) Lightweight Vision Transformer with Bidirectional Interaction, [[Paper]](https://arxiv.org/pdf/2306.00396.pdf), [[Code]](https://github.com/qhfan/FAT)
- (arXiv 2023.06) Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles, [[Paper]](https://arxiv.org/pdf/2306.00989.pdf), [[Code]](https://github.com/facebookresearch/hiera)
- (arXiv 2023.06) Bytes Are All You Need: Transformers Operating Directly On File Bytes, [[Paper]](https://arxiv.org/pdf/2306.00238.pdf), [[Code]](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer)
- (arXiv 2023.06) Muti-Scale And Token Mergence: Make Your ViT More Efficient, [[Paper]](https://arxiv.org/pdf/2306.04897.pdf)
- (arXiv 2023.06) FasterViT: Fast Vision Transformers with Hierarchical Attention, [[Paper]](https://arxiv.org/pdf/2306.06189.pdf), [[Code]](https://github.com/NVlabs/FasterViT)
- (arXiv 2023.06) E(2)-Equivariant Vision Transformer, [[Paper]](https://arxiv.org/pdf/2306.06722.pdf), [[Code]](https://github.com/ZJUCDSYangKaifan/GEVit)
- (arXiv 2023.06) 2-D SSM: A General Spatial Layer for Visual Transformers, [[Paper]](https://arxiv.org/pdf/2306.06635.pdf), [[Code]](https://github.com/ethanbar11/ssm_2d)
- (arXiv 2023.06) Mitigating Transformer Overconfidence via Lipschitz Regularization, [[Paper]](https://arxiv.org/pdf/2306.06849.pdf), [[Code]](https://github.com/SZCHAI/LRFormer)
- (arXiv 2023.06) Reviving Shift Equivariance in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.07470.pdf)
- (arXiv 2023.06) Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training, [[Paper]](https://arxiv.org/pdf/2306.07346.pdf), [[Code]](https://github.com/aimagelab/MaPeT)
- (arXiv 2023.06) Fast Training of Diffusion Models with Masked Transformers, [[Paper]](https://arxiv.org/pdf/2306.09305.pdf), [[Code]](https://github.com/Anima-Lab/MaskDiT)
- (arXiv 2023.06) RaViTT: Random Vision Transformer Tokens, [[Paper]](https://arxiv.org/pdf/2306.10959.pdf)
- (arXiv 2023.06) Vision Transformer with Attention Map Hallucination and FFN Compaction, [[Paper]](https://arxiv.org/pdf/2306.10875.pdf)
- (arXiv 2023.06) Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing, [[Paper]](https://arxiv.org/pdf/2306.12929.pdf)
- (arXiv 2023.06) Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window, [[Paper]](https://arxiv.org/pdf/2306.13776.pdf)
- (arXiv 2023.06) BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models, [[Paper]](https://arxiv.org/pdf/2306.16678.pdf)
- (arXiv 2023.06) Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing, [[Paper]](https://arxiv.org/pdf/2306.17848.pdf), [[Code]](https://arielnlee.github.io/PatchMixing/)
- (arXiv 2023.07) Stitched ViTs are Flexible Vision Backbones, [[Paper]](https://arxiv.org/pdf/2307.00154.pdf), [[Code]](https://github.com/ziplab/SN-Netv2)
- (arXiv 2023.07) MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.02321.pdf), [[Code]](https://github.com/ziplab/SN-Netv2)
- (arXiv 2023.07) Make A Long Image Short: Adaptive Token Length for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.02092.pdf)
- (arXiv 2023.07) Art Authentication with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.03039.pdf)
- (arXiv 2023.07) Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution, [[Paper]](https://arxiv.org/pdf/2307.06304.pdf)
- (arXiv 2023.07) What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation, [[Paper]](https://arxiv.org/pdf/2307.06006.pdf)
- (arXiv 2023.07) Scale-Aware Modulation Meet Transformer, [[Paper]](https://arxiv.org/pdf/2307.08579.pdf), [[Code]](https://github.com/AFeng-x/SMT)
- (arXiv 2023.07) RepViT: Revisiting Mobile CNN From ViT Perspective, [[Paper]](https://arxiv.org/pdf/2307.09283.pdf), [[Code]](https://github.com/jameslahm/RepViT)
- (arXiv 2023.07) R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut, [[Paper]](https://arxiv.org/pdf/2307.09050.pdf)
- (arXiv 2023.07) Learned Thresholds Token Merging and Pruning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.10780.pdf)
- (arXiv 2023.07) Sparse then Prune: Toward Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.11988.pdf), [[Code]](https://github.com/yogiprsty/Sparse-ViT)
- (arXiv 2023.07) Sparse Double Descent in Vision Transformers: real or phantom threat?, [[Paper]](https://arxiv.org/pdf/2307.14253.pdf)
- (arXiv 2023.07) Adaptive Frequency Filters As Efficient Global Token Mixers, [[Paper]](https://arxiv.org/pdf/2307.14008.pdf)
- (arXiv 2023.07) E2VPT: An Effective and Efficient Approach for Visual Prompt Tuning, [[Paper]](https://arxiv.org/pdf/2307.13770.pdf), [[Code]](https://github.com/ChengHan111/E2VPT)
- (arXiv 2023.07) Pre-training Vision Transformers with Very Limited Synthesized Images, [[Paper]](https://arxiv.org/pdf/2307.14710.pdf), [[Code]](https://github.com/ryoo-nakamura/OFDB/)
- (arXiv 2023.08) LGViT: Dynamic Early Exiting for Accelerating Vision Transformer, [[Paper]](https://arxiv.org/abs/2308.00255)
- (arXiv 2023.08) Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique, [[Paper]](https://arxiv.org/pdf/2308.00197.pdf)
- (arXiv 2023.08) FLatten Transformer: Vision Transformer using Focused Linear Attention, [[Paper]](https://arxiv.org/pdf/2308.00442.pdf), [[Code]](https://github.com/LeapLabTHU/FLatten-Transformer)
- (arXiv 2023.08) A Multidimensional Analysis of Social Biases in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.01948.pdf)
- (arXiv 2023.08) Which Tokens to Use? Investigating Token Reduction in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.04657.pdf), [[Code]](https://vap.aau.dk/tokens)
- (arXiv 2023.08) DiT: Efficient Vision Transformers with Dynamic Token Routing, [[Paper]](https://arxiv.org/pdf/2308.03409.pdf), [[Code]](https://github.com/Maycbj/DiT)
- (arXiv 2023.08) Revisiting Vision Transformer from the View of Path Ensemble, [[Paper]](https://arxiv.org/pdf/2308.06548.pdf)
- (arXiv 2023.08) Patch Is Not All You Need, [[Paper]](https://arxiv.org/pdf/2308.10729.pdf)
- (arXiv 2023.08) ConcatPlexer: Additional Dim1 Batching for Faster ViTs, [[Paper]](https://arxiv.org/pdf/2308.11199.pdf), [[Code]](https://github.com/Maycbj/DiT)
- (arXiv 2023.08) SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation, [[Paper]](https://arxiv.org/pdf/2308.11568.pdf), [[Code]](https://doranlyong.github.io/projects/spanet/)
- (arXiv 2023.08) SG-Former: Self-guided Transformer with Evolving Token Reallocation, [[Paper]](https://arxiv.org/pdf/2308.12216.pdf), [[Code]](https://github.com/OliverRensu/SG-Former)
- (arXiv 2023.08) Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.13494.pdf)
- (arXiv 2023.08) Learning Diverse Features in Vision Transformers for Improved Generalization, [[Paper]](https://arxiv.org/pdf/2308.16274.pdf)
- (arXiv 2023.09) DAT++: Spatially Dynamic Vision Transformer with Deformable Attention, [[Paper]](https://arxiv.org/pdf/2309.01430.pdf), [[Code]](https://github.com/LeapLabTHU/DAT)
- (arXiv 2023.09) ExMobileViT: Lightweight Classifier Extension for Mobile Vision Transformer, [[Paper]](https://arxiv.org/pdf/2309.01310.pdf)
- (arXiv 2023.09) Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts, [[Paper]](https://arxiv.org/pdf/2309.04354.pdf)
- (arXiv 2023.09) CNN or ViT? Revisiting Vision Transformers Through the Lens of Convolution, [[Paper]](https://arxiv.org/pdf/2309.05375.pdf), [[Code]](https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention)
- (arXiv 2023.09) SparseSwin: Swin Transformer with Sparse Transformer Block, [[Paper]](https://arxiv.org/pdf/2309.05224.pdf), [[Code]](https://github.com/KrisnaPinasthika/SparseSwin)
- (arXiv 2023.09) DeViT: Decomposing Vision Transformers for Collaborative Inference in Edge Devices, [[Paper]](https://arxiv.org/pdf/2309.05015.pdf)
- (arXiv 2023.09) Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit, [[Paper]](https://arxiv.org/pdf/2309.06891.pdf), [[Code]](https://github.com/billpsomas/simpool)
- (arXiv 2023.09) Interpretability-Aware Vision Transformer, [[Paper]](https://arxiv.org/pdf/2309.08035.pdf)
- (arXiv 2023.09) Replacing softmax with ReLU in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2309.08586.pdf)
- (arXiv 2023.09) Interpret Vision Transformers as ConvNets with Dynamic Convolutions, [[Paper]](https://arxiv.org/pdf/2309.10713.pdf)
- (arXiv 2023.09) RMT: Retentive Networks Meet Vision Transformers, [[Paper]](https://arxiv.org/pdf/2309.11523.pdf)
- (arXiv 2023.09) DualToken-ViT: Position-aware Efficient Vision Transformer with Dual Token Fusion, [[Paper]](https://arxiv.org/pdf/2309.12424.pdf)
- (arXiv 2023.09) Associative Transformer Is A Sparse Representation Learner, [[Paper]](https://arxiv.org/pdf/2309.12862.pdf)
- (arXiv 2023.09) Masked Image Residual Learning for Scaling Deeper Vision Transformers, [[Paper]](https://arxiv.org/pdf/2309.14136.pdf)
- (arXiv 2023.09) Efficient Low-rank Backpropagation for Vision Transformer Adaptation, [[Paper]](https://arxiv.org/pdf/2309.15275.pdf)
- (arXiv 2023.09) Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words, [[Paper]](https://arxiv.org/pdf/2309.16108.pdf)
- (arXiv 2023.09) Vision Transformers Need Registers, [[Paper]](https://arxiv.org/pdf/2309.16588.pdf)
- (arXiv 2023.10) PPT: Token Pruning and Pooling for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.01812.pdf)
- (arXiv 2023.10) Selective Feature Adapter for Dense Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.01843.pdf)
- (arXiv 2023.10) GET: Group Event Transformer for Event-Based Vision, [[Paper]](https://arxiv.org/pdf/2310.02642.pdf), [[Code]](https://github.com/Peterande/GET-Group-Event-Transformer)
- (arXiv 2023.10) ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2310.02588.pdf)
- (arXiv 2023.10) SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy Efficiency of Inference Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.02544.pdf), [[Code]](https://github.com/UCDvision/SlowFormer)
- (arXiv 2023.10) TiC: Exploring Vision Transformer in Convolution, [[Paper]](https://arxiv.org/pdf/2310.04134.pdf), [[Code]](https://github.com/zs670980918/MSA-Conv)
- (arXiv 2023.10) Sub-token ViT Embedding via Stochastic Resonance Transformers, [[Paper]](https://arxiv.org/pdf/2310.03967.pdf)
- (arXiv 2023.10) No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling, [[Paper]](https://arxiv.org/pdf/2310.05654.pdf)
- (arXiv 2023.10) Plug n' Play: Channel Shuffle Module for Enhancing Tiny Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.05642.pdf)
- (arXiv 2023.10) Hierarchical Side-Tuning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.05393.pdf), [[Code]](https://github.com/AFeng-x/HST)
- (arXiv 2023.10) EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention, [[Paper]](https://arxiv.org/pdf/2310.06629.pdf), [[Code]](https://github.com/nkusyl)
- (arXiv 2023.10) Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing, [[Paper]](https://arxiv.org/pdf/2310.06234.pdf), [[Code]](https://github.com/DavidYanAnDe/ARC)
- (arXiv 2023.10) Accelerating Vision Transformers Based on Heterogeneous Attention Patterns, [[Paper]](https://arxiv.org/pdf/2310.07664.pdf)
- (arXiv 2023.10) MatFormer: Nested Transformer for Elastic Inference, [[Paper]](https://arxiv.org/pdf/2310.07707.pdf)
- (arXiv 2023.10) Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems, [[Paper]](https://arxiv.org/pdf/2310.12956.pdf)
- (arXiv 2023.10) ConvNets Match Vision Transformers at Scale, [[Paper]](https://arxiv.org/pdf/2310.16764.pdf)
- (arXiv 2023.10) MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory, [[Paper]](https://arxiv.org/pdf/2310.16898.pdf), [[Code]](https://github.com/liangyn22/MCUFormer)
- (arXiv 2023.10) Analyzing Vision Transformers for Image Classification in Class Embedding Space, [[Paper]](https://arxiv.org/pdf/2310.18969.pdf)
- (arXiv 2023.11) Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation, [[Paper]](https://arxiv.org/pdf/2311.00441.pdf)
- (arXiv 2023.11) Scattering Vision Transformer: Spectral Mixing Matters, [[Paper]](https://arxiv.org/pdf/2311.01310.pdf), [[Project]](https://badripatro.github.io/svt/)
- (arXiv 2023.11) GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values, [[Paper]](https://arxiv.org/pdf/2311.03426.pdf)
- (arXiv 2023.11) Mini but Mighty: Finetuning ViTs with Mini Adapters, [[Paper]](https://arxiv.org/pdf/2311.03873.pdf), [[Code]](https://github.com/IemProg/MiMi)
- (arXiv 2023.11) A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis, [[Paper]](https://arxiv.org/pdf/2311.04157.pdf), [[Code]](https://github.com/Imageomics/INTR)
- (arXiv 2023.11) SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers, [[Paper]](https://arxiv.org/pdf/2311.03747.pdf), [[Code]](https://github.com/xyongLu/SBCFormer)
- (arXiv 2023.11) FMViT: A multiple-frequency mixing Vision Transformer, [[Paper]](https://arxiv.org/pdf/2311.05707.pdf), [[Code]](https://github.com/tany0699/FMViT)
- (arXiv 2023.11) Cross-Axis Transformer with 2D Rotary Embeddings, [[Paper]](https://arxiv.org/pdf/2311.07184.pdf)
- (arXiv 2023.11) Aggregate, Decompose, and Fine-Tune: A Simple Yet Effective Factor-Tuning Method for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2311.06749.pdf), [[Code]](https://github.com/Dongping-Chen/EFFT-EFfective-Factor-Tuning)
- (arXiv 2023.11) Advancing Vision Transformers with Group-Mix Attention, [[Paper]](https://arxiv.org/pdf/2311.15157.pdf), [[Code]](https://github.com/AILab-CVC/GroupMixFormer)
- (arXiv 2023.11) Token Recycling for Efficient Sequential Inference with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.15335.pdf)
- (arXiv 2023.11) TransNeXt: Robust Foveal Visual Perception for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.17132.pdf)
- (arXiv 2023.11) Stochastic Vision Transformers with Wasserstein Distance-Aware Attention, [[Paper]](https://arxiv.org/pdf/2311.18645.pdf)
- (arXiv 2023.11) Improving Faithfulness for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.17983.pdf)
- (arXiv 2023.11) SCHEME: Scalable Channer Mixer for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.00412.pdf)
- (arXiv 2023.12) MABViT -- Modified Attention Block Enhances Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.01324.pdf)
- (arXiv 2023.12) Class-Discriminative Attention Maps for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.02364.pdf), [[Code]](https://github.com/lenbrocki/CDAM)
- (arXiv 2023.12) Factorization Vision Transformer: Modeling Long Range Dependency with Local Window Cost, [[Paper]](https://arxiv.org/pdf/2312.08614.pdf), [[Code]](https://github.com/q2479036243/FaViT)
- (arXiv 2023.12) Weight Subcloning: Direct Initialization of Transformers Using Larger Pretrained Ones, [[Paper]](https://arxiv.org/pdf/2312.09299.pdf)
- (arXiv 2023.12) Cached Transformers: Improving Transformers with Differentiable Memory Cache, [[Paper]](https://arxiv.org/pdf/2312.12742.pdf)
- (arXiv 2023.12) Partial Fine-Tuning: A Successor to Full Fine-Tuning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.15681.pdf)
- (arXiv 2023.12) Merging Vision Transformers from Different Tasks and Domains, [[Paper]](https://arxiv.org/pdf/2312.16240.pdf)
- (arXiv 2023.12) Universal Pyramid Adversarial Training for Improved ViT Performance, [[Paper]](https://arxiv.org/pdf/2312.16339.pdf)
- (arXiv 2024.01) Token Propagation Controller for Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.01470.pdf)
- (arXiv 2024.01) Intriguing Equivalence Structures of the Embedding Space of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.15568.pdf)
- (arXiv 2024.01) SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design, [[Paper]](https://arxiv.org/pdf/2401.16456.pdf)
- (arXiv 2024.02) LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition, [[Paper]](https://arxiv.org/pdf/2402.00033.pdf), [[Code]](https://github.com/edgeai1/LF-ViT.git)
- (arXiv 2024.02) A Manifold Representation of the Key in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2402.00534.pdf)
- (arXiv 2024.02) Faster Inference of Integer SWIN Transformer by Removing the GELU Activation, [[Paper]](https://arxiv.org/pdf/2402.01169.pdf)
- (arXiv 2024.02) SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization, [[Paper]](https://arxiv.org/pdf/2402.03317.pdf)
- (arXiv 2024.02) Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images, [[Paper]](https://arxiv.org/pdf/2402.03752.pdf)
- (arXiv 2024.02) Multi-Attribute Vision Transformers are Efficient and Robust Learners, [[Paper]](https://arxiv.org/abs/2402.08070), [[Code]](https://github.com/hananshafi/MTL-ViT)
- (arXiv 2024.02) FViT: A Focal Vision Transformer with Gabor Filter, [[Paper]](https://arxiv.org/pdf/2402.11303.pdf)
- (arXiv 2024.02) ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2402.11301.pdf)
- (arXiv 2024.02) Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers, [[Paper]](https://arxiv.org/pdf/2402.12138.pdf)
- (arXiv 2024.03) LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition, [[Paper]](https://arxiv.org/pdf/2403.01412.pdf), [[Code]](https://github.com/MaxLLF/LUM-ViT)
- (arXiv 2024.03) NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function, [[Paper]](https://arxiv.org/pdf/2403.02411.pdf)
- (arXiv 2024.03) T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.04523.pdf)
- (arXiv 2024.03) ACC-ViT : Atrous Convolution's Comeback in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.04200.pdf)
- (arXiv 2024.03) Scalable and Robust Transformer Decoders for Interpretable Image Classification with Foundation Models, [[Paper]](https://arxiv.org/pdf/2403.04125.pdf)
- (arXiv 2024.03) ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions, [[Paper]](https://arxiv.org/pdf/2403.07392.pdf), [[Code]](https://github.com/Traffic-X/ViT-CoMer)
- (arXiv 2024.03) Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.10030.pdf), [[Code]](https://github.com/mlvlab/MCTF)
- (arXiv 2024.03) HIRI-ViT: Scaling Vision Transformer with High Resolution Inputs, [[Paper]](https://arxiv.org/pdf/2403.11999.pdf)
- (arXiv 2024.03) Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation, [[Paper]](https://arxiv.org/pdf/2403.11808.pdf), [[Code]](https://github.com/NUS-HPC-AI-Lab/Dynamic-Tuning)
- (arXiv 2024.03) Rotary Position Embedding for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2403.13298.pdf), [[Code]](https://github.com/naver-ai/rope-vit)
- (arXiv 2024.03) Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.13677.pdf)

### Clustering
- (arXiv 2022.06) Vision Transformer for Contrastive Clustering, [[Paper]](https://arxiv.org/pdf/2206.12925.pdf)
- (arXiv 2023.04) Fairness in Visual Clustering: A Novel Transformer Clustering Approach, [[Paper]](https://arxiv.org/pdf/2304.07408.pdf)
- (arXiv 2023.06) Dynamic Clustering Transformer Network for Point Cloud Segmentation, [[Paper]](https://arxiv.org/pdf/2306.08073.pdf)

### Completion 
- (arXiv 2021.03) High-Fidelity Pluralistic Image Completion with Transformers, [[Paper]](https://arxiv.org/pdf/2103.14031.pdf), [[Code]](http://raywzy.com/ICT)
- (arXiv 2021.04) TFill: Image Completion via a Transformer-Based Architecture, [[Paper]](https://arxiv.org/pdf/2111.06707.pdf), [[Code]](https://github.com/yhlleo/MJP)
- (arXiv 2023.03) FishDreamer: Towards Fisheye Semantic Completion via Unified Image Outpainting and Segmentation, [[Paper]](https://arxiv.org/pdf/2303.13842.pdf), [[Code]](https://github.com/MasterHow/FishDreamer)
- (arXiv 2023.04) Contour Completion by Transformers and Its Application to Vector Font Data, [[Paper]](https://arxiv.org/pdf/2304.13988.pdf)
- (arXiv 2023.07) CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion, [[Paper]](https://arxiv.org/pdf/2307.07938.pdf)
- (arXiv 2023.10) Distance-based Weighted Transformer Network for Image Completion, [[Paper]](https://arxiv.org/pdf/2310.07440.pdf)
- (arXiv 2024.01) CRA-PCN: Point Cloud Completion with Intra- and Inter-level Cross-Resolution Transformers, [[Paper]](https://arxiv.org/pdf/2401.01552.pdf),[[Code]](https://github.com/EasyRy/CRA-PCN)

### Compression
- (arXiv 2021.10) Accelerating Framework of Transformer by hardware Design and Model Compression Co-Optimization, [[Paper]](https://arxiv.org/pdf/2110.10030.pdf)
- (arXiv 2021.11) Transformer-based Image Compression, [[Paper]](https://arxiv.org/pdf/2104.00845.pdf)
- (arXiv 2021.12) Towards End-to-End Image Compression and Analysis with Transformers, [[Paper]](https://arxiv.org/pdf/2112.09300.pdf), [[Code]](https://github.com/BYchao100/Towards-Image-Compression-and-Analysis-with-Transformers)
- (arXiv 2021.12) CSformer: Bridging Convolution and Transformer for Compressive Sensing, [[Paper]](https://arxiv.org/pdf/2112.15299.pdf)
- (arXiv 2022.01) Multi-Dimensional Model Compression of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.00043.pdf)
- (arXiv 2022.02) Entroformer: A Transformer-based Entropy Model for Learned Image Compression, [[Paper]](https://arxiv.org/pdf/2202.05492.pdf), [[Code]](https://github.com/mx54039q/entroformer)
- (arXiv 2022.03) Unified Visual Transformer Compression, [[Paper]](https://arxiv.org/pdf/2203.08243.pdf), [[Code]](https://github.com/VITA-Group/UVC)
- (arXiv 2022.03) Transformer Compressed Sensing via Global Image Tokens, [[Paper]](https://arxiv.org/pdf/2203.12861.pdf), [[supplementary]](https://github.com/uqmarlonbran/TCS)
- (arXiv 2022.03) Vision Transformer Compression with Structured Pruning and Low Rank Approximation, [[Paper]](https://arxiv.org/pdf/2203.13444.pdf)
- (arXiv 2022.04) Searching Intrinsic Dimensions of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.07722.pdf)
- (arXiv 2022.04) Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging, [[Paper]](https://arxiv.org/pdf/2205.10102.pdf)
- (arXiv 2022.06) VCT: A Video Compression Transformer, [[Paper]](https://arxiv.org/pdf/2206.07307.pdf), [[Code]](https://github.com/google-research/google-research/tree/master/vct)
- (arXiv 2022.07) TransCL: Transformer Makes Strong and Flexible Compressive Learning, [[Paper]](https://arxiv.org/pdf/2207.11972.pdf), [[Code]](https://github.com/MC-E/TransCL/)
- (arXiv 2022.08) Meta-DETR: Image-Level Few-Shot Detection with Inter-Class Correlation Exploitation, [[Paper]](https://arxiv.org/pdf/2208.00219.pdf), [[Code]](https://github.com/ZhangGongjie/Meta-DETR)
- (arXiv 2022.08) Unified Normalization for Accelerating and Stabilizing Transformers, [[Paper]](https://arxiv.org/pdf/2208.01313.pdf), [[Code]](https://github.com/hikvision-research/Unified-Normalization)
- (arXiv 2022.09) Uformer-ICS: A Specialized U-Shaped Transformer for Image Compressive Sensing, [[Paper]](https://arxiv.org/pdf/2209.01763.pdf)
- (arXiv 2022.09) Attacking Compressed Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.13785.pdf)
- (arXiv 2023.01) GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2301.05345.pdf)
- (arXiv 2023.03) SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage, [[Paper]](https://arxiv.org/pdf/2303.11114.pdf), [[Code]](https://github.com/naver-ai/seit)
- (arXiv 2023.03) Learned Image Compression with Mixed Transformer-CNN Architectures, [[Paper]](https://arxiv.org/pdf/2303.14978.pdf), [[Code]](https://github.com/jmliu206/LIC_TCM)
- (arXiv 2023.04) Optimization-Inspired Cross-Attention Transformer for Compressive Sensing, [[Paper]](https://arxiv.org/pdf/2304.13986.pdf), [[Code]](https://github.com/songjiechong/OCTUF)
- (arXiv 2023.05) ROI-based Deep Image Compression with Swin Transformers, [[Paper]](https://arxiv.org/pdf/2305.07783.pdf)
- (arXiv 2023.05) Transformer-based Variable-rate Image Compression with Region-of-interest Control, [[Paper]](https://arxiv.org/pdf/2305.10807.pdf)
- (arXiv 2023.06) Efficient Contextformer: Spatio-Channel Window Attention for Fast Context Modeling in Learned Image Compression, [[Paper]](https://arxiv.org/pdf/2306.14287.pdf)
- (arXiv 2023.07) AICT: An Adaptive Image Compression Transformer, [[Paper]](https://arxiv.org/pdf/2307.06091.pdf)
- (arXiv 2023.07) JPEG Quantized Coefficient Recovery via DCT Domain Spatial-Frequential Transformer, [[Paper]](https://arxiv.org/pdf/2308.09110.pdf)
- (arXiv 2023.09) Compressing Vision Transformers for Low-Resource Visual Learning, [[Paper]](https://arxiv.org/pdf/2309.02617.pdf)
- (arXiv 2023.09) CAIT: Triple-Win Compression towards High Accuracy, Fast Inference, and Favorable Transferability For ViTs, [[Paper]](https://arxiv.org/pdf/2309.15755.pdf)
- (arXiv 2023.10) USDC: Unified Static and Dynamic Compression for Visual Transformer, [[Paper]](https://arxiv.org/pdf/2310.11117.pdf)
- (arXiv 2023.10) Frequency-Aware Transformer for Learned Image Compression, [[Paper]](https://arxiv.org/pdf/2310.16387.pdf)
- (arXiv 2023.11) White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is, [[Paper]](https://arxiv.org/pdf/2311.13110.pdf), [[Code]](https://ma-lab-berkeley.github.io/CRATE)
- (arXiv 2023.11) Corner-to-Center Long-range Context Model for Efficient Learned Image Compression, [[Paper]](https://arxiv.org/pdf/2311.18103.pdf)
- (arXiv 2023.12) Input Compression with Positional Consistency for Efficient Training and Inference of Transformer Neural Networks, [[Paper]](https://arxiv.org/pdf/2312.12385.pdf), [[Code]](https://github.com/amrnag/ICPC)
- (arXiv 2024.01) UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.06426.pdf)
- (arXiv 2024.02) Memory-Efficient Vision Transformers: An Activation-Aware Mixed-Rank Compression Strategy, [[Paper]](https://arxiv.org/pdf/2402.06004.pdf)
- (arXiv 2024.03) Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer, [[Paper]](https://arxiv.org/pdf/2403.03736.pdf)
- (arXiv 2024.03) Content-aware Masked Image Modeling Transformer for Stereo Image Compression, [[Paper]](https://arxiv.org/pdf/2403.08505.pdf)

### Cross-view
- (arXiv 2022.03) Mutual Generative Transformer Learning for Cross-view Geo-localization, [[Paper]](https://arxiv.org/pdf/2203.09135.pdf)
- (arXiv 2022.04) TransGeo: Transformer Is All You Need for Cross-view Image Geo-localization, [[Paper]](https://arxiv.org/pdf/2204.00097.pdf), [[Code]](https://github.com/Jeff-Zilence/TransGeo2022)

### Crowd 
- (arXiv 2021.04) TransCrowd: Weakly-Supervised Crowd Counting with Transformer, [[Paper]](https://arxiv.org/pdf/2104.09116.pdf), [[Code]](https://github.com/dk-liang/TransCrowd)
- (arXiv 2021.05) Boosting Crowd Counting with Transformers, [[Paper]](https://arxiv.org/pdf/2105.10926.pdf), [[Code]](https://github.com/dk-liang/TransCrowd)
- (arXiv 2021.08) Congested Crowd Instance Localization with Dilated Convolutional Swin Transformer, [[Paper]](https://arxiv.org/pdf/2108.00584.pdf)
- (arXiv 2021.09) Audio-Visual Transformer Based Crowd Counting, [[Paper]](https://arxiv.org/pdf/2109.01926.pdf), [[Code]](https://github.com/rucv/avcc)
- (arXiv 2021.09) CCTrans: Simplifying and Improving Crowd Counting with Transformer, [[Paper]](https://arxiv.org/pdf/2109.14483.pdf)
- (arXiv 2022.01) Scene-Adaptive Attention Network for Crowd Counting, [[Paper]](https://arxiv.org/pdf/2112.15509.pdf)
- (arXiv 2022.03) An End-to-End Transformer Model for Crowd Localization, [[Paper]](https://arxiv.org/pdf/2202.13065.pdf)
- (arXiv 2022.03) Joint CNN and Transformer Network via weakly supervised Learning for efficient crowd counting, [[Paper]](https://arxiv.org/pdf/2203.06388.pdf)
- (arXiv 2022.06) Counting Varying Density Crowds Through Density Guided Adaptive Selection CNN and Transformer Estimation, [[Paper]](https://arxiv.org/pdf/2206.10075.pdf)
- (arXiv 2022.08) CounTR: Transformer-based Generalised Visual Counting, [[Paper]](https://arxiv.org/pdf/2208.13721.pdf)
- (arXiv 2023.01) RGB-T Multi-Modal Crowd Counting Based on Transformer, [[Paper]](https://arxiv.org/pdf/2301.03033.pdf), [[Code]](https://github.com/liuzywen/RGBTCC)
- (arXiv 2023.03) InCrowdFormer: On-Ground Pedestrian World Model From Egocentric Views, [[Paper]](https://arxiv.org/pdf/2303.09534.pdf)
- (arXiv 2023.05) Selecting Learnable Training Samples is All DETRs Need in Crowded Pedestrian Detection, [[Paper]](https://arxiv.org/pdf/2305.10801.pdf)
- (arXiv 2023.10) Query-adaptive DETR for Crowded Pedestrian Detection, [[Paper]](https://arxiv.org/pdf/2310.15725.pdf)
- (arXiv 2023.12) Regressor-Segmenter Mutual Prompt Learning for Crowd Counting, [[Paper]](https://arxiv.org/pdf/2312.01711.pdf)
- (arXiv 2024.01) Gramformer: Learning Crowd Counting via Graph-Modulated Transformer, [[Paper]](https://arxiv.org/pdf/2401.03870.pdf), [[Code]](https://github.com/LoraLinH/Gramformer)

### Deblurring
- (arXiv 2022.01) Flow-Guided Sparse Transformer for Video Deblurring,[[Paper]](https://arxiv.org/pdf/2201.01893.pdf)
- (arXiv 2022.04) Stripformer: Strip Transformer for Fast Image Deblurring, [[Paper]](https://arxiv.org/pdf/2204.04627.pdf)
- (arXiv 2022.04) VDTR: Video Deblurring with Transformer, [[Paper]](https://arxiv.org/pdf/2204.08023.pdf), [[Code]](https://github.com/ljzycmd/VDTR)
- (arXiv 2022.09) DMTNet: Dynamic Multi-scale Network for Dual-pixel Images Defocus Deblurring with Transformer, [[Paper]](https://arxiv.org/pdf/2209.06040.pdf)
- (arXiv 2022.11) Efficient Frequency Domain-based Transformers for High-Quality Image Deblurring, [[Paper]](https://arxiv.org/pdf/2211.12250.pdf), [[Code]](https://github.com/kkkls/FFTformer)
- (arXiv 2023.03) Image Deblurring by Exploring In-depth Properties of Transformer, [[Paper]](https://arxiv.org/pdf/2303.15198.pdf)
- (arXiv 2023.09) Aggregating Long-term Sharp Features via Hybrid Transformers for Video Deblurring, [[Paper]](https://arxiv.org/pdf/2309.07054.pdf), [[Code]](https://github.com/shangwei5/STGTN)
- (arXiv 2024.03) A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2403.02611.pdf), [[Code]](https://github.com/PieceZhang/MPT-CataBlur)
- (arXiv 2024.03) DeblurDiNAT: A Lightweight and Effective Transformer for Image Deblurring, [[Paper]](https://arxiv.org/pdf/2403.13163.pdf), [[Code]](https://github.com/HanzhouLiu/DeblurDiNAT.git)

### Depth
- (arXiv 2020.11) Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers, [[Paper]](https://arxiv.org/pdf/2011.02910.pdf), [[Code]](https://github.com/mli0603/stereo-transformer)
- (arXiv 2021.03) Vision Transformers for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2103.13413.pdf), [[Code]](https://github.com/intel-isl/DPT)
- (arXiv 2021.03) Transformers Solve the Limited Receptive Field for Monocular Depth Prediction, [[Paper]](https://arxiv.org/pdf/2103.12091.pdf), [[Code]](https://github.com/ygjwd12345/TransDepth)
- (arXiv 2021.09) Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2109.10563.pdf)
- (arXiv 2022.02) GLPanoDepth: Global-to-Local Panoramic Depth Estimation, [[Paper]](https://arxiv.org/pdf/2202.02796.pdf)
- (arXiv 2022.02) Transformers in Self-Supervised Monocular Depth Estimation with Unknown Camera Intrinsics, [[Paper]](https://arxiv.org/pdf/2202.03131.pdf)
- (arXiv 2022.03) OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion, [[Paper]](https://arxiv.org/pdf/2203.00838.pdf)
- (arXiv 2022.03) PanoFormer: Panorama Transformer for Indoor 360掳 Depth Estimation, [[Paper]](https://arxiv.org/pdf/2203.09283.pdf)
- (arXiv 2022.03) DepthGAN: GAN-based Depth Generation of Indoor Scenes from Semantic Layouts, [[Paper]](https://arxiv.org/pdf/2203.11453.pdf)
- (arXiv 2022.03) DepthFormer: Exploiting Long-Range Correlation and Local Information for Accurate Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2203.14211.pdf), [[Code]](https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox)
- (arXiv 2022.04) BinsFormer: Revisiting Adaptive Bins for Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2204.00987.pdf), [[Code]](https://github.com/zhyever/Monocular-Depth-Estimation-Toolbox)
- (arXiv 2022.04) SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation, [[Paper]](https://arxiv.org/pdf/2204.03636.pdf), [[Project]](https://surrounddepth.ivg-research.xyz/)
- (arXiv 2022.04) Multi-Frame Self-Supervised Depth with Transformers, [[Paper]](https://arxiv.org/pdf/2204.07616.pdf), [[Project]](https://sites.google.com/tri.global/depthformer)
- (arXiv 2022.05) SideRT: A Real-time Pure Transformer Architecture for Single Image Depth Estimation, [[Paper]](https://arxiv.org/pdf/2204.13892.pdf)
- (arXiv 2022.05) Depth Estimation with Simplified Transformer, [[Paper]](https://arxiv.org/pdf/2204.13791.pdf)
- (arXiv 2022.05) MonoFormer: Towards Generalization of self-supervised monocular depth estimation with Transformers, [[Paper]](https://arxiv.org/pdf/2205.11083.pdf)
- (arXiv 2022.06) SparseFormer: Attention-based Depth Completion Network, [[Paper]](https://arxiv.org/pdf/2206.04557.pdf)
- (arXiv 2022.06) Forecasting of depth and ego-motion with transformers and self-supervision, [[Paper]](https://arxiv.org/pdf/2206.07435.pdf)
- (arXiv 2022.07) Depthformer : Multiscale Vision Transformer For Monocular Depth Estimation With Local Global Information Fusion, [[Paper]](https://arxiv.org/pdf/2207.04535.pdf), [[Code]](https://github.com/ashutosh1807/Depthformer.git)
- (arXiv 2022.08) MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer, [[Paper]](https://arxiv.org/pdf/2208.03543.pdf), [[Code]](https://github.com/zxcqlf/MonoViT)
- (arXiv 2022.09) TODE-Trans: Transparent Object Depth Estimation with Transformer, [[Paper]](https://arxiv.org/pdf/2209.08455.pdf), [[Code]](https://github.com/yuchendoudou/TODE)
- (arXiv 2022.10) Context-Enhanced Stereo Transformer, [[Paper]](https://arxiv.org/pdf/2210.11719.pdf), [[Code]](https://github.com/guoweiyu/Context-Enhanced-Stereo-Transformer)
- (arXiv 2022.11) Hybrid Transformer Based Feature Fusion for Self-Supervised Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2211.11066.pdf)
- (arXiv 2022.11) Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2211.13202.pdf), [[Code]](https://github.com/noahzn/lite-mono)
- (arXiv 2022.12) Event-based Monocular Dense Depth Estimation with Recurrent Transformers, [[Paper]](https://arxiv.org/pdf/2212.02791.pdf)
- (arXiv 2022.12) ROIFormer: Semantic-Aware Region of Interest Transformer for Efficient Self-Supervised Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2212.05729.pdf)
- (arXiv 2023.01) Dyna-DepthFormer: Multi-frame Transformer for Self-Supervised Depth Estimation in Dynamic Scenes, [[Paper]](https://arxiv.org/pdf/2301.05871.pdf)
- (arXiv 2023.01) SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network, [[Paper]](https://arxiv.org/pdf/2301.06715.pdf)
- (arXiv 2023.02) URCDC-Depth: Uncertainty Rectified Cross-Distillation with CutFlip for Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2302.08149.pdf), [[Code]](https://github.com/ShuweiShao/URCDC-Depth)
- (arXiv 2023.03) STDepthFormer: Predicting Spatio-temporal Depth from Video with a Self-supervised Transformer Model, [[Paper]](https://arxiv.org/pdf/2303.01196.pdf), [[Code]](https://github.com/ShuweiShao/URCDC-Depth)
- (arXiv 2023.03) DwinFormer: Dual Window Transformers for End-to-End Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2303.02968.pdf)
- (arXiv 2023.03) DEHRFormer: Real-time Transformer for Depth Estimation and Haze Removal from Varicolored Haze Scenes, [[Paper]](https://arxiv.org/pdf/2303.06905.pdf)
- (arXiv 2023.03) Channel-Aware Distillation Transformer for Depth Estimation on Nano Drones, [[Paper]](https://arxiv.org/pdf/2303.10386.pdf)
- (arXiv 2023.04) EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation, [[Paper]](https://arxiv.org/pdf/2304.07803.pdf)
- (arXiv 2023.04) CompletionFormer: Depth Completion with Convolutions and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2304.13030.pdf), [[Code]](https://github.com/youmi-zym/CompletionFormer)
- (arXiv 2023.08) Improving Depth Gradient Continuity in Transformers: A Comparative Study on Monocular Depth Estimation with CNN, [[Paper]](https://arxiv.org/pdf/2308.08333.pdf)
- (arXiv 2023.08) Semi-Supervised Semantic Depth Estimation using Symbiotic Transformer and NearFarMix Augmentation, [[Paper]](https://arxiv.org/pdf/2308.14400.pdf)
- (arXiv 2023.09) SQLdepth: Generalizable Self-Supervised Fine-Structured Monocular Depth Estimation, [[Paper]](https://arxiv.org/pdf/2309.00526.pdf), [[Code]](https://github.com/hisfog/SQLdepth-Impl)
- (arXiv 2023.10) GSDC Transformer: An Efficient and Effective Cue Fusion for Monocular Multi-Frame Depth Estimation, [[Paper]](https://arxiv.org/pdf/2309.17059.pdf)
- (arXiv 2023.10) FocDepthFormer: Transformer with LSTM for Depth Estimation from Focus, [[Paper]](https://arxiv.org/pdf/2309.17059.pdf)
- (arXiv 2023.10) Metrically Scaled Monocular Depth Estimation through Sparse Priors for Underwater Robots, [[Paper]](https://arxiv.org/pdf/2310.16750.pdf), [[Code]](https://github.com/ebnerluca/uw_depth)
- (arXiv 2023.12) Transformers in Unsupervised Structure-from-Motion, [[Paper]](https://arxiv.org/pdf/2312.10529.pdf), [[Code]](https://github.com/NeurAI-Lab/MT-SfMLearner)
- (arXiv.2024.02) CLIP Can Understand Depth, [[Paper]](https://arxiv.org/pdf/2402.03251.pdf)
- (arXiv.2024.03) Depth Estimation Algorithm Based on Transformer-Encoder and Feature Fusion, [[Paper]](https://arxiv.org/pdf/2403.01370.pdf)
- (arXiv.2024.03) METER: a mobile vision transformer architecture for monocular depth estimation, [[Paper]](https://arxiv.org/pdf/2403.08368.pdf)
- (arXiv.2024.03) SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption of Monocular Depth Estimation in Autonomous Navigation Applications, [[Paper]](https://arxiv.org/pdf/2403.11515.pdf)

### Deepfake Detection
- (arXiv.2021.02) Deepfake Video Detection Using Convolutional Vision Transformer, [[Paper]](https://arxiv.org/abs/2102.11126)
- (arXiv 2021.04) Deepfake Detection Scheme Based on Vision Transformer and Distillation, [[Paper]](https://arxiv.org/abs/2104.01353)
- (arXiv 2021.04) M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection, [[Paper]](https://arxiv.org/pdf/2104.09770.pdf)
- (arXiv 2021.07) Combining EfficientNet and Vision Transformers for Video Deepfake Detection, [[Paper]](https://arxiv.org/pdf/2107.02612.pdf)
- (arXiv 2021.08) Video Transformer for Deepfake Detection with Incremental Learning, [[Paper]](https://arxiv.org/pdf/2108.05307.pdf)
- (arXiv 2022.03) Self-supervised Transformer for Deepfake Detection, [[Paper]](https://arxiv.org/pdf/2203.01265.pdf), [[Code]](https://github.com/IDKiro/DehazeFormer)
- (arXiv 2022.06) Cross-Forgery Analysis of Vision Transformers and CNNs for Deepfake Image Detection, [[Paper]](https://arxiv.org/pdf/2206.13829.pdf)
- (arXiv 2022.07) Deepfake Video Detection with Spatiotemporal Dropout Transformer, [[Paper]](https://arxiv.org/pdf/2207.06612.pdf)
- (arXiv 2022.07) Hybrid Transformer Network for Deepfake Detection, [[Paper]](https://arxiv.org/pdf/2208.05820.pdf)
- (arXiv 2022.09) Deep Convolutional Pooling Transformer for Deepfake Detection, [[Paper]](https://arxiv.org/pdf/2303.07033.pdf)
- (arXiv 2023.04) Deepfake Detection with Deep Learning: Convolutional Neural Networks versus Transformers, [[Paper]](https://arxiv.org/pdf/2304.03698.pdf)
- (arXiv 2023.07) Deepfake Video Detection Using Generative Convolutional Vision Transformer, [[Paper]](https://arxiv.org/pdf/2307.07036.pdf), [[Code]](https://github.com/erprogs/GenConViT)
- (arXiv 2023.07) Self-Supervised Graph Transformer for Deepfake Detection, [[Paper]](https://arxiv.org/pdf/2307.15019.pdf)
- (arXiv 2023.09) DF-TransFusion: Multimodal Deepfake Detection via Lip-Audio Cross-Attention and Facial Self-Attention, [[Paper]](https://arxiv.org/pdf/2309.06511.pdf)
- (arXiv 2024.03) TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer, [[Paper]](https://arxiv.org/pdf/2403.12481.pdf)

### Diffusion
- (arXiv 2022.12) Scalable Diffusion Models with Transformers, [[Paper]](https://arxiv.org/pdf/2212.09748.pdf), [[Code]](https://www.wpeebles.com/DiT)
- (arXiv 2023.03) Masked Diffusion Transformer is a Strong Image Synthesizer, [[Paper]](https://arxiv.org/pdf/2303.14389.pdf), [[Code]](https://github.com/sail-sg/MDT)
- (arXiv 2023.04) ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis, [[Paper]](https://arxiv.org/pdf/2304.01053.pdf)
- (arXiv 2023.06) DFormer: Diffusion-guided Transformer for Universal Image Segmentation, [[Paper]](https://arxiv.org/pdf/2306.03437.pdf), [[Code]](https://github.com/cp3wan/DFormer)
- (arXiv 2023.08) Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers, [[Paper]](https://arxiv.org/pdf/2308.14152.pdf)
- (arXiv 2023.09) Large-Vocabulary 3D Diffusion Model with Transformer, [[Paper]](https://arxiv.org/pdf/2309.07920.pdf), [[Project]](https://ziangcao0312.github.io/difftf_pages/)
- (arXiv 2023.09) Cartoondiff: Training-free Cartoon Image Generation with Diffusion Transformer Models, [[Paper]](https://arxiv.org/pdf/2309.08251.pdf), [[Project]](https://cartoondiff.github.io/)
- (arXiv 2023.12) DiffiT: Diffusion Vision Transformers for Image Generation, [[Paper]](https://arxiv.org/pdf/2312.02139.pdf)
- (arXiv 2023.12) DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2312.06400.pdf)
- (arXiv 2024.01) SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers, [[Paper]](https://arxiv.org/pdf/2401.08740.pdf), [[Code]](https://github.com/willisma/SiT)
- (arXiv 2024.01) Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2401.11605.pdf), [[Code]](https://crowsonkb.github.io/hourglass-diffusion-transformers)
- (arXiv 2024.02) Cross-view Masked Diffusion Transformers for Person Image Synthesis, [[Paper]](https://arxiv.org/pdf/2402.01516.pdf)
- (arXiv 2024.02) FiT: Flexible Vision Transformer for Diffusion Model, [[Paper]](https://arxiv.org/pdf/2402.12376.pdf), [[Code]](https://github.com/whlzy/FiT)
- (arXiv 2024.03) Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts, [[Paper]](https://arxiv.org/pdf/2403.09176.pdf), [[Code]](https://byeongjun-park.github.io/Switch-DiT/)

### Dehazing
- (arXiv 2021.09) Hybrid Local-Global Transformer for Image Dehazing, [[Paper]](https://arxiv.org/pdf/2109.07100.pdf)
- (arXiv 2022.04) Vision Transformers for Single Image Dehazing, [[Paper]](https://arxiv.org/pdf/2204.03883.pdf)
- (arXiv 2022.10) Semi-UFormer: Semi-supervised Uncertainty-aware Transformer for Image Dehazing, [[Paper]](https://arxiv.org/pdf/2210.16057.pdf)
- (arXiv 2023.03) SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency, [[Paper]](https://arxiv.org/pdf/2210.16057.pdf)
- (arXiv 2023.04) A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer, [[Paper]](https://arxiv.org/pdf/2304.07874.pdf), [[Code]](https://github.com/yangyiliu21/ntire2023_ITBdehaze)
- (arXiv 2023.05) NightHazeFormer: Single Nighttime Haze Removal Using Prior Query Transformer, [[Paper]](https://arxiv.org/pdf/2305.09533.pdf)
- (arXiv 2023.08) MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing, [[Paper]](https://arxiv.org/pdf/2308.14036.pdf), [[Code]](https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer)
- (arXiv 2023.12) DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing, [[Paper]](https://arxiv.org/pdf/2312.09955.pdf)
- (arXiv 2024.01) WaveletFormerNet: A Transformer-based Wavelet Network for Real-world Non-homogeneous and Dense Fog Removal, [[Paper]](https://arxiv.org/pdf/2401.04550.pdf)

### Deraining
- (arXiv 2022.04) DRT: A Lightweight Single Image Deraining Recursive Transformer, [[Paper]](https://arxiv.org/pdf/2204.11385.pdf)
- (arXiv 2022.07) Magic ELF: Image Deraining Meets Association Learning and Transformer, [[Paper]](https://arxiv.org/pdf/2207.10455.pdf), [[Code]](https://github.com/kuijiang94/Magic-ELF)
- (arXiv 2023.03) Learning A Sparse Transformer Network for Effective Image Deraining, [[Paper]](https://arxiv.org/pdf/2303.11950.pdf), [[Code]](https://github.com/cschenxiang/DRSformer)
- (arXiv 2023.08) Learning Image Deraining Transformer Network with Dynamic Dual Self-Attention, [[Paper]](https://arxiv.org/pdf/2303.11950.pdf)
- (arXiv 2023.08) Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks, [[Paper]](https://arxiv.org/pdf/2308.14153.pdf)
- (arXiv 2024.01) NightRain: Nighttime Video Deraining via Adaptive-Rain-Removal and Adaptive-Correction, [[Paper]](https://arxiv.org/pdf/2401.00729.pdf)
- (arXiv 2024.02) Diving Deep into Regions: Exploiting Regional Information Transformer for Single Image Deraining, [[Paper]](https://arxiv.org/pdf/2402.16033.pdf), [[Code]](https://github.com/ztMotaLee/Regformer)
- (arXiv 2024.03) Gabor-guided transformer for single image deraining, [[Paper]](https://arxiv.org/pdf/2403.07380.pdf)

### Denoising
- (arXiv 2021.12) Neuromorphic Camera Denoising using Graph Neural Network-driven Transformers, [[Paper]](https://arxiv.org/pdf/2112.09685.pdf)
- (arXiv 2022.03) Transformers in Self-Supervised Monocular Depth Estimation with Unknown Camera Intrinsics, [[Paper]](https://arxiv.org/pdf/2202.14009.pdf), [[Code]](https://github.com/FanChiMao/SUNet)
- (arXiv 2022.03) Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis, [[Paper]](https://arxiv.org/pdf/2203.13278.pdf), [[Code]](https://github.com/cszn/SCUNet)
- (arXiv 2022.05) Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer, [[Paper]](https://arxiv.org/pdf/2205.00214.pdf)
- (arXiv 2022.05) Dense residual Transformer for image denoising, [[Paper]](https://arxiv.org/pdf/2205.00214.pdf)
- (arXiv 2022.07) DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer, [[Paper]](https://arxiv.org/pdf/2207.13861.pdf)
- (arXiv 2022.11) Spatial-Spectral Transformer for Hyperspectral Image Denoising, [[Paper]](https://arxiv.org/pdf/2211.14090.pdf), [[Code]](https://github.com/myuli/sst)
- (arXiv 2023.03) Xformer: Hybrid X-Shaped Transformer for Image Denoising, [[Paper]](https://arxiv.org/pdf/2303.06440.pdf)
- (arXiv 2023.03) Hybrid Spectral Denoising Transformer with Learnable Query, [[Paper]](https://arxiv.org/pdf/2303.09040.pdf)
- (arXiv 2023.04) Spectral Enhanced Rectangle Transformer for Hyperspectral Image Denoising, [[Paper]](https://arxiv.org/pdf/2303.09040.pdf), [[Code]](https://github.com/MyuLi/SERT)
- (arXiv 2023.04) Exploration of Lightweight Single Image Denoising with Transformers and Truly Fair Training, [[Paper]](https://arxiv.org/pdf/2304.01805.pdf), [[Code]](https://github.com/rami0205/LWDN)
- (arXiv 2023.04) Self-Supervised Image Denoising for Real-World Images with Context-aware Transformer, [[Paper]](https://arxiv.org/pdf/2304.01627.pdf)
- (arXiv 2023.04) DDT: Dual-branch Deformable Transformer for Image Denoising, [[Paper]](https://arxiv.org/pdf/2304.06346.pdf), [[Code]](https://github.com/Merenguelkl/DDT)
- (arXiv 2023.04) EWT: Efficient Wavelet-Transformer for Single Image Denoising, [[Paper]](https://arxiv.org/pdf/2304.06274.pdf)
- (arXiv 2023.04) NoiseTrans: Point Cloud Denoising with Transformers, [[Paper]](https://arxiv.org/pdf/2304.11812.pdf)
- (arXiv 2023.05) RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset, [[Paper]](https://arxiv.org/pdf/2305.00767.pdf)
- (arXiv 2023.05) Degradation-Noise-Aware Deep Unfolding Transformer for Hyperspectral Image Denoising, [[Paper]](https://arxiv.org/pdf/2305.04047.pdf)
- (arXiv 2023.10) Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising, [[Paper]](https://arxiv.org/pdf/2310.09126.pdf)
- (arXiv 2023.10) A cross Transformer for image denoising, [[Paper]](https://arxiv.org/pdf/2310.10408.pdf), [[Code]](https://github.com/hellloxiaotian/CTNet)
- (arXiv 2023.10) Complex Image Generation SwinTransformer Network for Audio Denoising, [[Paper]](https://arxiv.org/pdf/2310.16109.pdf)
- (arXiv 2024.01) Denoising Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.02957.pdf), [[Code]](https://jiawei-yang.github.io/DenoisingViT/)
- (arXiv 2024.01) Hyperspectral Image Denoising via Spatial-Spectral Recurrent Transformer, [[Paper]](https://arxiv.org/pdf/2401.03885.pdf), [[Code]](https://github.com/lronkitty/SSRT)

### Detection
- (ECCV'20) DETR: End-to-End Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2005.12872), [[Code]](https://github.com/facebookresearch/detr)
- (ICLR'21) Deformable DETR: Deformable Transformers for End-to-End Object Detection, [[Paper]](https://arxiv.org/pdf/2010.04159), [[Code]](https://github.com/fundamentalvision/Deformable-DETR)
- (CVPR'21) UP-DETR: Unsupervised Pre-training for Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2011.09094), [[Code]](https://github.com/dddzg/up-detr)
- (arXiv 2020.11) End-to-End Object Detection with Adaptive Clustering Transformer, [[Paper]](https://arxiv.org/pdf/2011.09315)
- (arXiv 2020.11) Rethinking Transformer-based Set Prediction for Object Detection, [[Paper]](https://arxiv.org/pdf/2011.10881)
- (arXiv 2020.12) Toward Transformer-Based Object Detection, [[Paper]](https://arxiv.org/pdf/2012.09958)
- (arXiv 2020.12) DETR for Pedestrian Detection, [[Paper]](https://arxiv.org/pdf/2012.06785)
- (arXiv 2021.01) Line Segment Detection Using Transformers without Edges, [[Paper]](https://arxiv.org/abs/2101.01909)
- (arXiv 2021.01) Fast Convergence of DETR with Spatially Modulated Co-Attention, [[Paper]](https://arxiv.org/pdf/2101.07448.pdf)
- (arXiv 2021.02) GEM: Glare or Gloom, I Can Still See You 鈥? End-to-End Multimodal Object Detector, [[Paper]](https://arxiv.org/pdf/2102.12319.pdf)
- (arXiv 2021.03) SSTN: Self-Supervised Domain Adaptation Thermal Object Detection for Autonomous Driving, [[Paper]](https://arxiv.org/abs/2103.03150)
- (arXiv 2021.03) Meta-DETR: Few-Shot Object Detection via Unified Image-Level Meta-Learning, [[Paper]](https://arxiv.org/pdf/2103.11731.pdf)
- (arXiv 2021.03) CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification, [[Paper]](https://arxiv.org/abs/2103.14899)
- (arXiv 2021.03) DA-DETR: Domain Adaptive Detection Transformer by Hybrid Attention, [[Paper]](https://arxiv.org/abs/2103.17084)
- (arXiv 2021.04) Efficient DETR: Improving End-to-End Object Detector with Dense Prior, [[Paper]](https://arxiv.org/pdf/2104.01318.pdf)
- (arXiv 2021.04) Points as Queries: Weakly Semi-supervised Object Detection by Points, [[Paper]](https://arxiv.org/pdf/2104.07434.pdf)
- (arXiv 2021.04) CAT: Cross-Attention Transformer for One-Shot Object Detection, [[Paper]](https://arxiv.org/pdf/2104.14984.pdf)
- (arXiv 2021.05) Content-Augmented Feature Pyramid Network with Light Linear Transformers, [[Paper]](https://arxiv.org/pdf/2105.09464.pdf)
- (arXiv 2021.06) You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection, [[Paper]](https://arxiv.org/pdf/2106.00666.pdf)
- (arXiv 2021.06) DETReg: Unsupervised Pretraining with Region Priors for Object Detection, [[Paper]](https://arxiv.org/pdf/2106.04550.pdf),[[Project]](https://www.amirbar.net/detreg/)
- (arXiv 2021.06) Oriented Object Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2106.03146.pdf)
- (arXiv 2021.06) MODETR: Moving Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2106.11422.pdf)
- (arXiv 2021.07) ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer, [[Paper]](https://arxiv.org/pdf/2107.05887.pdf)
- (arXiv 2021.07) OODformer: Out-Of-Distribution Detection Transformer, [[Paper]](https://arxiv.org/pdf/2107.08976.pdf)
- (arXiv 2021.07) Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers, [[Paper]](https://arxiv.org/pdf/2107.12636.pdf),[[Code]](https://github.com/encounter1997/SFA)
- (arXiv 2021.08) Fast Convergence of DETR with Spatially Modulated Co-Attention, [[Paper]](https://arxiv.org/pdf/2108.02404.pdf),[[Code]](https://github.com/gaopengcuhk/SMCA-DETR)
- (arXiv 2021.08) PSViT: Better Vision Transformer via Token Pooling and Attention Sharing, [[Paper]](https://arxiv.org/pdf/2108.03428.pdf)
- (arXiv 2021.08) Multiview Detection with Shadow Transformer (and View-Coherent Data Augmentation), [[Paper]](https://arxiv.org/pdf/2108.05888.pdf),[[Code]](https://github.com/hou-yz/MVDeTr)
- (arXiv 2021.08) Conditional DETR for Fast Training Convergence, [[Paper]](https://arxiv.org/pdf/2108.06152.pdf),[[Code]](https://github.com/Atten4Vis/ConditionalDETR)
- (arXiv 2021.08) Guiding Query Position and Performing Similar Attention for Transformer-Based Detection Heads, [[Paper]](https://arxiv.org/pdf/2108.09691.pdf)
- (arXiv 2021.08) TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios, [[Paper]](https://arxiv.org/pdf/2108.11539.pdf)
- (arXiv 2021.09) Anchor DETR: Query Design for Transformer-Based Detector, [[Paper]](https://arxiv.org/pdf/2109.07107.pdf),[[Code]](https://github.com/megvii-model/AnchorDETR)
- (arXiv 2021.09) SDTP: Semantic-aware Decoupled Transformer Pyramid for Dense Image Prediction, [[Paper]](https://arxiv.org/pdf/2109.08963.pdf)
- (arXiv 2021.09) Infrared Small-Dim Target Detection with Transformer under Complex Backgrounds, [[Paper]](https://arxiv.org/pdf/2109.14379.pdf)
- (arXiv 2021.10) IViDT: An Efficient and Effective Fully Transformer-based Object Detector, [[Paper]](https://arxiv.org/pdf/2110.03921.pdf),[[Code]](https://github.com/naver-ai/vidt)
- (arXiv 2021.10) DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries, [[Paper]](https://arxiv.org/pdf/2110.06922.pdf),[[Code]](https://github.com/WangYueFt/detr3d)
- (arXiv 2021.10) CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector, [[Paper]](https://arxiv.org/pdf/2110.12364.pdf),[[Code]](https://github.com/albert-jin/CvT-ASSD)
- (arXiv 2021.11) Cross-Modality Fusion Transformer for Multispectral Object Detection, [[Paper]](https://arxiv.org/pdf/2111.00273.pdf),[[Code]](https://github.com/DocF/multispectral-object-detection)
- (arXiv 2021.11) Benchmarking Detection Transfer Learning with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.11429.pdf)
- (arXiv 2021.11) BoxeR: Box-Attention for 2D and 3D Transformers, [[Paper]](https://arxiv.org/pdf/2111.13087.pdf)
- (arXiv 2021.11) Sparse DETR: Efficient End-to-End Object Detection with Learnable Sparsity, [[Paper]](https://arxiv.org/pdf/2111.14330.pdf), [[Code]](https://github.com/kakaobrain/sparse-detr)
- (arXiv 2021.12) OW-DETR: Open-world Detection Transformer, [[Paper]](https://arxiv.org/pdf/2112.01513.pdf)
- (arXiv 2021.12) Recurrent Glimpse-based Decoder for Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2112.04632.pdf), [[Code]](https://github.com/zhechen/Deformable-DETR-REGO)
- (arXiv 2021.12) BEVDet: High-Performance Multi-Camera 3D Object Detection in Bird-Eye-View, [[Paper]](https://arxiv.org/pdf/2112.11790.pdf)
- (arXiv 2021.12) Miti-DETR: Object Detection based on Transformers with Mitigatory Self-Attention Convergence, [[Paper]](https://arxiv.org/pdf/2112.13310.pdf)
- (arXiv 2022.01) Pedestrian Detection: Domain Generalization, CNNs, Transformers and Beyond, [[Paper]](https://arxiv.org/pdf/2201.03176.pdf), [[Code]](https://github.com/hasanirtiza/Pedestron)
- (arXiv 2022.01) DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR, [[Paper]](https://arxiv.org/pdf/2201.12329.pdf), [[Code]](https://github.com/SlongLiu/DAB-DETR)
- (arXiv 2022.03) DN-DETR: Accelerate DETR Training by Introducing Query DeNoising, [[Paper]](https://arxiv.org/pdf/2203.01305.pdf), [[Code]](https://github.com/FengLi-ust/DN-DETR)
- (arXiv 2022.03) DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection, [[Paper]](https://arxiv.org/pdf/2203.03605.pdf), [[Code]](https://github.com/IDEACVR/DINO)
- (arXiv 2022.03) Knowledge Amalgamation for Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2203.03187.pdf)
- (arXiv 2022.03) Accelerating DETR Convergence via Semantic-Aligned Matching, [[Paper]](https://arxiv.org/pdf/2203.06883.pdf), [[Code]](https://github.com/ZhangGongjie/SAM-DETR)
- (arXiv 2022.03) Progressive End-to-End Object Detection in Crowded Scenes, [[Paper]](https://arxiv.org/pdf/2203.07669.pdf), [[Code]](https://github.com/megvii-model/Iter-E2EDET)
- (arXiv 2022.03) Towards Data-Efficient Detection Transformers, [[Paper]](https://arxiv.org/pdf/2203.09507.pdf), [[Code]](https://github.com/encounter1997/DE-DETRs)
- (arXiv 2022.03) Semantic-aligned Fusion Transformer for One-shot Object Detection, [[Paper]](https://arxiv.org/pdf/2203.09093.pdf)
- (arXiv 2022.03) MonoDTR: Monocular 3D Object Detection with Depth-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2203.10981.pdf)
- (arXiv 2022.03) TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2203.11496.pdf), [[Code]](https://github.com/XuyangBai/TransFusion/)
- (arXiv 2022.03) Open-Vocabulary DETR with Conditional Matching, [[Paper]](https://arxiv.org/pdf/2203.11876.pdf)
- (arXiv 2022.03) MonoDETR: Depth-aware Transformer for Monocular 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2203.13310.pdf), [[Code]](https://github.com/ZrrSkywalker/MonoDETR.git)
- (arXiv 2022.03) Few-Shot Object Detection with Fully Cross-Transformer, [[Paper]](https://arxiv.org/pdf/2203.15021.pdf)
- (arXiv 2022.03) Exploring Plain Vision Transformer Backbones for Object Detection, [[Paper]](https://arxiv.org/pdf/2203.16527.pdf)
- (arXiv 2022.03) Omni-DETR: Omni-Supervised Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2203.16089.pdf), [[Code]](https://github.com/amazon-research/omni-detr)
- (arXiv 2022.04) CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2204.00325.pdf)
- (arXiv 2022.04) Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection, [[Paper]](https://arxiv.org/pdf/2204.02964.pdf), [[Code]](https://github.com/hustvl/MIMDet)
- (arXiv 2022.04) An Extendable, Efficient and Effective Transformer-based Object Detector, [[Paper]](https://arxiv.org/pdf/2204.07962.pdf), [[Code]](https://github.com/naver-ai/vidt)
- (arXiv 2022.04) Learning Future Object Prediction with a Spatiotemporal Detection Transformer, [[Paper]](https://arxiv.org/pdf/2204.10321.pdf)
- (arXiv 2022.04) DFAM-DETR: Deformable feature based attention mechanism DETR on slender object detection, [[Paper]](https://arxiv.org/pdf/2204.10667.pdf)
- (arXiv 2022.04) Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2204.11582.pdf)
- (arXiv 2022.05) Incremental-DETR: Incremental Few-Shot Object Detection via Self-Supervised Learning, [[Paper]](https://arxiv.org/pdf/2205.04042.pdf)
- (arXiv 2022.05) An Empirical Study of Self-supervised Learning Approaches for Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2205.05543.pdf), [[Code1]](https://github.com/gokulkarthik/detr), [[Code2]](https://github.com/gokulkarthik/Deformable-DETR)
- (arXiv 2022.05) Simple Open-Vocabulary Object Detection with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.06230.pdf), [[Code]](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)
- (arXiv 2022.05) Vision Transformer Adapter for Dense Predictions, [[Paper]](https://arxiv.org/pdf/2205.08534.pdf), [[Code]](https://github.com/czczup/ViT-Adapter)
- (arXiv 2022.05) Integral Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection, [[Paper]](https://arxiv.org/pdf/2205.09613.pdf)
- (arXiv 2022.05) Boosting Camouflaged Object Detection with Dual-Task Interactive Transformer, [[Paper]](https://arxiv.org/pdf/2205.10579.pdf),[[Code]](https://github.com/liuzywen/COD)
- (arXiv 2022.05) Transformer-based out-of-distribution detection for clinically safe segmentation, [[Paper]](https://arxiv.org/pdf/2205.10650.pdf)
- (arXiv 2022.05) AO2-DETR: Arbitrary-Oriented Object Detection Transformer, [[Paper]](https://arxiv.org/pdf/2205.12785.pdf),[[Code]](https://github.com/Ixiaohuihuihui)
- (arXiv 2022.06) Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation, [[Paper]](https://arxiv.org/pdf/2206.02777.pdf),[[Code]](https://github.com/IDEACVR/MaskDINO)
- (arXiv 2022.06) DETR++: Taming Your Multi-Scale Detection Transformer, [[Paper]](https://arxiv.org/pdf/2206.02977.pdf),[[Code]](https://github.com/IDEACVR/MaskDINO)
- (arXiv 2022.06) Visual Transformer for Object Detection, [[Paper]](https://arxiv.org/pdf/2206.06323.pdf)
- (arXiv 2022.06) Efficient Decoder-free Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2206.06829.pdf)
- (arXiv 2022.07) QKVA grid: Attention in Image Perspective and Stacked DETR, [[Paper]](https://arxiv.org/pdf/2207.04313.pdf),[[Code]](https://github.com/shengwenyuan/sdetr)
- (arXiv 2022.07) Symmetry-Aware Transformer-based Mirror Detection, [[Paper]](https://arxiv.org/pdf/2207.06332.pdf),[[Code]](https://github.com/tyhuang0428/SATNet)
- (arXiv 2022.07) Transformer-based Context Condensation for Boosting Feature Pyramids in Object Detection, [[Paper]](https://arxiv.org/pdf/2207.06603.pdf)
- (arXiv 2022.07) Defect Transformer: An Efficient Hybrid Transformer Architecture for Surface Defect Detection, [[Paper]](https://arxiv.org/pdf/2207.08319.pdf)
- (arXiv 2022.07) Conditional DETR V2: Efficient Detection Transformer with Box Queries, [[Paper]](https://arxiv.org/pdf/2207.08914.pdf)
- (arXiv 2022.07) Group DETR: Fast Training Convergence with Decoupled One-to-Many Label Assignment, [[Paper]](https://arxiv.org/pdf/2207.13085.pdf)
- (arXiv 2022.07) DETRs with Hybrid Matching, [[Paper]](https://arxiv.org/pdf/2207.13080.pdf), [[Code]](https://github.com/HDETR)
- (arXiv 2022.07) Semantic-Aligned Matching for Enhanced DETR Convergence and Multi-Scale Feature Fusion, [[Paper]](https://arxiv.org/pdf/2207.14172.pdf),[[Code]](https://github.com/ZhangGongjie/SAM-DETR)
- (arXiv 2022.08) An Empirical Study of Pseudo-Labeling for Image-based 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2208.07137.pdf)
- (arXiv 2022.08) Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors, [[Paper]](https://arxiv.org/pdf/2208.11356.pdf), [[Project]](https://github.com/ZhangGongjie/IMFA)
- (arXiv 2022.08) Swin-transformer-yolov5 For Real-time Wine Grape Bunch Detection, [[Paper]](https://arxiv.org/pdf/2208.14508.pdf)
- (arXiv 2022.09) SEFormer: Structure Embedding Transformer for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2209.01745.pdf)
- (arXiv 2022.09) Vision Transformers and YoloV5 based Driver Drowsiness Detection Framework, [[Paper]](https://arxiv.org/pdf/2209.01401.pdf)
- (arXiv 2022.09) Sar Ship Detection based on Swin Transformer and Feature Enhancement Feature Pyramid Network, [[Paper]](https://arxiv.org/pdf/2209.10421.pdf)
- (arXiv 2022.09) CrossDTR: Cross-view and Depth-guided Transformers for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2209.13507.pdf),[[Code]](https://github.com/sty61010/CrossDTR)
- (arXiv 2022.09) A lightweight Transformer-based model for fish landmark detection, [[Paper]](https://arxiv.org/pdf/2209.05777.pdf)
- (arXiv 2022.09) ComplETR: Reducing the cost of annotations for object detection in dense scenes with vision transformers, [[Paper]](https://arxiv.org/pdf/2209.05654.pdf)
- (arXiv 2022.09) CenterFormer: Center-based Transformer for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2209.05588.pdf), [[Project]](https://github.com/TuSimple/centerformer)
- (arXiv 2022.09) CRAFT: Camera-Radar 3D Object Detection with Spatio-Contextual Fusion Transformer, [[Paper]](https://arxiv.org/pdf/2209.06535.pdf)
- (arXiv 2022.10) Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning, [[Paper]](https://arxiv.org/pdf/2210.01035.pdf)
- (arXiv 2022.10) MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer for Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2210.15316.pdf)
- (arXiv 2022.10) Li3DeTr: A LiDAR based 3D Detection Transformer, [[Paper]](https://arxiv.org/pdf/2210.15365.pdf)
- (arXiv 2022.10) Pair DETR: Contrastive Learning Speeds Up DETR Training, [[Paper]](https://arxiv.org/pdf/2210.16476.pdf)
- (arXiv 2022.11) SAP-DETR: Bridging the Gap Between Salient Points and Queries-Based Transformer Detector for Fast Model Convergency, [[Paper]](https://arxiv.org/pdf/2211.02006.pdf)
- (arXiv 2022.11) Group DETR v2: Strong Object Detector with Encoder-Decoder Pretraining, [[Paper]](https://arxiv.org/pdf/2211.03594.pdf)
- (arXiv 2022.11) Teach-DETR: Better Training DETR with Teachers, [[Paper]](https://arxiv.org/pdf/2211.11953.pdf),[[Code]](https://github.com/LeonHLJ/Teach-DETR)
- (arXiv 2022.11) DETRs with Collaborative Hybrid Assignments Training, [[Paper]](https://arxiv.org/pdf/2211.12860.pdf),[[Code]](https://github.com/Sense-X/Co-DETR)
- (arXiv 2022.11) DQ-DETR: Dual Query Detection Transformer for Phrase Extraction and Grounding, [[Paper]](https://arxiv.org/pdf/2211.15516.pdf),[[Code]](https://github.com/IDEA-Research/DQ-DETR)
- (arXiv 2022.11) How to Backpropagate through Hungarian in Your DETR, [[Paper]](https://arxiv.org/pdf/2211.14448.pdf)
- (arXiv 2022.11) Concealed Object Detection for Passive Millimeter-Wave Security Imaging Based on Task-Aligned Detection Transformer, [[Paper]](https://arxiv.org/pdf/2212.00313.pdf)
- (arXiv 2022.12) Recurrent Vision Transformers for Object Detection with Event Cameras, [[Paper]](https://arxiv.org/pdf/2212.05598.pdf)
- (arXiv 2022.12) CNN-transformer mixed model for object detection, [[Paper]](https://arxiv.org/pdf/2212.06714.pdf)
- (arXiv 2022.12) DETR4D: Direct Multi-View 3D Object Detection with Sparse Attention, [[Paper]](https://arxiv.org/pdf/2212.07849.pdf)
- (arXiv 2022.12) GPTR: Gestalt-Perception Transformer for Diagram Object Detection, [[Paper]](https://arxiv.org/pdf/2212.14232.pdf)
- (arXiv 2023.01) Dynamic Background Reconstruction via Transformer for Infrared Small Target Detection, [[Paper]](https://arxiv.org/pdf/2301.04497.pdf)
- (arXiv 2023.01) Learning to View: Decision Transformers for Active Object Detection, [[Paper]](https://arxiv.org/pdf/2301.09544.pdf)
- (arXiv 2023.01) Aerial Image Object Detection With Vision Transformer Detector, [[Paper]](https://arxiv.org/pdf/2301.12058.pdf)
- (arXiv 2023.01) Priors are Powerful: Improving a Transformer for Multi-camera 3D Detection with 2D Priors, [[Paper]](https://arxiv.org/pdf/2301.13592.pdf)
- (arXiv 2023.01) IH-ViT: Vision Transformer-based Integrated Circuit Appear-ance Defect Detection, [[Paper]](https://arxiv.org/pdf/2301.13592.pdf)
- (arXiv 2023.02) Team-DETR: Guide Queries as a Professional Team in Detection Transformers, [[Paper]](https://arxiv.org/pdf/2302.07116.pdf),[[Code]](https://github.com/horrible-dong/TeamDETR)
- (arXiv 2023.02) Hyneter: Hybrid Network Transformer for Object Detection, [[Paper]](https://arxiv.org/pdf/2302.09365.pdf)
- (arXiv 2023.02) KS-DETR: Knowledge Sharing in Attention Learning for Detection Transformer, [[Paper]](https://arxiv.org/pdf/2302.11208.pdf),[[Code]](https://github.com/edocanonymous/KS-DETR)
- (arXiv 2023.03) D2Q-DETR: Decoupling and Dynamic Queries for Oriented Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2303.00542.pdf)
- (arXiv 2023.03) FeatAug-DETR: Enriching One-to-Many Matching for DETRs with Feature Augmentation, [[Paper]](https://arxiv.org/pdf/2303.01503.pdf),[[Code]](https://github.com/rongyaofang/FeatAug-DETR)
- (arXiv 2023.03) A Computer Vision Enabled damage detection model with improved YOLOv5 based on Transformer Prediction Head, [[Paper]](https://arxiv.org/pdf/2303.04275.pdf)
- (arXiv 2023.03) ARS-DETR: Aspect Ratio Sensitive Oriented Object Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2303.04989.pdf),[[Code]](https://github.com/httle/ARS-DETR)
- (arXiv 2023.03) Lite DETR : An Interleaved Multi-Scale Encoder for Efficient DETR, [[Paper]](https://arxiv.org/pdf/2303.07335.pdf),[[Code]](https://github.com/IDEA-Research/Lite-DETR)
- (arXiv 2023.03) FAQ: Feature Aggregated Queries for Transformer-based Video Object Detectors, [[Paper]](https://arxiv.org/pdf/2303.08319.pdf)
- (arXiv 2023.03) Query-guided Attention in Vision Transformers for Localizing Objects Using a Single Sketch, [[Paper]](https://arxiv.org/pdf/2303.08784.pdf)
- (arXiv 2023.03) SeqCo-DETR: Sequence Consistency Training for Self-Supervised Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2303.08481.pdf)
- (arXiv 2023.03) MonoATT: Online Monocular 3D Object Detection with Adaptive Token Transformer, [[Paper]](https://arxiv.org/pdf/2303.13018.pdf)
- (arXiv 2023.03) Transformer-based Multi-Instance Learning for Weakly Supervised Object Detection, [[Paper]](https://arxiv.org/pdf/2303.14999.pdf)
- (arXiv 2023.03) Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2303.14816.pdf),[[Code]](https://github.com/ZhouHuang23/FSPNet)
- (arXiv 2023.03) T-FFTRadNet: Object Detection with Swin Vision Transformers from Raw ADC Radar Signals, [[Paper]](https://arxiv.org/pdf/2303.16940.pdf)
- (arXiv 2023.03) SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.17605.pdf),[[Code]](https://sparsevit.mit.edu/)
- (arXiv 2023.04) Siamese DETR, [[Paper]](https://arxiv.org/pdf/2303.18144.pdf),[[Code]](https://github.com/Zx55/SiameseDETR)
- (arXiv 2023.04) Training Strategies for Vision Transformers for Object Detection, [[Paper]](https://arxiv.org/pdf/2304.02186.pdf)
- (arXiv 2023.04) Language-aware Multiple Datasets Detection Pretraining for DETRs, [[Paper]](https://arxiv.org/pdf/2304.03580.pdf)
- (arXiv 2023.04) Detection Transformer with Stable Matching, [[Paper]](https://arxiv.org/pdf/2304.04742.pdf),[[Code]](https://github.com/IDEA-Research/Stable-DINO)
- (arXiv 2023.04) Use the Detection Transformer as a Data Augmenter, [[Paper]](https://arxiv.org/pdf/2304.04554.pdf)
- (arXiv 2023.04) DETR with Additional Global Aggregation for Cross-domain Weakly Supervised Object Detection, [[Paper]](https://arxiv.org/pdf/2304.07082.pdf)
- (arXiv 2023.04) DETRs Beat YOLOs on Real-time Object Detection, [[Paper]](https://arxiv.org/pdf/2304.08069.pdf)
- (arXiv 2023.04) DETR-based Layered Clothing Segmentation and Fine-Grained Attribute Recognition, [[Paper]](https://arxiv.org/pdf/2304.08107.pdf)
- (arXiv 2023.04) Align-DETR: Improving DETR with Simple IoU-aware BCE loss, [[Paper]](https://arxiv.org/pdf/2304.07527.pdf),[[Code]](https://github.com/FelixCaae/AlignDETR)
- (arXiv 2023.04) Transformer-based stereo-aware 3D object detection from binocular images, [[Paper]](https://arxiv.org/pdf/2304.11906.pdf)
- (arXiv 2023.05) End to End Lane detection with One-to-Several Transformer, [[Paper]](https://arxiv.org/pdf/2305.00675.pdf),[[Code]](https://github.com/zkyseu/O2SFormer)
- (arXiv 2023.05) TransCAR: Transformer-based Camera-And-Radar Fusion for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2305.00397.pdf),[[Code]](https://github.com/zkyseu/O2SFormer)
- (arXiv 2023.05) SSD-MonoDTR: Supervised Scale-constrained Deformable Transformer for Monocular 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2305.07270.pdf),[[Code]](https://github.com/mikasa3lili/SSD-MonoDETR)
- (arXiv 2023.05) RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection, [[Paper]](https://arxiv.org/pdf/2305.07598.pdf)
- (arXiv 2023.06) detrex: Benchmarking Detection Transformers, [[Paper]](https://arxiv.org/pdf/2306.07265.pdf),[[Code]](https://github.com/IDEA-Research/detrex)
- (arXiv 2023.06) Revisiting Token Pruning for Object Detection and Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2306.07186.pdf)
- (arXiv 2023.06) Bridging the Performance Gap between DETR and R-CNN for Graphical Object Detection in Document Images, [[Paper]](https://arxiv.org/pdf/2306.13526.pdf)
- (arXiv 2023.06) C2Former: Calibrated and Complementary Transformer for RGB-Infrared Object Detection, [[Paper]](https://arxiv.org/pdf/2306.16175.pdf),[[Code]](https://github.com/yuanmaoxun/Calibrated-and-Complementary-Transformer-for-RGB-Infrared-Object-Detection.git)
- (arXiv 2023.07) PM-DETR: Domain Adaptive Prompt Memory for Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2307.00313.pdf)
- (arXiv 2023.07) Spatial-Temporal Enhanced Transformer Towards Multi-Frame 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2307.00347.pdf),[[Code]](https://github.com/Eaphan/STEMD)
- (arXiv 2023.07) Box-DETR: Understanding and Boxing Conditional Spatial Queries, [[Paper]](https://arxiv.org/pdf/2307.08353.pdf),[[Code]](https://github.com/tiny-smart/box-detr)
- (arXiv 2023.07) Semi-DETR: Semi-Supervised Object Detection with Detection Transformers, [[Paper]](https://arxiv.org/pdf/2307.08095.pdf),[[Code]](https://github.com/PaddlePaddle/PaddleDetection/tree/develop/configs/semi_det/semi_detr)
- (arXiv 2023.07) Cascade-DETR: Delving into High-Quality Universal Object Detection, [[Paper]](https://arxiv.org/pdf/2307.11035.pdf),[[Code]](https://github.com/SysCV/cascade-detr)
- (arXiv 2023.07) Less is More: Focus Attention for Efficient DETR, [[Paper]](https://arxiv.org/pdf/2307.12612.pdf), [[Code]](https://github.com/huawei-noah/noah-research)
- (arXiv 2023.07) DQ-Det: Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation, [[Ppaer]](https://arxiv.org/pdf/2307.12239.pdf)
- (arXiv 2023.07) Enhancing Your Trained DETRs with Box Refinement, [[Paper]](https://arxiv.org/pdf/2307.11828.pdf), [[Code]](https://github.com/YiqunChen1999/RefineBox)
- (arXiv 2023.07) RecursiveDet: End-to-End Region-based Recursive Object Detection, [[Paper]](https://arxiv.org/pdf/2307.13619.pdf), [[Code]](https://github.com/bravezzzzzz/RecursiveDet)
- (arXiv 2023.07) SimDETR: Simplifying self-supervised pretraining for DETR, [[Paper]](https://arxiv.org/pdf/2307.15697.pdf)
- (arXiv 2023.08) Revisiting DETR Pre-training for Object Detection, [[Paper]](https://arxiv.org/pdf/2308.01300.pdf)
- (arXiv 2023.08) DETR Doesn鈥檛 Need Multi-Scale or Locality Design, [[Paper]](https://arxiv.org/pdf/2308.01904.pdf), [[Code]](https://github.com/impiga/Plain-DETR)
- (arXiv 2023.08) FocalFormer3D : Focusing on Hard Instance for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2308.04556.pdf), [[Code]](https://github.com/NVlabs/FocalFormer3D)
- (arXiv 2023.08) V-DETR: DETR with Vertex Relative Position Encoding for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2308.04409.pdf), [[Code]](https://github.com/yichaoshen-MS/V-DETR)
- (arXiv 2023.08) SODFormer: Streaming Object Detection with Transformer Using Events and Frames, [[Paper]](https://arxiv.org/pdf/2308.04047.pdf), [[Code]](https://github.com/dianzl/SODFormer)
- (arXiv 2023.08) Spatial Transform Decoupling for Oriented Object Detection, [[Paper]](https://arxiv.org/pdf/2308.10561.pdf), [[Code]](https://github.com/yuhongtian17/Spatial-Transform-Decoupling)
- (arXiv 2023.08) Towards a High-Performance Object Detector: Insights from Drone Detection Using ViT and CNN-based Deep Learning Models, [[Paper]](https://arxiv.org/pdf/2308.09899.pdf)
- (arXiv 2023.08) Enhancing Landmark Detection in Cluttered Real-World Scenarios with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.13671.pdf)
- (arXiv 2023.09) Supervised Shape&Scale-perceptive Deformable Transformer for Monocular 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2309.00928.pdf), [[Code]](https://github.com/mikasa3lili/S3-MonoDETR)
- (arXiv 2023.09) OccupancyDETR: Making Semantic Scene Completion as Straightforward as Object Detection, [[Paper]](https://arxiv.org/pdf/2309.08504.pdf), [[Code]](https://github.com/jypjypjypjyp/OccupancyDETR)
- (arXiv 2023.10) Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2310.01401.pdf), [[Code]](https://github.com/ymingxie/parq)
- (arXiv 2023.10) Uni3DETR: Unified 3D Detection Transformer, [[Paper]](https://arxiv.org/pdf/2310.05699.pdf), [[Code]](https://github.com/zhenyuw16/Uni3DETR)
- (arXiv 2023.10) SimPLR: A Simple and Plain Transformer for Object Detection and Segmentation, [[Paper]](https://arxiv.org/pdf/2310.05920.pdf)
- (arXiv 2023.10) Rank-DETR for High Quality Object Detection, [[Paper]](https://arxiv.org/pdf/2310.08854.pdf), [[Code]](https://github.com/LeapLabTHU/Rank-DETR)
- (arXiv 2023.10) Investigating the Robustness and Properties of Detection Transformers (DETR) Toward Difficult Images, [[Paper]](https://arxiv.org/pdf/2310.08772.pdf)
- (arXiv 2023.10) Multi Self-supervised Pre-fine-tuned Transformer Fusion for Better Intelligent Transportation Detection, [[Paper]](https://arxiv.org/pdf/2310.11307.pdf)
- (arXiv 2023.10) Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection, [[Paper]](https://arxiv.org/pdf/2310.15955.pdf)
- (arXiv 2023.10) Towards Few-Annotation Learning for Object Detection: Are Transformer-based Models More Efficient, [[Paper]](https://arxiv.org/pdf/2310.19936.pdf)
- (arXiv 2023.11) AiluRus: A Scalable ViT Framework for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2311.01197.pdf), [[Code]](https://github.com/caddyless/ailurus/tree/main)
- (arXiv 2023.11) TokenMotion: Motion-Guided Vision Transformer for Video Camouflaged Object Detection Via Learnable Token Selection, [[Paper]](https://arxiv.org/pdf/2311.02535.pdf)
- (arXiv 2023.11) Cal-DETR: Calibrated Detection Transformer, [[Paper]](https://arxiv.org/pdf/2311.03570.pdf), [[Code]](https://github.com/akhtarvision/cal-detr)
- (arXiv 2023.11) FusionViT: Hierarchical 3D Object Detection via LiDAR-Camera Vision Transformer Fusion, [[Paper]](https://arxiv.org/pdf/2311.03620.pdf)
- (arXiv 2023.11) Algorithms for Object Detection in Substations, [[Paper]](https://arxiv.org/pdf/2311.07577.pdf)
- (arXiv 2023.11) Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection, [[Paper]](https://arxiv.org/pdf/2311.08747.pdf)
- (arXiv 2023.11) Decoupled DETR For Few-shot Object Detection, [[Paper]](https://arxiv.org/pdf/2311.11570.pdf)
- (arXiv 2023.12) RotaTR: Detection Transformer for Dense and Rotated Object, [[Paper]](https://arxiv.org/pdf/2312.02821.pdf)
- (arXiv 2023.12) Explainable Multi-Camera 3D Object Detection with Transformer-Based Saliency Maps, [[Paper]](https://arxiv.org/pdf/2312.14606.pdf)
- (arXiv 2023.12) Context Enhanced Transformer for Single Image Object Detection, [[Paper]](https://arxiv.org/pdf/2312.14492.pdf), [[Code]](https://ku-cvlab.github.io/CETR)
- (arXiv 2024.01) TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection, [[Paper]](https://arxiv.org/pdf/2401.02309.pdf), [[Code]](https://github.com/mingyao1120/TR-DETR)
- (arXiv 2024.01) MS-DETR: Efficient DETR Training with Mixed Supervision, [[Paper]](https://arxiv.org/pdf/2401.03989.pdf)
- (arXiv 2024.01) YOLO-Former: YOLO Shakes Hand With ViT, [[Paper]](https://arxiv.org/pdf/2401.06244.pdf)
- (arXiv 2024.01) Small Object Detection by DETR via Information Augmentation and Adaptive Feature Fusion, [[Paper]](https://arxiv.org/pdf/2401.08017.pdf)
- (arXiv 2024.01) SCTransNet: Spatial-channel Cross Transformer Network for Infrared Small Target Detection, [[Paper]](https://arxiv.org/pdf/2401.15583.pdf), [[Code]](https://github.com/xdFai)
- (arXiv 2024.01) TCI-Former: Thermal Conduction-Inspired Transformer for Infrared Small Target Detection, [[Paper]](https://arxiv.org/pdf/2402.02046.pdf)
- (arXiv 2024.02) Deployment Prior Injection for Run-time Calibratable Object Detection, [[Paper]](https://arxiv.org/pdf/2402.17207.pdf)
- (arXiv 2024.03) DAMS-DETR: Dynamic Adaptive Multispectral Detection Transformer with Competitive Query Selection and Adaptive Feature Fusion, [[Paper]](https://arxiv.org/pdf/2403.00326.pdf), [[Code]](https://github.com/gjj45/DAMS-DETR)
- (arXiv 2024.03) AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection, [[Paper]](https://arxiv.org/pdf/2403.04309.pdf), [[Code]](https://github.com/Limingyuan001/AO-DETR-test)
- (arXiv 2024.03) SimPB: A Single Model for 2D and 3D Object Detection from Multiple Cameras, [[Paper]](https://arxiv.org/pdf/2403.10353.pdf), [[Code]](https://github.com/nullmax-vision/SimPB)
  - (arXiv 2024.03) LiFT: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors, [[Paper]](https://arxiv.org/pdf/2403.14625.pdf), [[Project]](https://www.cs.umd.edu/~sakshams/LiFT/)

### Edge
- (arXiv 2022.03) EDTER: Edge Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2203.08566.pdf), [[Code]](https://github.com/MengyangPu/EDTER)
- (arXiv 2022.06) XBound-Former: Toward Cross-scale Boundary Modeling in Transformers, [[Paper]](https://arxiv.org/pdf/2206.00806.pdf), [[Code]](https://github.com/jcwang123/xboundformer)
- (arXiv 2022.06) Structured Context Transformer for Generic Event Boundary Detection, [[Paper]](https://arxiv.org/pdf/2206.02985.pdf)
- (arXiv 2022.06) SC-Transformer++: Structured Context Transformer for Generic Event Boundary Detection, [[Paper]](https://arxiv.org/pdf/2206.12634.pdf), [[Project]](https://github.com/ZhangGongjie/IMFA)
- (arXiv 2023.07) CT-Net: Arbitrary-Shaped Text Detection via Contour Transformer, [[Paper]](https://arxiv.org/pdf/2307.13310.pdf)

### Enhancement
- (arXiv 2021.11) U-shape Transformer for Underwater Image Enhancement, [[Paper]](https://arxiv.org/pdf/2111.10135.pdf)
- (arXiv 2022.01) DocEnTr: An End-to-End Document Image Enhancement Transformer, [[Paper]](https://arxiv.org/pdf/2201.10252.pdf), [[Code]](https://github.com/dali92002/DocEnTR)
- (arXiv 2022.04) Underwater Image Enhancement Using Pre-trained Transformer, [[Paper]](https://arxiv.org/pdf/2204.04199.pdf)
- (arXiv 2022.04) VISTA: Vision Transformer enhanced by U-Net and Image Colorfulness Frame Filtration for Automatic Retail Checkout, [[Paper]](https://arxiv.org/pdf/2204.11024.pdf), [[Code]](https://github.com/istiakshihab/automated-retail-checkout-aicity22)
- (arXiv 2022.05) Reinforced Swin-Convs Transformer for Underwater Image Enhancement, [[Paper]](https://arxiv.org/pdf/2205.00434.pdf)
- (arXiv 2022.07) Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement, [[Paper]](https://arxiv.org/pdf/2207.07828.pdf)
- (arXiv 2022.10) End-to-end Transformer for Compressed Video Quality Enhancement, [[Paper]](https://arxiv.org/pdf/2210.13827.pdf)
- (arXiv 2022.12) WavEnhancer: Unifying Wavelet and Transformer for Image Enhancement, [[Paper]](https://arxiv.org/pdf/2212.08327.pdf)
- (arXiv 2022.12) Ultra-High-Definition Low-Light Image Enhancement: A Benchmark and Transformer-Based Method, [[Paper]](https://arxiv.org/pdf/2212.11548.pdf), [[Code]](https://github.com/TaoWangzj/LLFormer)
- (arXiv 2023.03) Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement, [[Paper]](https://arxiv.org/pdf/2303.06705.pdf)
- (arXiv 2023.06) Unsupervised Low Light Image Enhancement Using SNR-Aware Swin Transformer, [[Paper]](https://arxiv.org/pdf/2306.02082.pdf)
- (arXiv 2023.06) Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Network, [[Paper]](https://arxiv.org/pdf/2308.08220.pdf)
- (arXiv 2023.09) Underwater Image Enhancement by Transformer-based Diffusion Model with Non-uniform Sampling for Skip Strategy, [[Paper]](https://arxiv.org/pdf/2309.03445.pdf), [[Code]](https://github.com/piggy2009/DM_underwater)
- (arXiv 2023.09) DEFormer: DCT-driven Enhancement Transformer for Low-light Image and Dark Vision, [[Paper]](https://arxiv.org/pdf/2309.06941.pdf)
- (arXiv 2023.10) UWFormer: Underwater Image Enhancement via a Semi-Supervised Multi-Scale Transformer, [[Paper]](https://arxiv.org/pdf/2310.20210.pdf)
- (arXiv 2023.12) A Layer-Wise Tokens-to-Token Transformer Network for Improved Historical Document Image Enhancement, [[Paper]](https://arxiv.org/pdf/2312.03946.pdf), [[Code]](https://github.com/RisabBiswas/T2T-BinFormer)
- (arXiv 2023.12) Transformer-based No-Reference Image Quality Assessment via Supervised Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2312.06995.pdf), [[Code]](https://github.com/I2-Multimedia-Lab/SaTQA)
- (arXiv 2023.12) A Non-Uniform Low-Light Image Enhancement Method with Multi-Scale Attention Transformer and Luminance Consistency Loss, [[Paper]](https://arxiv.org/pdf/2312.16498.pdf), [[Code]](https://github.com/fang001021/MSATr)
- (arXiv 2024.01) LYT-Net: Lightweight YUV Transformer-based Network for Low-Light Image Enhancement, [[Paper]](https://arxiv.org/pdf/2401.15204.pdf), [[Code]](https://github.com/albrateanu/LYT-Net)
- (arXiv 2024.03) LoLiSRFlow: Joint Single Image Low-light Enhancement and Super-resolution via Cross-scale Transformer-based Conditional Flow, [[Paper]](https://arxiv.org/pdf/2402.18871.pdf)
- (arXiv 2024.03) Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement, [[Paper]](https://arxiv.org/pdf/2403.01497.pdf)

### Face
- (arXiv 2021.03) Face Transformer for Recognition, [[Paper]](https://arxiv.org/abs/2103.14803)
- (arXiv 2021.03) Robust Facial Expression Recognition with Convolutional Visual Transformers, [[Paper]](https://arxiv.org/abs/2103.16854)
- (arXiv 2021.04) TransRPPG: Remote Photoplethysmography Transformer for 3D Mask Face Presentation Attack Detection, [[Paper]](https://arxiv.org/pdf/2104.07419.pdf)
- (arXiv 2021.04) Facial Attribute Transformers for Precise and Robust Makeup Transfer, [[Paper]](https://arxiv.org/pdf/2104.02894.pdf)
- (arXiv 2021.04) Learning to Cluster Faces via Transformer, [[Paper]](https://arxiv.org/pdf/2104.11502.pdf)
- (arXiv 2021.06) VidFace: A Full-Transformer Solver for Video Face Hallucination with Unaligned Tiny Snapshots, [[Paper]](https://arxiv.org/pdf/2105.14954.pdf)
- (arXiv 2021.06) MViT: Mask Vision Transformer for Facial Expression Recognition in the wild, [[Paper]](https://arxiv.org/pdf/2106.04520.pdf)
- (arXiv 2021.06) Shuffle Transformer with Feature Alignment for Video Face Parsing, [[Paper]](https://arxiv.org/pdf/2106.08650.pdf)
- (arXiv 2021.06) A Latent Transformer for Disentangled and Identity-Preserving Face Editing, [[Paper]](https://arxiv.org/pdf/2106.11895.pdf), [[Code]](https://github.com/InterDigitalInc/Latent-Transformer)
- (arXiv 2021.07) ST-DETR: Spatio-Temporal Object Traces Attention Detection Transformer, [[Paper]](https://arxiv.org/pdf/2107.03107.pdf)
- (arXiv 2021.08) FT-TDR: Frequency-guided Transformer and Top-Down Refinement Network for Blind Face Inpainting, [[Paper]](https://arxiv.org/pdf/2108.04424.pdf)
- (arXiv 2021.08) Learning Fair Face Representation With Progressive Cross Transformer, [[Paper]](https://arxiv.org/pdf/2108.04983.pdf)
- (arXiv 2021.08) TransFER: Learning Relation-aware Facial Expression Representations with Transformers, [[Paper]](https://arxiv.org/pdf/2108.11116.pdf)
- (arXiv 2021.09) TANet: A new Paradigm for Global Face Super-resolution via Transformer-CNN Aggregation Network, [[Paper]](https://arxiv.org/pdf/2109.08174.pdf)
- (arXiv 2021.09) Expression Snippet Transformer for Robust Video-based Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2109.08409.pdf),[[Code]](https://anonymous.4open.science/r/ATSE-C58B)
- (arXiv 2021.09) Sparse Spatial Transformers for Few-Shot Learning, [[Paper]](https://arxiv.org/pdf/2109.10057.pdf),[[Code]](https://github.com/chenhaoxing/SSFormers)
- (arXiv 2021.09) MFEViT: A Robust Lightweight Transformer-based Network for Multimodal 2D+3D Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2109.13086.pdf)
- (arXiv 2021.11) FakeTransformer: Exposing Face Forgery From Spatial-Temporal Representation Modeled By Facial Pixel Variations, [[Paper]](https://arxiv.org/pdf/2111.07601.pdf)
- (arXiv 2021.12) SSAT: A Symmetric Semantic-Aware Transformer Network for Makeup Transfer and Removal, [[Paper]](https://arxiv.org/pdf/2112.03631.pdf),[[Code]](https://gitee.com/sunzhaoyang0304/ssat-msp)
- (arXiv 2021.12) FaceFormer: Speech-Driven 3D Facial Animation with Transformers, [[Paper]](https://arxiv.org/pdf/2112.05329.pdf)
- (arXiv 2021.12) Short and Long Range Relation Based Spatio-Temporal Transformer for Micro-Expression Recognition, [[Paper]](https://arxiv.org/pdf/2112.05851.pdf)
- (arXiv 2022.01) RestoreFormer: High-Quality Blind Face Restoration From Undegraded Key-Value Pairs, [[Paper]](https://arxiv.org/pdf/2201.06374.pdf)
- (arXiv 2022.03) Protecting Celebrities with Identity Consistency Transformer, [[Paper]](https://arxiv.org/pdf/2203.01318.pdf)
- (arXiv 2022.03) Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning, [[Paper]](https://arxiv.org/pdf/2203.06541.pdf),[[Code]](https://github.com/Jiahao-UTS/SLPT-master)
- (arXiv 2022.03) HP-Capsule: Unsupervised Face Part Discovery by Hierarchical Parsing Capsule Network, [[Paper]](https://arxiv.org/pdf/2203.10699.pdf)
- (arXiv 2022.03) Mask Usage Recognition using Vision Transformer with Transfer Learning and Data Augmentation, [[Paper]](https://arxiv.org/pdf/2203.11542.pdf)
- (arXiv 2022.03) Transformer-based Multimodal Information Fusion for Facial Expression Analysis, [[Paper]](https://arxiv.org/pdf/2203.12367.pdf)
- (arXiv 2022.03) Adaptive Transformers for Robust Few-shot Cross-domain Face Anti-spoofing, [[Paper]](https://arxiv.org/pdf/2203.12175.pdf)
- (arXiv 2022.03) Facial Expression Recognition with Swin Transformer, [[Paper]](https://arxiv.org/pdf/2203.13472.pdf)
- (arXiv 2022.03) TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing, [[Paper]](https://arxiv.org/pdf/2203.17266.pdf),[[Code]](https://github.com/BillyXYB/TransEditor)
- (arXiv 2022.04) Vision Transformer Equipped with Neural Resizer on Facial Expression Recognition Task, [[Paper]](https://arxiv.org/pdf/2204.02181.pdf)
- (arXiv 2022.04) POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2204.04083.pdf),[[Code]](https://github.com/zczcwh/POSTER)
- (arXiv 2022.05) Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild, [[Paper]](https://arxiv.org/pdf/2205.04749.pdf)
- (arXiv 2022.05) Towards Robust Blind Face Restoration with Codebook Lookup Transformer, [[Paper]](https://arxiv.org/pdf/2206.11253.pdf),[[Code]](https://shangchenzhou.com/projects/CodeFormer)
- (arXiv 2022.07) RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection, [[Paper]](https://arxiv.org/pdf/2207.03917.pdf)
- (arXiv 2022.07) TransFA: Transformer-based Representation for Face Attribute Evaluation, [[Paper]](https://arxiv.org/pdf/2207.05456.pdf)
- (arXiv 2022.07) FaceFormer: Scale-aware Blind Face Restoration with Transformers, [[Paper]](https://arxiv.org/pdf/2207.09790.pdf)
- (arXiv 2022.07) AU-Supervised Convolutional Vision Transformers for Synthetic Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2207.09777.pdf),[[Code]](https://github.com/msy1412/ABAW4)
- (arXiv 2022.07) Hybrid CNN-Transformer Model For Facial Affect Recognition In the ABAW4 Challenge, [[Paper]](https://arxiv.org/pdf/2207.10201.pdf)
- (arXiv 2022.07) Facial Expression Recognition using Vanilla ViT backbones with MAE Pretraining, [[Paper]](https://arxiv.org/pdf/2207.11081.pdf)
- (arXiv 2022.08) Towards Accurate Facial Landmark Detection via Cascaded Transformers, [[Paper]](https://arxiv.org/pdf/2208.10808.pdf)
- (arXiv 2022.10) Multi-Scale Wavelet Transformer for Face Forgery Detection, [[Paper]](https://arxiv.org/pdf/2210.03899.pdf)
- (arXiv 2022.10) Ensemble Learning using Transformers and Convolutional Networks for Masked Face Recognition, [[Paper]](https://arxiv.org/pdf/2210.04816.pdf),[[Code]](https://github.com/Hamzah-Luqman/MFR)
- (arXiv 2022.10) GGViT:Multistream Vision Transformer Network in Face2Face Facial Reenactment Detection, [[Paper]](https://arxiv.org/pdf/2210.05990.pdf)
- (arXiv 2022.10) Prepended Domain Transformer: Heterogeneous Face Recognition without Bells and Whistles, [[Paper]](https://arxiv.org/pdf/2210.06529.pdf)
- (arXiv 2022.10) A Saccaded Visual Transformer for General Object Spotting, [[Paper]](https://arxiv.org/pdf/2210.09220.pdf)
- (arXiv 2022.10) Face Pyramid Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.11974.pdf), [[Project]](https://khawar-islam.github.io/fpvt/)
- (arXiv 2022.10) UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision Transformer for Face Forgery Detection, [[Paper]](https://arxiv.org/pdf/2210.12752.pdf)
- (arXiv 2022.11) AU-Aware Vision Transformers for Biased Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2211.06609.pdf)
- (arXiv 2022.11) Part-based Face Recognition with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.00057.pdf)
- (arXiv 2022.12) Vision Transformer with Attentive Pooling for Robust Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2212.05463.pdf)
- (arXiv 2023.01) SFI-Swin: Symmetric Face Inpainting with Swin Transformer by Distinctly Learning Face Components Distributions, [[Paper]](https://arxiv.org/pdf/2301.03130.pdf)
- (arXiv 2023.02) PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer, [[Paper]](https://arxiv.org/pdf/2302.03548.pdf)
- (arXiv 2023.02) MorphGANFormer: Transformer-based Face Morphing and De-Morphing, [[Paper]](https://arxiv.org/pdf/2302.09404.pdf)
- (arXiv 2023.03) Enhancing General Face Forgery Detection via Vision Transformer with Low-Rank Adaptation, [[Paper]](https://arxiv.org/pdf/2303.00917.pdf)
- (arXiv 2023.03) DAA: A Delta Age AdaIN operation for age estimation via binary code transformer, [[Paper]](https://arxiv.org/pdf/2303.07929.pdf)
- (arXiv 2023.03) Precise Facial Landmark Detection by Reference Heatmap Transformer, [[Paper]](https://arxiv.org/pdf/2303.07840.pdf)
- (arXiv 2023.03) Quaternion Orthogonal Transformer for Facial Expression Recognition in the Wild, [[Paper]](https://arxiv.org/pdf/2303.07831.pdf),[[Code]](https://github.com/Gabrella/QOT)
- (arXiv 2023.03) Multi-Modal Facial Expression Recognition with Transformer-Based Fusion Networks and Dynamic Sampling, [[Paper]](https://arxiv.org/pdf/2303.08419.pdf)
- (arXiv 2023.03) Facial Affect Recognition based on Transformer Encoder and Audiovisual Fusion for the ABAW5 Challenge, [[Paper]](https://arxiv.org/pdf/2303.09158.pdf)
- (arXiv 2023.03) Spatial-temporal Transformer for Affective Behavior Analysis, [[Paper]](https://arxiv.org/pdf/2303.10561.pdf)
- (arXiv 2023.03) FER-former: Multi-modal Transformer for Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2303.12997.pdf)
- (arXiv 2023.04) Face Transformer: Towards High Fidelity and Accurate Face Swapping, [[Paper]](https://arxiv.org/pdf/2304.02530.pdf)
- (arXiv 2023.04) Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition, [[Paper]](https://arxiv.org/pdf/2304.04420.pdf)
- (arXiv 2023.04) MC-ViViT: Multi-branch Classifier-ViViT to Detect Mild Cognitive Impairment in Older Adults using Facial Videos, [[Paper]](https://arxiv.org/pdf/2304.05292.pdf)
- (arXiv 2023.04) PATMAT: Person Aware Tuning of Mask-Aware Transformer for Face Inpainting, [[Paper]](https://arxiv.org/pdf/2304.06107.pdf)
- (arXiv 2023.04) MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing, [[Paper]](https://arxiv.org/pdf/2304.07549.pdf)
- (arXiv 2023.05) Noise-Resistant Multimodal Transformer for Emotion Recognition, [[Paper]](https://arxiv.org/pdf/2305.02814.pdf)
- (arXiv 2023.05) LOGO-Former: Local-Global Spatio-Temporal Transformer for Dynamic Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2305.03343.pdf)
- (arXiv 2023.05) FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing, [[Paper]](https://arxiv.org/pdf/2305.03277.pdf)
- (arXiv 2023.07) MiVOLO: Multi-input Transformer for Age and Gender Estimation, [[Paper]](https://arxiv.org/pdf/2307.04616.pdf),[[Code]](https://github.com/WildChlamydia/MiVOLO.git)
- (arXiv 2023.07) Robust face anti-spoofing framework with Convolutional Vision Transformer, [[Paper]](https://arxiv.org/pdf/2307.12459.pdf)
- (arXiv 2023.08) RestoreFormer++: Towards Real-World Blind Face Restoration from Undegraded Key-Value Pairs, [[Paper]](https://arxiv.org/pdf/2308.07279.pdf)
- (arXiv 2023.08) Dual-path TokenLearner for Remote Photoplethysmography-based Physiological Measurement with Facial Videos, [[Paper]](https://arxiv.org/pdf/2308.07771.pdf),[[Code]](https://github.com/VUT-HFUT/Dual-TL)
- (arXiv 2023.08) TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective, [[Paper]](https://arxiv.org/pdf/2308.10133.pdf),[[Code]](https://github.com/DanJun6737/TransFace)
- (arXiv 2023.08) Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer, [[Paper]](https://arxiv.org/pdf/2308.10196.pdf)
- (arXiv 2023.08) SwinFace: A Multi-task Transformer for Face Recognition, Expression Recognition, Age Estimation and Attribute Estimation, [[Paper]](https://arxiv.org/pdf/2308.11509.pdf),[[Code]](https://github.com/lxq1000/SwinFace)
- (arXiv 2023.08) A Unified Transformer-based Network for multimodal Emotion Recognition, [[Paper]](https://arxiv.org/pdf/2308.14160.pdf)
- (arXiv 2023.09) S-Adapter: Generalizing Vision Transformer for Face Anti-Spoofing with Statistical Tokens, [[Paper]](https://arxiv.org/pdf/2309.04038.pdf)
- (arXiv 2023.09) Self-Supervised Transformer with Domain Adaptive Reconstruction for General Face Forgery Video Detection, [[Paper]](https://arxiv.org/pdf/2309.04795.pdf)
- (arXiv 2023.09) Forgery-aware Adaptive Vision Transformer for Face Forgery Detection, [[Paper]](https://arxiv.org/pdf/2309.11092.pdf)
- (arXiv 2023.10) 1DFormer: Learning 1D Landmark Representations via Transformer for Facial Landmark Tracking, [[Paper]](https://arxiv.org/pdf/2311.00241.pdf)
- (arXiv 2023.11) Fast and Interpretable Face Identification for Out-Of-Distribution Data Using Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.02803.pdf),[[Code]](https://github.com/anguyen8/face-vit)
- (arXiv 2023.12) Hypergraph-Guided Disentangled Spectrum Transformer Networks for Near-Infrared Facial Expression Recognition, [[Paper]](https://arxiv.org/pdf/2312.05907.pdf)
- (arXiv 2023.12) Modality-Collaborative Transformer with Hybrid Feature Reconstruction for Robust Emotion Recognition, [[Paper]](https://arxiv.org/pdf/2312.15848.pdf)
- (arXiv 2024.01) CATFace: Cross-Attribute-Guided Transformer with Self-Attention Distillation for Low-Quality Face Recognition, [[Paper]](https://arxiv.org/pdf/2401.03037.pdf)
- (arXiv 2024.02) DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2402.05712.pdf),[[Code]](https://github.com/theEricMa/DiffSpeaker)
- (arXiv 2024.03) BFRFormer: Transformer-based generator for Real-World Blind Face Restoration, [[Paper]](https://arxiv.org/pdf/2402.18811.pdf),[[Code]](https://github.com/s8Znk/BFRFormer)
- (arXiv 2024.03) AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit Detectors, [[Paper]](https://arxiv.org/pdf/2403.04697.pdf),[[Code]](https://github.com/yuankaishen2001/AUFormer)
- (arXiv 2024.03) A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product, [[Paper]](https://arxiv.org/pdf/2403.08511.pdf)
- (arXiv 2024.03) Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild, [[Paper]](https://arxiv.org/pdf/2403.10488.pdf)
- (arXiv 2024.03) Boosting Continuous Emotion Recognition with Self-Pretraining using Masked Autoencoders, Temporal Convolutional Networks, and Transformers, [[Paper]](https://arxiv.org/pdf/2403.11440.pdf)
- (arXiv 2024.03) Emotion Recognition Using Transformers with Masked Learning, [[Paper]](https://arxiv.org/pdf/2403.13731.pdf),[[Code]](https://github.com/msjae/ABAW)
- (arXiv 2024.03) FaceXFormer: A Unified Transformer for Facial Analysis, [[Paper]](https://arxiv.org/pdf/2403.12960.pdf),[[Code]](https://kartik-3004.github.io/facexformer_web/)
- (arXiv 2024.03) A Hybrid Transformer-Sequencer approach for Age and Gender classification from in-wild facial images, [[Paper]](https://arxiv.org/pdf/2403.12483.pdf)

### Federated Learning 
- (arXiv 2022.11) FedTune: A Deep Dive into Efficient Federated Fine-Tuning with Pre-trained Transformers, [[Paper]](https://arxiv.org/pdf/2211.08025.pdf)
- (arXiv 2023.06) FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling, [[Paper]](https://arxiv.org/pdf/2306.14638.pdf),[[Code]](https://github.com/faresmalik/FeSViBS)
- (arXiv 2023.08) Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning, [[Paper]](https://arxiv.org/pdf/2308.04373.pdf)
- (arXiv 2023.08) FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning, [[Paper]](https://arxiv.org/pdf/2308.09160.pdf), [[Code]](https://github.com/imguangyu/FedPerfix)
- (arXiv 2024.01) OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated learning, [[Paper]](https://arxiv.org/pdf/2401.11652pdf)

### Few-shot Learning
- (arXiv 2021.04) Rich Semantics Improve Few-shot Learning, [[Paper]](https://arxiv.org/pdf/2104.12709.pdf), [[Code]](https://github.com/MohamedAfham/RS_FSL)
- (arXiv 2021.04) Few-Shot Segmentation via Cycle-Consistent Transformer, [[Paper]](https://arxiv.org/pdf/2106.02320.pdf)
- (arXiv 2021.09) Sparse Spatial Transformers for Few-Shot Learning, [[Paper]](https://arxiv.org/pdf/2109.12932.pdf)
- (arXiv 2021.12) Cost Aggregation Is All You Need for Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2112.11685.pdf), [[Code]](https://github.com/Seokju-Cho/Volumetric-Aggregation-Transformer)
- (arXiv 2022.01) HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning, [[Paper]](https://arxiv.org/pdf/2201.04182.pdf)
- (arXiv 2022.02) Task-Adaptive Feature Transformer with Semantic Enrichment for Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2202.06498.pdf)
- (arXiv 2022.03) Self-Promoted Supervision for Few-Shot Transformer, [[Paper]](https://arxiv.org/pdf/2203.07057.pdf), [[Code]](https://github.com/DongSky/few-shot-vit)
- (arXiv 2022.03) Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-shot Learning, [[Paper]](https://arxiv.org/pdf/2203.09064.pdf), [[Code]](https://github.com/StomachCold/HCTransformers)
- (arXiv 2022.04) CATrans: Context and Affinity Transformer for Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2204.12817.pdf)
- (arXiv 2022.05) Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning, [[Paper]](https://arxiv.org/pdf/2205.09995.pdf)
- (arXiv 2022.05) Few-Shot Diffusion Models, [[Paper]](https://arxiv.org/pdf/2205.15463.pdf)
- (arXiv 2022.06) Prompting Decision Transformer for Few-Shot Policy Generalization, [[Paper]](https://arxiv.org/pdf/2206.13499.pdf), [[Code]](https://mxu34.github.io/PromptDT/)
- (arXiv 2022.07) Learning Cross-Image Object Semantic Relation in Transformer for Few-Shot Fine-Grained Image Classification, [[Paper]](https://arxiv.org/pdf/2207.00784.pdf), [[Code]](https://github.com/JiakangYuan/HelixFormer)
- (arXiv 2022.07) Few-shot Object Counting and Detection, [[Paper]](https://arxiv.org/pdf/2207.10988.pdf), [[Code]](https://github.com/VinAIResearch/Counting-DETR)
- (arXiv 2022.07) Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2207.10866.pdf), [[Code]](https://seokju-cho.github.io/VAT/)
- (arXiv 2022.08) Few-Shot Learning Meets Transformer: Unified Query-Support Transformers for Few-Shot Classification, [[Paper]](https://arxiv.org/pdf/2208.12398.pdf)
- (arXiv 2022.10) BaseTransformers: Attention over base data-points for One Shot Learning, [[Paper]](https://arxiv.org/pdf/2210.02476.pdf), [[Code]](https://github.com/mayug/BaseTransformers)
- (arXiv 2022.10) FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training, [[Paper]](https://arxiv.org/pdf/2210.04845.pdf)
- (arXiv 2022.10) Feature-Proxy Transformer for Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2210.06908.pdf)
- (arXiv 2022.11) tSF: Transformer-based Semantic Filter for Few-Shot Learning, [[Paper]](https://arxiv.org/pdf/2211.00868.pdf)
- (arXiv 2022.11) Enhancing Few-shot Image Classification with Cosine Transformer, [[Paper]](https://arxiv.org/pdf/2211.06828.pdf), [[Code]](https://github.com/vinuni-vishc/Few-Shot-Cosine-Transformer)
- (arXiv 2023.01) Mask Matching Transformer for Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2301.01208.pdf), [[Code]](https://github.com/Picsart-AI-Research/Mask-Matching-Transformer)
- (arXiv 2023.01) Exploring Efficient Few-shot Adaptation for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.02419.pdf), [[Code]](https://github.com/loadder/eTT_TMLR2022)
- (arXiv 2023.01) Continual Few-Shot Learning Using HyperTransformers, [[Paper]](https://arxiv.org/pdf/2301.04584.pdf)
- (arXiv 2023.02) SpatialFormer: Semantic and Target Aware Attentions for Few-Shot Learning, [[Paper]](https://arxiv.org/pdf/2303.09281.pdf)
- (arXiv 2023.04) From Saliency to DINO: Saliency-guided Vision Transformer for Few-shot Keypoint Detection, [[Paper]](https://arxiv.org/pdf/2304.03140.pdf)
- (arXiv 2023.04) Analogy-Forming Transformers for Few-Shot 3D Parsing, [[Paper]](https://arxiv.org/pdf/2304.14382.pdf), [[Project]](http://analogicalnets.github.io/)
- (arXiv 2023.05) Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting, [[Paper]](https://arxiv.org/pdf/2305.04440.pdf)
- (arXiv 2023.07) Multiscale Memory Comparator Transformer for Few-Shot Video Segmentation, [[Paper]](https://arxiv.org/pdf/2307.07812.pdf), [[Code]](https://github.com/MSiam/MMC-MultiscaleMemory)
- (arXiv 2023.07) Target-aware Bi-Transformer for Few-shot Segmentation, [[Paper]](https://arxiv.org/pdf/2309.09492.pdf)
- (arXiv 2023.10) PrototypeFormer: Learning to Explore Prototype Relationships for Few-shot Image Classification, [[Paper]](https://arxiv.org/pdf/2310.03517.pdf)
- (arXiv 2023.11) Focus on Query: Adversarial Mining Transformer for Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2311.17626.pdf),[[Code]](https://github.com/Wyxdm/AMNet)

### Fusion
- (arXiv 2022.01) TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2201.07451.pdf)
- (arXiv 2022.01) TGFuse: An Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network, [[Paper]](https://arxiv.org/pdf/2201.10147.pdf)
- (arXiv 2022.04) SwinFuse: A Residual Swin Transformer Fusion Network for Infrared and Visible Images, [[Paper]](https://arxiv.org/pdf/2201.10147.pdf), [[Code]](https://github.com/Zhishe-Wang/SwinFuse)
- (arXiv 2022.07) Array Camera Image Fusion using Physics-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2207.02250.pdf)
- (arXiv 2023.09) Holistic Dynamic Frequency Transformer for Image Fusion and Exposure Correction, [[Paper]](https://arxiv.org/pdf/2309.01183.pdf)
- (arXiv 2024.02) FuseFormer: A Transformer for Visual and Thermal Image Fusion, [[Paper]](https://arxiv.org/pdf/2402.00971.pdf), [[Code]](https://github.com/aytekXR/FuseFormer-Infrared-Fusion)

### Gait
- (arXiv 2022.04) Spatial Transformer Network on Skeleton-based Gait Recognition, [[Paper]](https://arxiv.org/pdf/2204.03873.pdf)
- (arXiv 2022.06) Exploring Transformers for Behavioural Biometrics: A Case Study in Gait Recognition, [[Paper]](https://arxiv.org/pdf/2206.01441.pdf)
- (arXiv 2022.06) GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation, [[Paper]](https://arxiv.org/pdf/2207.00106.pdf), [[Code]](https://github.com/markendo/GaitForeMer)
- (arXiv 2022.10) Multi-view Gait Recognition based on Siamese Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.10421.pdf)
- (arXiv 2023.07) GaitFormer: Revisiting Intrinsic Periodicity for Gait Recognition, [[Paper]](https://arxiv.org/pdf/2307.13259.pdf)
- (arXiv 2023.08) GaitPT: Skeletons Are All You Need For Gait Recognition, [[Paper]](https://arxiv.org/pdf/2308.10623.pdf)
- (arXiv 2023.10) HCT: Hybrid Convnet-Transformer for Parkinson鈥檚 disease detection and severity prediction from gait, [[Paper]](https://arxiv.org/pdf/2310.17078.pdf), [[Code]](https://github.com/SafwenNaimi/HCT-Hybrid-Convnet-Transformer-for-Parkinson-s-disease-detection-and-severity-prediction-from-gait)
- (arXiv 2023.10) GaitFormer: Learning Gait Representations with Noisy Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2310.19418.pdf)
- (arXiv 2023.11) 1D-Convolutional transformer for Parkinson disease diagnosis from gait, [[Paper]](https://arxiv.org/pdf/2311.03177.pdf), [[Code]](https://github.com/SafwenNaimi/1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait)
- (arXiv 2023.11) GaitContour: Efficient Gait Recognition based on a Contour-Pose Representation, [[Paper]](https://arxiv.org/pdf/2311.16497.pdf)
- (arXiv 2023.12) Learning to Estimate Critical Gait Parameters from Single-View RGB Videos with Transformer-Based Attention Network, [[Paper]](https://arxiv.org/pdf/2312.00398.pdf), [[Code]](https://github.com/vinuni-vishc/transformer-gait-analysis)

### Gaze
- (arXiv 2021.06) Gaze Estimation using Transformer, [[Paper]](https://arxiv.org/pdf/2105.14424.pdf), [[Code]](https://github.com/yihuacheng/GazeTR)
- (arXiv 2022.03) End-to-End Human-Gaze-Target Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2203.10433.pdf)
- (arXiv 2022.05) Eye-gaze-guided Vision Transformer for Rectifying Shortcut Learning, [[Paper]](https://arxiv.org/pdf/2205.12466.pdf)
- (arXiv 2022.08) In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation, [[Paper]](https://arxiv.org/pdf/2208.04464.pdf), [[Code]](https://bolinlai.github.io/GLC-EgoGazeEst)
- (arXiv 2022.09) MGTR: End-to-End Mutual Gaze Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2209.10930.pdf), [[Code]](https://github.com/Gmbition/MGTR)
- (arXiv 2023.08) Interaction-aware Joint Attention Estimation Using People Attributes, [[Paper]](https://arxiv.org/pdf/2308.05382.pdf), [[Code]](https://github.com/chihina/PJAE)
- (arXiv 2023.08) DVGaze: Dual-View Gaze Estimation, [[Paper]](https://arxiv.org/pdf/2308.10310.pdf), [[Code]](https://github.com/yihuacheng/DVGaze)
- (arXiv 2023.10) Sharingan: A Transformer-based Architecture for Gaze Following, [[Paper]](https://arxiv.org/pdf/2310.00816.pdf)
- (arXiv 2023.11) Dual input stream transformer for eye-tracking line assignment, [[Paper]](https://arxiv.org/pdf/2311.06095.pdf)
- (arXiv 2024.01) GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance, [[Paper]](https://arxiv.org/pdf/2401.00260.pdf)
- (arXiv 2024.01) EmMixformer: Mix transformer for eye movement recognition, [[Paper]](https://arxiv.org/pdf/2401.04956.pdf)
- (arXiv 2024.02) TransGOP: Transformer-Based Gaze Object Prediction, [[Paper]](https://arxiv.org/pdf/2402.13578.pdf), [[Code]](https://github.com/chenxi-Guo/TransGOP)
- (arXiv 2024.03) ViTGaze: Gaze Following with Interaction Features in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.12778.pdf), [[Code]](https://github.com/hustvl/ViTGaze)

### Generative Model
- (arXiv 2021.02) TransGAN: Two Transformers Can Make One Strong GAN, [[Paper]](https://arxiv.org/pdf/2102.07074.pdf), [[Code]](https://github.com/VITA-Group/TransGAN)
- (arXiv 2021.03) Generative Adversarial Transformers, [[Paper]](https://arxiv.org/pdf/2103.01209.pdf), [[Code]](https://github.com/dorarad/gansformer)
- (arXiv 2021.04) VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.06757.pdf), [[Code]](https://github.com/SharifAmit/VTGAN)
- (arXiv 2021.05) Combining Transformer Generators with Convolutional Discriminators, [[Paper]](https://arxiv.org/pdf/2105.10189.pdf), [[Code]](https://github.com/SharifAmit/VTGAN)
- (arXiv 2021.06) ViT-Inception-GAN for Image Colourising, [[Paper]](https://arxiv.org/pdf/2106.06321.pdf)
- (arXiv 2021.06) Improved Transformer for High-Resolution GANs, [[Paper]](https://arxiv.org/pdf/2106.07631.pdf)
- (arXiv 2021.06) Styleformer: Transformer based Generative Adversarial Networks with Style Vector, [[Paper]](https://arxiv.org/pdf/2106.07023.pdf), [[Code]](https://github.com/Jeeseung-Park/Styleformer)
- (arXiv 2021.07) ViTGAN: Training GANs with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.04589.pdf)
- (arXiv 2021.10) Generating Symbolic Reasoning Problems with Transformer GANs, [[Paper]](https://arxiv.org/pdf/2110.10054.pdf)
- (arXiv 2021.10) STransGAN: An Empirical Study on Transformer in GANs, [[Paper]](https://arxiv.org/pdf/2110.13107.pdf), [[Project]](https://nbei.github.io/stransgan.html)
- (arXiv 2021.12) StyleSwin: Transformer-based GAN for High-resolution Image Generation, [[Paper]](https://arxiv.org/pdf/2112.10762.pdf), [[Code]](https://github.com/microsoft/StyleSwin)
- (arXiv 2022.01) RFormer: Transformer-based Generative Adversarial Network for Real Fundus Image Restoration on A New Clinical Benchmark, [[Paper]](https://arxiv.org/pdf/2201.00466.pdf)
- (arXiv 2022.03) Style Transformer for Image Inversion and Editing, [[Paper]](https://arxiv.org/pdf/2203.07932.pdf), [[Code]](https://github.com/sapphire497/style-transformer)
- (arXiv 2022.06) Cycle text2face: cycle text-to-face gan via transformers, [[Paper]](https://arxiv.org/pdf/2206.04503.pdf)
- (arXiv 2022.06) Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing Framework for Alzheimer's Disease, [[Paper]](https://arxiv.org/pdf/2206.13393.pdf)
- (arXiv 2022.08) Your ViT is Secretly a Hybrid Discriminative-Generative Diffusion Model, [[Paper]](https://arxiv.org/pdf/2208.07791.pdf), [[Code]](https://github.com/sndnyang/Diffusion_ViT)
- (arXiv 2022.08) User-Controllable Latent Transformer for StyleGAN Image Layout Editing, [[Paper]](https://arxiv.org/pdf/2208.12408.pdf), [[Code]](http://www.cgg.cs.tsukuba.ac.jp/~endo/projects/UserControllableLT)
- (arXiv 2023.02) CFFT-GAN: Cross-domain Feature Fusion Transformer for Exemplar-based Image Translation, [[Paper]](https://arxiv.org/pdf/2302.01608.pdf)
- (arXiv 2023.02) TcGAN: Semantic-Aware and Structure-Preserved GANs with Individual Vision Transformer for Fast Arbitrary One-Shot Image Generation, [[Paper]](https://arxiv.org/pdf/2302.08047.pdf)
- (arXiv 2023.03) StraIT: Non-autoregressive Generation with Stratified Image Transformer, [[Paper]](https://arxiv.org/pdf/2303.00865.pdf)
- (arXiv 2023.03) Graph Transformer GANs for Graph-Constrained House Generation, [[Paper]](https://arxiv.org/pdf/2303.08225.pdf)
- (arXiv 2023.03) Investigating GANsformer: A Replication Study of a State-of-the-Art Image Generation Model, [[Paper]](https://arxiv.org/pdf/2303.08577.pdf)
- (arXiv 2023.03) StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model, [[Paper]](https://arxiv.org/pdf/2303.09268.pdf), [[Code]](https://github.com/zipengxuc/StylerDALLE)
- (arXiv 2023.03) Q-RBSA: High-Resolution 3D EBSD Map Generation Using An Efficient Quaternion Transformer Network, [[Paper]](https://arxiv.org/pdf/2303.10722.pdf)
- (arXiv 2023.05) Reinforcement Learning finetuned Vision-Code Transformer for UI-to-Code Generation, [[Paper]](https://arxiv.org/pdf/2305.14637.pdf)
- (arXiv 2023.06) A Conditional Generative Chatbot using Transformer Model, [[Paper]](https://arxiv.org/pdf/2306.02074.pdf)
- (arXiv 2023.07) StylePrompter: All Styles Need Is Attention, [[Paper]](https://arxiv.org/pdf/2307.16151.pdf), [[Code]](https://github.com/I2-Multimedia-Lab/StylePrompter)
- (arXiv 2023.07) Enhancing Object Detection in Ancient Documents with Synthetic Data Generation and Transformer-Based Models, [[Paper]](https://arxiv.org/pdf/2307.16005.pdf)
- (arXiv 2023.08) Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts, [[Paper]](https://arxiv.org/pdf/2308.11793.pdf), [[Code]](https://github.com/VITA-Group/GNT-MOVE)
- (arXiv 2023.10) Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.05400.pdf)
- (arXiv 2023.12) GIVT: Generative Infinite-Vocabulary Transformers, [[Paper]](https://arxiv.org/pdf/2312.02116.pdf)
- (arXiv 2024.01) Efficient generative adversarial networks using linear additive-attention Transformers, [[Paper]](https://arxiv.org/pdf/2401.09596.pdf)

### Graph
- (arXiv 2022.09) Graph Reasoning Transformer for Image Parsing, [[Paper]](https://arxiv.org/pdf/2209.09545.pdf)
- (arXiv 2022.11) Rethinking Batch Sample Relationships for Data Representation: A Batch-Graph Transformer based Approach, [[Paper]](https://arxiv.org/pdf/2211.10622.pdf)
- (arXiv 2022.12) A Generalization of ViT/MLP-Mixer to Graphs, [[Paper]](https://arxiv.org/pdf/2212.13350.pdf), [[Code]](https://github.com/XiaoxinHe/Graph-MLPMixer)
- (arXiv 2023.02) Energy Transformer, [[Paper]](https://arxiv.org/pdf/2302.07253.pdf), [[Code]](https://github.com/bhoov/energy-transformer-jax)
- (arXiv 2023.02) MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis, [[Paper]](https://arxiv.org/pdf/2302.10574.pdf)
- (arXiv 2023.02) Contrastive Video Question Answering via Video Graph Transformer, [[Paper]](https://arxiv.org/pdf/2302.13668.pdf), [[Code]](https://github.com/doc-doc/CoVGT)
- (arXiv 2023.03) AMIGO: Sparse Multi-Modal Graph Transformer with Shared-Context Processing for Representation Learning of Giga-pixel Images, [[Paper]](https://arxiv.org/pdf/2303.00865.pdf), [[Code]](https://github.com/doc-doc/CoVGT)
- (arXiv 2023.03) An Adaptive GViT for Gas Mixture Identification and Concentration Estimation, [[Paper]](https://arxiv.org/pdf/2303.05685.pdf)
- (arXiv 2023.04) Transformer-based Graph Neural Networks for Outfit Generation, [[Paper]](https://arxiv.org/pdf/2304.08098.pdf)
- (arXiv 2023.05) GTNet: Graph Transformer Network for 3D Point Cloud Classification and ation, [[Paper]](https://arxiv.org/pdf/2305.15213.pdf)
- (arXiv 2023.05) Multi-scale Efficient Graph-Transformer for Whole Slide Image Classification, [[Paper]](https://arxiv.org/pdf/2305.15773.pdf)
- (arXiv 2023.06) NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning, [[Paper]](https://arxiv.org/pdf/2306.10792.pdf)
- (arXiv 2023.08) Geometric Learning-Based Transformer Network for Estimation of Segmentation Errors, [[Paper]](https://arxiv.org/pdf/2308.05068.pdf)
- (arXiv 2023.08) Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images, [[Paper]](https://arxiv.org/pdf/2308.11015.pdf)
- (arXiv 2023.08) Deep Prompt Tuning for Graph Transformers, [[Paper]](https://arxiv.org/pdf/2309.10131.pdf)
- (arXiv 2023.11) GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation, [[Paper]](https://arxiv.org/pdf/2311.03035.pdf), [[Code]](https://github.com/Ackesnal/GTP-ViT)
- (arXiv 2023.11) GMTR: Graph Matching Transformers, [[Paper]](https://arxiv.org/pdf/2311.08141.pdf)
- (arXiv 2023.12) GSGFormer: Generative Social Graph Transformer for Multimodal Pedestrian Trajectory Prediction, [[Paper]](https://arxiv.org/pdf/2312.04479.pdf)
- (arXiv 2023.12) Large-scale Graph Representation Learning of Dynamic Brain Connectome with Transformers, [[Paper]](https://arxiv.org/pdf/2312.14939.pdf)
- (arXiv 2024.01) Graph Transformer GANs with Graph Masked Modeling for Architectural Layout Generation, [[Paper]](https://arxiv.org/pdf/2401.07721.pdf)
- (arXiv 2024.02) Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification, [[Paper]](https://arxiv.org/pdf/2402.19339.pdf), [[Code]](https://github.com/delfimpandiani/Stitching-Gaps)

### Hand Gesture
- (arXiv 2022.01) ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density  EMG Signals, [[Paper]](https://arxiv.org/pdf/2201.10060.pdf)
- (arXiv 2023.07) Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting, [[Paper]](https://arxiv.org/pdf/2307.08243.pdf), [[Code]](https://github.com/Cogito2012/USST)
- (arXiv 2023.08) Nonrigid Object Contact Estimation With Regional Unwrapping Transformer, [[Paper]](https://arxiv.org/pdf/2308.14074.pdf)
- (arXiv 2023.10) BodyFormer: Semantics-guided 3D Body Gesture Synthesis with Transformer, [[Paper]](https://arxiv.org/pdf/2310.06851.pdf)
- (arXiv 2023.11) Improving Hand Recognition in Uncontrolled and Uncooperative Environments using Multiple Spatial Transformers and Loss Functions, [[Paper]](https://arxiv.org/pdf/2311.05383.pdf)
- (arXiv 2023.12) Reconstructing Hands in 3D with Transformers, [[Paper]](https://arxiv.org/pdf/2312.05251.pdf), [[Code]](https://geopavlakos.github.io/hamer/)

### High Dynamic Range Imaging
- (arXiv 2022.08) Ghost-free High Dynamic Range Imaging with Context-aware Transformer, [[Paper]](https://arxiv.org/pdf/2208.05114.pdf), [[Code]](https://github.com/megvii-research/HDR-Transformer)
- (arXiv 2023.03) SpiderMesh: Spatial-aware Demand-guided Recursive Meshing for RGB-T ation, [[Paper]](https://arxiv.org/pdf/2303.08704.pdf)
- (arXiv 2023.04) High Dynamic Range Imaging with Context-aware Transformer, [[Paper]](https://arxiv.org/pdf/2304.04416.pdf)
- (arXiv 2023.05) Alignment-free HDR Deghosting with Semantics Consistent Transformer, [[Paper]](https://arxiv.org/pdf/2305.18135.pdf), [[Code]](https://steven-tel.github.io/sctnet)
- (arXiv 2023.09) IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging, [[Paper]](https://arxiv.org/pdf/2309.15019.pdf)

### HOI
- (CVPR'21) HOTR: End-to-End Human-Object Interaction Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2104.13682.pdf), [[Code]](https://github.com/bbepoch/HoiTransformer)
- (arXiv 2021.03) QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information, [[Paper]](https://arxiv.org/pdf/2103.05399), [[Code]](https://github.com/hitachi-rd-cv/qpic)
- (arXiv 2021.03) Reformulating HOI Detection as Adaptive Set Prediction, [[Paper]](https://arxiv.org/pdf/2103.05983), [[Code]](https://github.com/yoyomimi/AS-Net)
- (arXiv 2021.03) End-to-End Human Object Interaction Detection with HOI Transformer, [[Paper]](https://arxiv.org/pdf/2103.04503.pdf), [[Code]](https://github.com/bbepoch/HoiTransformer)
- (arXiv 2021.05) Visual Composite Set Detection Using Part-and-Sum Transformers, [[Paper]](https://arxiv.org/pdf/2105.02170.pdf)
- (arXiv 2021.08) GTNet:Guided Transformer Network for Detecting Human-Object Interactions, [[Paper]](https://arxiv.org/pdf/2108.00596.pdf), [[Code]](https://github.com/ASMIftekhar/GTNet)
- (arXiv 2021.12) Efficient Two-Stage Detection of Human-Object Interactions with a Novel Unary-Pairwise Transformer, [[Paper]](https://arxiv.org/pdf/2112.01838.pdf), [[Code]](https://github.com/fredzzhang/upt)
- (arXiv 2022.03) Iwin: Human-Object Interaction Detection via Transformer with Irregular Windows, [[Paper]](https://arxiv.org/pdf/2203.10537.pdf)
- (arXiv 2022.03) MSTR: Multi-Scale Transformer for End-to-End Human-Object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2203.14709.pdf)
- (arXiv 2022.04) What to look at and where: Semantic and Spatial Refined Transformer for detecting human-object interactions, [[Paper]](https://arxiv.org/pdf/2204.00746.pdf)
- (arXiv 2022.04) End-to-End Zero-Shot HOI Detection via Vision and Language Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2204.03541.pdf), [[Code]](https://github.com/mrwu-mac/EoID)
- (arXiv 2022.04) Category-Aware Transformer Network for Better Human-Object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2204.04911.pdf)
- (arXiv 2022.04) Consistency Learning via Decoding Path Augmentation for Transformers in Human Object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2204.04836.pdf), [[Code]](https://github.com/mlvlab/CPChoi)
- (arXiv 2022.04) Human-Object Interaction Detection via Disentangled Transformer, [[Paper]](https://arxiv.org/pdf/2204.09290.pdf)
- (arXiv 2022.06) Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2206.06291.pdf), [[Code]](https://github.com/zyong812/STIP)
- (arXiv 2022.07) Towards Hard-Positive Query Mining for DETR-based Human-Object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2207.05293.pdf), [[Code]](https://github.com/MuchHair/HQM)
- (arXiv 2022.07) IGFormer: Interaction Graph Transformer for Skeleton-based Human Interaction Recognition, [[Paper]](https://arxiv.org/pdf/2207.12100.pdf)
- (arXiv 2023.04) ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for Human-Object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2304.08114.pdf), [[Code]](https://github.com/Jeeseung-Park/ViPLO)
- (arXiv 2023.08) Exploring Predicate Visual Context in Detecting of Human鈥揙bject Interactions, [[Paper]](https://arxiv.org/pdf/2308.06202.pdf), [[Code]](https://github.com/fredzzhang/pvic)
- (arXiv 2023.08) Compositional Learning in Transformer-Based Human-Object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2308.05961.pdf)
- (arXiv 2023.08) Agglomerative Transformer for Human-Object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2308.08370.pdf), [[Code]](https://github.com/six6607/AGER.git)
- (arXiv 2024.01) A Two-stream Hybrid CNN-Transformer Network for Skeleton-based Human Interaction Recognition, [[Paper]](https://arxiv.org/pdf/2401.00409.pdf), [[Code]](https://github.com/six6607/AGER.git)

### Hyperspectral
- (arXiv 2021.07) SpectralFormer: Rethinking Hyperspectral Image Classification with Transformers, [[Paper]](https://arxiv.org/pdf/2107.02988.pdf), [[Code]](https://sites.google.com/view/danfeng-hong)
- (arXiv 2021.10) 3D-ANAS v2: Grafting Transformer Module on Automatically Designed ConvNet for Hyperspectral Image Classification, [[Paper]](https://arxiv.org/pdf/2110.11084.pdf), [[Code]](https://github.com/xmm/3D-ANAS-V2)
- (arXiv 2021.11) Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction, [[Paper]](https://arxiv.org/pdf/2111.07910.pdf)
- (arXiv 2021.11) Learning A 3D-CNN and Transformer Prior for Hyperspectral Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2111.13923.pdf)
- (arXiv 2022.03) HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening, [[Paper]](https://arxiv.org/pdf/2203.02503.pdf)
- (arXiv 2022.03) Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Image Classificationtion, [[Paper]](https://arxiv.org/pdf/2203.04771.pdf)
- (arXiv 2022.03) Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction, [[Paper]](https://arxiv.org/pdf/2203.04845.pdf)
- (arXiv 2022.03) Deep Hyperspectral Unmixing using Transformer Network, [[Paper]](https://arxiv.org/pdf/2203.17076.pdf), [[Code]](https://github.com/preetam22n/DeepTrans-HSU)
- (arXiv 2022.04) MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction, [[Paper]](https://arxiv.org/pdf/2204.07908.pdf), [[Code]](https://github.com/cuiziteng/IlluminationAdaptive-Transformer)
- (arXiv 2022.09) S^2-Transformer for Mask-Aware Hyperspectral Image Reconstruction, [[Paper]](https://arxiv.org/pdf/2209.12075.pdf)
- (arXiv 2023.03) MSFA-Frequency-Aware Transformer for Hyperspectral Images Demosaicing, [[Paper]](https://arxiv.org/pdf/2303.13404.pdf)
- (arXiv 2023.04) MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for Methane Detection, [[Paper]](https://arxiv.org/pdf/2304.02767.pdf)
- (arXiv 2023.04) DCN-T: Dual Context Network with Transformer for Hyperspectral Image Classification, [[Paper]](https://arxiv.org/pdf/2304.09915.pdf), [[Code]](https://github.com/DotWang/DCN-T)
- (arXiv 2023.05) SST-ReversibleNet: Reversible-prior-based Spectral-Spatial Transformer for Efficient Hyperspectral Image Reconstruction, [[Paper]](https://arxiv.org/pdf/2305.04054.pdf), [[Code]](https://github.com/caizeyu1992/SST)
- (arXiv 2023.06) SaaFormer: Spectral-spatial Axial Aggregation Transformer for Hyperspectral Image Classification, [[Paper]](https://arxiv.org/pdf/2306.16759.pdf)
- (arXiv 2023.08) Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction, [[Paper]](https://arxiv.org/pdf/2308.10820.pdf), [[Code]](https://github.com/MyuLi/PADUT)
- (arXiv 2023.09) FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training, [[Paper]](https://arxiv.org/pdf/2309.09431.pdf), [[Code]](https://github.com/csiro-robotics/factoformer)
- (arXiv 2023.10) Multiview Transformer: Rethinking Spatial Information in Hyperspectral Image Classification, [[Paper]](https://arxiv.org/pdf/2310.07186.pdf)
- (arXiv 2023.10) MLP-AMDC: An MLP Architecture for Adaptive-Mask-based Dual-Camera snapshot hyperspectral imaging, [[Paper]](https://arxiv.org/pdf/2310.08002.pdf), [[Code]](https://github.com/caizeyu1992/MLP-AMDC)
- (arXiv 2023.11) Learning transformer-based heterogeneously salient graph representation for multimodal fusion classification of hyperspectral image and LiDAR data, [[Paper]](https://arxiv.org/pdf/2311.10320.pdf)
- (arXiv 2023.12) Pixel-to-Abundance Translation: Conditional Generative Adversarial Networks Based on Patch Transformer for Hyperspectral Unmixing, [[Paper]](https://arxiv.org/pdf/2401.15275.pdf)

### Illumination
- (arXiv 2022.05) Illumination Adaptive Transformer, [[Paper]](https://arxiv.org/pdf/2205.14871.pdf), [[Code]](https://github.com/caiyuanhao1998/MST-plus-plus)

### Incremental Learning
- (arXiv 2021.12) Improving Vision Transformers for Incremental Learning, [[Paper]](https://arxiv.org/pdf/2112.06103.pdf)
- (arXiv 2022.03) Meta-attention for ViT-backed Continual Learning, [[Paper]](https://arxiv.org/pdf/2203.11684.pdf), [[Code]](https://github.com/zju-vipa/MEAT-TIL)
- (arXiv 2022.03)Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization, [[Paper]](https://arxiv.org/pdf/2203.13167.pdf)
- (arXiv 2022.08) D3Former: Debiased Dual Distilled Transformer for Incremental Learning, [[Paper]](https://arxiv.org/pdf/2208.00777.pdf), [[Code]](https://tinyurl.com/d3former)
- (arXiv 2022.10) A Memory Transformer Network for Incremental Learning, [[Paper]](https://arxiv.org/pdf/2210.04485.pdf)
- (arXiv 2023.01) Combined Use of Federated Learning and Image Encryption for Privacy-Preserving Image Classification with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2301.09255.pdf)
- (arXiv 2023.03) Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning, [[Paper]](https://arxiv.org/pdf/2303.08250.pdf)
- (arXiv 2023.03) Dense Network Expansion for Class Incremental Learning, [[Paper]](https://arxiv.org/pdf/2303.12696.pdf)
- (arXiv 2023.03) Semantic-visual Guided Transformer for Few-shot Class-incremental Learning, [[Paper]](https://arxiv.org/pdf/2303.15494.pdf)
- (arXiv 2023.04) Continual Detection Transformer for Incremental Object Detection, [[Paper]](https://arxiv.org/pdf/2304.03110.pdf)
- (arXiv 2023.04) Preserving Locality in Vision Transformers for Class Incremental Learning, [[Paper]](https://arxiv.org/pdf/2304.06971.pdf)
- (arXiv 2023.05) BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning, [[Paper]](https://arxiv.org/pdf/2305.04769.pdf), [[Code]](https://github.com/NeurAI-Lab/BiRT)
- (arXiv 2023.06) TADIL: Task-Agnostic Domain-Incremental Learning through Task-ID Inference using Transformer Nearest-Centroid Embeddings, [[Paper]](https://arxiv.org/pdf/2306.11955.pdf)
- (arXiv 2023.08) On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.09372.pdf), [[Code]](https://github.com/tdemin16/Continual-LayerNorm-Tuning)
- (arXiv 2023.08) Exemplar-Free Continual Transformer with Convolutions, [[Paper]](https://arxiv.org/pdf/2308.11357.pdf), [[Projet]](https://cvir.github.io/projects/contracon)
- (arXiv 2023.08) Introducing Language Guidance in Prompt-based Continual Learning, [[Paper]](https://arxiv.org/pdf/2308.15827.pdf)
- (arXiv 2023.11) CMFDFormer: Transformer-based Copy-Move Forgery Detection with Continual Learning, [[Paper]](https://arxiv.org/pdf/2311.13263.pdf)
- (arXiv 2023.12) Fine-Grained Knowledge Selection and Restoration for Non-Exemplar Class Incremental Learning, [[Paper]](https://arxiv.org/pdf/2312.12722.pdf), [[Code]](https://github.com/scok30/)
- (arXiv 2024.01) PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning, [[Paper]](https://arxiv.org/pdf/2401.14807.pdf), [[Code]](https://github.com/TianSongS/PL-FSCIL)
- (arXiv 2024.01) Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks, [[Paper]](https://arxiv.org/pdf/2401.14807.pdf), [[Code]](https://github.com/TianSongS/PL-FSCIL)

### In-painting
- (ECCV'20) Learning Joint Spatial-Temporal Transformations for Video Inpainting, [[Paper]](https://arxiv.org/abs/2007.10247), [[Code]](https://github.com/researchmm/STTN)
- (arXiv 2021.04) Aggregated Contextual Transformations for High-Resolution Image Inpainting, [[Paper]](https://arxiv.org/abs/2104.01431), [[Code]](https://github.com/researchmm/AOT-GAN-for-Inpainting)
- (arXiv 2021.04) Decoupled Spatial-Temporal Transformer for Video Inpainting, [[Paper]](https://arxiv.org/pdf/2112.08275.pdf), [[Code]](https://github.com/wjf5203/SeqFormer)
- (arXiv 2022.03) Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding, [[Paper]](https://arxiv.org/pdf/2203.00867.pdf), [[Code]](https://github.com/DQiaole/ZITS_inpainting)
- (arXiv 2022.03) MAT: Mask-Aware Transformer for Large Hole Image Inpainting, [[Paper]](https://arxiv.org/pdf/2203.15270.pdf), [[Code]](https://github.com/fenglinglwb/MAT)
- (arXiv 2022.05) Reduce Information Loss in Transformers for Pluralistic Image Inpainting, [[Paper]](https://arxiv.org/pdf/2205.05076.pdf)
- (arXiv 2022.08) Flow-Guided Transformer for Video Inpainting, [[Paper]](https://arxiv.org/pdf/2208.06768.pdf), [[Code]](https://github.com/hitachinsk/FGT)
- (arXiv 2022.09) DeViT: Deformed Vision Transformers in Video Inpainting, [[Paper]](https://arxiv.org/pdf/2209.13925.pdf)
- (arXiv 2022.10) TPFNet: A Novel Text In-painting Transformer for Text Removal, [[Paper]](https://arxiv.org/pdf/2210.14461.pdf), [[Code]](https://github.com/CandleLabAI/TPFNet)
- (arXiv 2023.01) Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting, [[Paper]](https://arxiv.org/pdf/2301.10048.pdf)
- (arXiv 2023.05) T-former: An Efficient Transformer for Image Inpainting, [[Paper]](https://arxiv.org/pdf/2305.07239.pdf), [[Code]](https://github.com/dengyecode/T-former_image_inpainting)
- (arXiv 2023.06) TransRef: Multi-Scale Reference Embedding Transformer for Reference-Guided Image Inpainting, [[Paper]](https://arxiv.org/pdf/2306.11528.pdf), [[Code]](https://github.com/Cameltr/TransRef)
- (arXiv 2023.07) Deficiency-Aware Masked Transformer for Video Inpainting, [[Paper]](https://arxiv.org/pdf/2307.08629.pdf), [[Code]](http://github.com/yeates/DMT)
- (arXiv 2023.09) ProPainter: Improving Propagation and Transformer for Video Inpainting, [[Paper]](https://arxiv.org/pdf/2309.03897.pdf), [[Code]](https://github.com/sczhou/ProPainter)
- (arXiv 2024.01) Federated Class-Incremental Learning with Prototype Guided Transformer, [[Paper]](https://arxiv.org/pdf/2401.02094.pdf)
- (arXiv 2024.02) HINT: High-quality INPainting Transformer with Mask-Aware Encoding and Enhanced Attention, [[Paper]](https://arxiv.org/pdf/2402.14185.pdf), [[Code]](https://github.com/ChrisChen1023/HINT)

### Instance Segmentation
- (CVPR'21) End-to-End Video Instance Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14503), [[Code]](https://github.com/Epiphqny/VisTR)
- (arXiv 2021.04) ISTR: End-to-End Instance Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2105.00637.pdf), [[Code]](https://github.com/hujiecpp/ISTR)
- (arXiv 2021.08) SOTR: Segmenting Objects with Transformers, [[Paper]](https://arxiv.org/pdf/2108.06747.pdf), [[Code]](https://github.com/easton-cau/SOTR)
- (arXiv 2021.12) SeqFormer: a Frustratingly Simple Model for Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2108.06747.pdf), [[Code]](https://github.com/easton-cau/SOTR)
- (arXiv 2021.12) A Simple Single-Scale Vision Transformer for Object Localization and Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2112.09747.pdf)
- (arXiv 2021.12) SOIT: Segmenting Objects with Instance-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2112.11037.pdf), [[Code]](https://github.com/yuxiaodongHRI/SOIT)
- (arXiv 2022.03) Video Instance Segmentation via Multi-scale Spatio-temporal Split Attention Transformer, [[Paper]](https://arxiv.org/pdf/2203.13253.pdf), [[Code]](https://github.com/OmkarThawakar/MSSTS-VIS)
- (arXiv 2022.04) Temporally Efficient Vision Transformer for Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2204.08412.pdf), [[Code]](https://github.com/hustvl/TeViT)
- (arXiv 2022.04) Less than Few: Self-Shot Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2204.08874.pdf)
- (arXiv 2022.06) Consistent Video Instance Segmentation with Inter-Frame Recurrent Attention, [[Paper]](https://arxiv.org/pdf/2206.07011.pdf)
- (arXiv 2022.06) Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2206.10845.pdf), [[Code]](https://accessibility-cv.github.io/)
- (arXiv 2022.07) OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2207.02255.pdf), [[Code]](https://github.com/PJLallen/OSFormer)
- (arXiv 2022.07) In Defense of Online Models for Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2207.10661.pdf), [[Code]](https://github.com/wjf5203/VNext)
- (arXiv 2022.07) Video Mask Transfiner for High-Quality Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2207.14012.pdf), [[Project]](https://www.vis.xyz/pub/vmt)
- (arXiv 2022.08) InstanceFormer: An Online Video Instance Segmentation Framework, [[Paper]](https://arxiv.org/pdf/2208.10547.pdf), [[Code]](https://github.com/rajatkoner08/InstanceFormer)
- (arXiv 2022.09) RNGDet++: Road Network Graph Detection by Transformer with Instance Segmentation and Multi-scale Features Enhancement, [[Paper]](https://arxiv.org/pdf/2209.10150.pdf), [[Code]](https://tonyxuqaq.github.io/projects/RNGDetPlusPlus/)
- (arXiv 2022.10) AISFormer: Amodal Instance Segmentation with Transformer, [[Paper]](https://arxiv.org/pdf/2210.06323.pdf), [[Code]](https://github.com/UARK-AICV/AISFormer)
- (arXiv 2022.10) TOIST: Task Oriented Instance Segmentation Transformer with Noun-Pronoun Distillation, [[Paper]](https://arxiv.org/pdf/2210.10775.pdf), [[Code]](https://github.com/AIR-DISCOVER/TOIST)
- (arXiv 2022.11) Mean Shift Mask Transformer for Unseen Object Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2211.11679.pdf), [[Code]](https://github.com/YoungSean/UnseenObjectsWithMeanShift)
- (arXiv 2022.11) Transformer for 3D Scene Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2211.15766.pdf), [[Code]](https://github.com/sunjiahao1999/SPFormer)
- (arXiv 2023.01) Vision Transformers Are Good Mask Auto-Labelers, [[Paper]](https://arxiv.org/pdf/2301.03992.pdf), [[Code]](https://github.com/NVlabs/mask-auto-labeler)
- (arXiv 2023.01) Towards Robust Video Instance Segmentation with Temporal-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2301.09416.pdf)
- (arXiv 2023.03) MobileInst: Video Instance Segmentation on the Mobile, [[Paper]](https://arxiv.org/pdf/2303.17594.pdf)
- (arXiv 2023.04) DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive Segmentation Transformer, [[Paper]](https://arxiv.org/pdf/2304.06668.pdf)
- (arXiv 2023.04) Vision Transformers Are Good Mask Auto-Labelers, [[Paper]](https://arxiv.org/pdf/2305.04609.pdf), [[Code]](https://github.com/ayanban011/SwinDocSegmenter)
- (arXiv 2023.06) CalibNet: Dual-branch Cross-modal Calibration for RGB-D Salient Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2307.08098.pdf), [[Code]](https://github.com/PJLallen/CalibNet)
- (arXiv 2023.08) Partitioned Saliency Ranking with Dense Pyramid Transformers, [[Paper]](https://arxiv.org/pdf/2308.00236.pdf), [[Code]](https://github.com/ssecv/PSR)
- (arXiv 2023.08) Exploring Transformers for Open-world Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2308.04206.pdf)
- (arXiv 2023.08) Mask Frozen-DETR: High Quality Instance Segmentation with One GPU, [[Paper]](https://arxiv.org/pdf/2308.03747.pdf)
- (arXiv 2023.08) A Unified Query-based Paradigm for Camouflaged Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2308.07392.pdf), [[Code]](https://github.com/dongbo811/UQFormer)
- (arXiv 2023.08) NOVIS: A Case for End-to-End Near-Online Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2308.15266.pdf)
- (arXiv 2023.09) Mask-Attention-Free Transformer for 3D Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2309.01692.pdf), [[Code]](https://github.com/dvlab-research/Mask-Attention-Free-Transformer)
- (arXiv 2023.09) TCOVIS: Temporally Consistent Online Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2309.11857.pdf), [[Code]](https://github.com/jun-long-li/TCOVIS)
- (arXiv 2023.09) 3D Indoor Instance Segmentation in an Open-World, [[Paper]](https://arxiv.org/pdf/2309.14338.pdf), [[Code]](https://github.com/aminebdj/3D-OWIS)
- (arXiv 2023.10) MSFormer: A Skeleton-multiview Fusion Method For Tooth Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2310.14489.pdf)
- (arXiv 2023.12) PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation, [[Paper]](https://arxiv.org/pdf/2312.03015.pdf),[[Code]](https://github.com/zyc00/PartSLIP2)
- (arXiv 2023.12) EipFormer: Emphasizing Instance Positions in 3D Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2312.05602.pdf)
- (arXiv 2024.03) ShapeFormer: Shape Prior Visible-to-Amodal Transformer-based Amodal Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2403.11376.pdf),[[Code]](https://github.com/UARK-AICV/ShapeFormer)

### Knowledge Distillation
- (arXiv 2022.04) DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.12997.pdf)
- (arXiv 2022.05) Knowledge Distillation via the Target-aware Transformer, [[Paper]](https://arxiv.org/pdf/2205.10793.pdf)
- (arXiv 2022.05) Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation, [[Paper]](https://arxiv.org/pdf/2208.08037.pdf), [[Code]](https://github.com/SwinTransformer/Feature-Distillation)
- (arXiv 2022.09) ViTKD: Practical Guidelines for ViT feature knowledge distillation, [[Paper]](https://arxiv.org/pdf/2209.02432.pdf), [[Code]](https://github.com/yzd-v/cls_KD)
- (arXiv 2022.11) Knowledge Distillation for Detection Transformer with Consistent Distillation Points Sampling, [[Paper]](https://arxiv.org/pdf/2211.08071.pdf)
- (arXiv 2022.11) D3ETR: Decoder Distillation for Detection Transformer, [[Paper]](https://arxiv.org/pdf/2211.09768.pdf)
- (arXiv 2022.11) DETRDistill: A Universal Knowledge Distillation Framework for DETR-families, [[Paper]](https://arxiv.org/pdf/2211.10156.pdf)
- (arXiv 2022.12) Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning, [[Paper]](https://arxiv.org/pdf/2212.08320.pdf), [[Code]](https://github.com/RunpeiDong/ACT)
- (arXiv 2022.12) OVO: One-shot Vision Transformer Search with Online distillation, [[Paper]](https://arxiv.org/pdf/2212.13766.pdf)
- (arXiv 2023.02) Knowledge Distillation in Vision Transformers: A Critical Review, [[Paper]](https://arxiv.org/pdf/2302.02108.pdf)
- (arXiv 2023.02) MaskedKD: Efficient Distillation of Vision Transformers with Masked Images, [[Paper]](https://arxiv.org/pdf/2302.10494.pdf)
- (arXiv 2023.03) Multi-view knowledge distillation transformer for human action recognition, [[Paper]](https://arxiv.org/pdf/2303.14358.pdf)
- (arXiv 2023.03) Supervised Masked Knowledge Distillation for Few-Shot Transformers, [[Paper]](https://arxiv.org/pdf/2303.15466.pdf), [[Code]](https://github.com/HL-hanlin/SMKD)
- (arXiv 2023.05) Vision Transformers for Small Histological Datasets Learned through Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2305.17370.pdf)
- (arXiv 2023.05) Are Large Kernels Better Teachers than Transformers for ConvNets?, [[Paper]](https://arxiv.org/pdf/2305.19412.pdf), [[Code]](https://github.com/VITA-Group/SLaK)
- (arXiv 2023.07) Cumulative Spatial Knowledge Distillation for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.08500.pdf)
- (arXiv 2023.10) CLIPSelf: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction, [[Paper]](https://arxiv.org/pdf/2310.01403.pdf), [[Code]](https://github.com/wusize/CLIPSelf)
- (arXiv 2023.10) Distilling Efficient Vision Transformers from CNNs for ation, [[Paper]](https://arxiv.org/pdf/2310.07265.pdf), [[Code]](https://vlislab22.github.io/C2VKD/)
- (arXiv 2023.10) One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2310.19444.pdf), [[Code]](https://github.com/Hao840/OFAKD)
- (arXiv 2023.11) Learning Contrastive Self-Distillation for Ultra-Fine-Grained Visual Categorization Targeting Limited Samples, [[Paper]](https://arxiv.org/pdf/2311.06056.pdf)
- (arXiv 2023.12) GIST: Improving Parameter Efficient Fine Tuning via Knowledge Interaction, [[Paper]](https://arxiv.org/pdf/2312.07255.pdf)
- (arXiv 2024.02) m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers, [[Paper]](https://arxiv.org/pdf/2402.16918.pdf), [[Code]](https://github.com/kamanphoebe/m2mKD)

### Lane
- (arXiv 2022.03) Laneformer: Object-aware Row-Column Transformers for Lane Detection, [[Paper]](https://arxiv.org/pdf/2203.09830.pdf)
- (arXiv 2022.03) PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark, [[Paper]](https://arxiv.org/pdf/2203.11089.pdf), [[Project]](https://github.com/OpenPerceptionX/OpenLane)
- (arXiv 2022.09) PriorLane: A Prior Knowledge Enhanced Lane Detection Approach Based on Transformer, [[Paper]](https://arxiv.org/pdf/2209.06994.pdf), [[Code]](https://github.com/vincentqqb/PriorLane)
- (arXiv 2022.09) CurveFormer: 3D Lane Detection by Curve Propagation with Curve Queries and Attention, [[Paper]](https://arxiv.org/pdf/2209.07989.pdf)
- (arXiv 2023.08) LATR: 3D Lane Detection from Monocular Images with Transformer, [[Paper]](https://arxiv.org/pdf/2308.04583.pdf), [[Code]](https://github.com/JMoonr/LATR)
- (arXiv 2024.02) CurveFormer++: 3D Lane Detection by Curve Propagation with Temporal Curve Queries and Attention, [[Paper]](https://arxiv.org/pdf/2402.06423.pdf), [[Code]](https://github.com/JMoonr/LATR)
- (arXiv 2024.03) LDTR: Transformer-based Lane Detection with Anchor-chain Representation, [[Paper]](https://arxiv.org/pdf/2403.14354.pdf)

### Layout
- (CVPR'21) Variational Transformer Networks for Layout Generation, [[Paper]](https://arxiv.org/abs/2104.02416)
- (arXiv 2021.10) The Layout Generation Algorithm of Graphic Design Based on Transformer-CVAE, [[Paper]](https://arxiv.org/abs/2110.06794)
- (arXiv 2021.12) BLT: Bidirectional Layout Transformer for Controllable Layout Generation, [[Paper]](https://arxiv.org/abs/2112.05112)
- (arXiv 2022.02) ATEK: Augmenting Transformers with Expert Knowledge for Indoor Layout Synthesis, [[Paper]](https://arxiv.org/abs/2112.05112)
- (arXiv 2022.03) LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network, [[Paper]](https://arxiv.org/abs/2203.01824), [[Code]](https://github.com/zhigangjiang/LGT-Net)
- (arXiv 2022.08) UniLayout: Taming Unified Sequence-to-Sequence Transformers for Graphic Layout Generation, [[Paper]](https://arxiv.org/pdf/2208.08037.pdf)
- (arXiv 2022.09) Geometry Aligned Variational Transformer for Image-conditioned Layout Generation, [[Paper]](https://arxiv.org/pdf/2209.00852.pdf)
- (arXiv 2022.12) LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer, [[Paper]](https://arxiv.org/pdf/2212.09877.pdf), [[Code]](https://github.com/salesforce/LayoutDETR)
- (arXiv 2022.12) PanoViT: Vision Transformer for Room Layout Estimation from a Single Panoramic Image, [[Paper]](https://arxiv.org/pdf/2212.12156.pdf)
- (arXiv 2023.03) DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer, [[Paper]](https://arxiv.org/pdf/2303.03755.pdf)
- (arXiv 2023.04) GUILGET: GUI Layout GEneration with Transformer, [[Paper]](https://arxiv.org/pdf/2304.09012.pdf)
- (arXiv 2023.05) LayoutDM: Transformer-based Diffusion Model for Layout Generation, [[Paper]](https://arxiv.org/pdf/2305.02567.pdf)
- (arXiv 2023.08) MapPrior: Bird鈥檚-Eye View Map Layout Estimation with Generative Models, [[Paper]](https://arxiv.org/pdf/2308.12963.pdf), [[Code]](https://mapprior.github.io/)
- (arXiv 2023.08) Vision Grid Transformer for Document Layout Analysis, [[Paper]](https://arxiv.org/pdf/2308.14978.pdf), [[Code]](https://github.com/AlibabaResearch/AdvancedLiterateMachinery)
- (arXiv 2023.08) Document AI: A Comparative Study of Transformer-Based, Graph-Based Models, and Convolutional Neural Networks For Document Layout Analysis, [[Paper]](https://arxiv.org/pdf/2308.15517.pdf)
- (arXiv 2023.10) Dolfin: Diffusion Layout Transformers without Autoencoder, [[Paper]](https://arxiv.org/pdf/2310.16305.pdf)
- (arXiv 2023.11) LayoutPrompter: Awaken the Design Ability of Large Language Models, [[Paper]](https://arxiv.org/pdf/2311.06495.pdf), [[Code]](https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter)
- (arXiv 2023.11) Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation, [[Paper]](https://arxiv.org/pdf/2311.13602.pdf), [[Project]](https://udonda.github.io/RALF/)

### Lighting
- (arXiv 2022.02) Spatio-Temporal Outdoor Lighting Aggregation on Image Sequences using Transformer Networks, [[Paper]](https://arxiv.org/abs/2202.09206)
- (arXiv 2023.05) Ray-Patch: An Efficient Decoder for Light Field Transformers, [[Paper]](https://arxiv.org/abs/2305.09566)

### LLM/LVM
- (arXiv 2023.11) NExT-Chat: An LMM for Chat, Detection and Segmentation, [[Paper]](https://arxiv.org/pdf/2311.04498.pdf), [[Code]](https://next-chatv.github.io/)
- (arXiv 2023.11) u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model, [[Paper]](https://arxiv.org/pdf/2311.05348.pdf)
- (arXiv 2023.11) Towards Open-Ended Visual Recognition with Large Language Model, [[Paper]](https://arxiv.org/pdf/2311.08400.pdf), [[Code]](https://github.com/bytedance/OmniScient-Model)
- (arXiv 2023.11) Stable Segment Anything Model, [[Paper]](https://arxiv.org/pdf/2311.15776.pdf), [[Code]](https://github.com/fanq15/Stable-SAM)
- (arXiv 2023.11) Adapter is All You Need for Tuning Visual Tasks, [[Paper]](https://arxiv.org/pdf/2311.15010.pdf), [[Code]](https://github.com/Leiyi-Hu/mona)
- (arXiv 2023.11) LLaFS: When Large-Language Models Meet Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2311.16926.pdf), [[Code]](https://github.com/lanyunzhu99/LLaFS)
- (arXiv 2023.11) Efficient In-Context Learning in Vision-Language Models for Egocentric Videos, [[Paper]](https://arxiv.org/pdf/2311.17041.pdf), [[Code]](https://github.com/yukw777/EILEV)
- (arXiv 2023.11) Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model, [[Paper]](https://arxiv.org/pdf/2311.17112.pdf)
- (arXiv 2023.11) PoseGPT: Chatting about 3D Human Pose, [[Paper]](https://arxiv.org/pdf/2311.18836.pdf), [[Code]](https://yfeng95.github.io/posegpt)
- (arXiv 2023.11) InstructSeq: Unifying Vision Tasks with Instruction-conditioned Multi-modal Sequence Generation, [[Paper]](https://arxiv.org/pdf/2311.18835.pdf), [[Code]](https://github.com/rongyaofang/InstructSeq)
- (arXiv 2023.11) Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2311.18592.pdf), [[Code]](https://github.com/Event-AHU/SAFE_LargeVLM)
- (arXiv 2023.11) Contrastive Vision-Language Alignment Makes Efficient Instruction Learner, [[Paper]](https://arxiv.org/pdf/2311.17945.pdf), [[Code]](https://github.com/lizhaoliu-Lec/CG-VLM)
- (arXiv 2023.12) Bootstrapping SparseFormers from Vision Foundation Models, [[Paper]](https://arxiv.org/pdf/2312.01987.pdf), [[Code]](https://github.com/showlab/sparseformer)
- (arXiv 2023.12) IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks, [[Paper]](https://arxiv.org/pdf/2312.01771.pdf), [[Code]](https://jerryxu.net/IMProv)
- (arXiv 2023.12) Segment and Caption Anything, [[Paper]](https://arxiv.org/pdf/2312.00869.pdf), [[Code]](https://xk-huang.github.io/segment-caption-anything/)
- (arXiv 2023.12) EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything, [[Paper]](https://arxiv.org/pdf/2312.00863.pdf)
- (arXiv 2023.12) Segment Any 3D Gaussians, [[Paper]](https://arxiv.org/pdf/2312.00860.pdf), [[Code]](https://github.com/Jumpat/SegAnyGAussians)
- (arXiv 2023.12) Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts, [[Paper]](https://arxiv.org/pdf/2312.00968.pdf)
- (arXiv 2023.12) PixelLM: Pixel Reasoning with Large Multimodal Model, [[Paper]](https://arxiv.org/pdf/2312.02228.pdf), [[Code]](https://github.com/MaverickRen/PixelLM)
- (arXiv 2023.12) Foundation Model Assisted Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2312.03585.pdf)
- (arXiv 2023.12) AI-SAM: Automatic and Interactive Segment Anything Model, [[Paper]](https://arxiv.org/pdf/2312.03119.pdf), [[Code]](https://github.com/ymp5078/AI-SAM)
- (arXiv 2023.12) MobileSAMv2: Faster Segment Anything to Everything, [[Paper]](https://arxiv.org/abs/2312.09579),[[Code]](https://github.com/ChaoningZhang/MobileSAM)
- (arXiv 2023.12) MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices, [[Paper]](https://arxiv.org/abs/2312.16886),[[Code]](https://github.com/Meituan-AutoML/MobileVLM)
- (arXiv 2024.01) One for All: Toward Unified Foundation Models for Earth Vision, [[Paper]](https://arxiv.org/pdf/2401.07527.pdf)
- (arXiv 2024.01) RAP-SAM: Towards Real-Time All-Purpose Segment Anything, [[Paper]](https://arxiv.org/pdf/2401.10228.pdf), [[Code]](https://github.com/xushilin1/RAP-SAM/)
- (arXiv 2024.02) MobileVLM V2: Faster and Stronger Baseline for Vision Language Model, [[Paper]](https://arxiv.org/pdf/2402.03766.pdf), [[Code]](https://github.com/Meituan-AutoML/MobileVLM)
- (arXiv 2024.02) Data-efficient Large Vision Models through Sequential Autoregression, [[Paper]](https://arxiv.org/pdf/2402.04841.pdf), [[Code]](https://github.com/ggjy/DeLVM)
- (arXiv 2024.02) EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss, [[Paper]](https://arxiv.org/pdf/2402.05008.pdf), [[Code]](https://github.com/mit-han-lab/efficientvit)
- (arXiv 2024.02) Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models, [[Paper]](https://arxiv.org/pdf/2402.08473.pdf)
- (arXiv 2024.02) PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter, [[Paper]](https://arxiv.org/pdf/2402.10896.pdf)
- (arXiv 2024.02) GROUNDHOG : Grounding Large Language Models to Holistic Segmentation, [[Paper]](https://arxiv.org/pdf/2402.16846.pdf)
- (arXiv 2024.03) VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model, [[Paper]](https://arxiv.org/pdf/2403.05346.pdf)
- (arXiv 2024.03) Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models, [[Paper]](https://arxiv.org/pdf/2403.09635.pdf), [[Code]](https://github.com/akhilkedia/TranformersGetStable)

### Matching
- (CVPR'21') LoFTR: Detector-Free Local Feature Matching with Transformers, [[Paper]](https://arxiv.org/abs/2104.00680), [[Code]](https://zju3dv.github.io/loftr/)
- (arXiv 2022.02) Local Feature Matching with Transformers for low-end devices, [[Paper]](https://arxiv.org/pdf/2202.00770.pdf), [[Code]](https://github.com/Kolkir/Coarse_LoFTR_TRT)
- (arXiv 2022.02) CATs++: Boosting Cost Aggregation with Convolutions and Transformers, [[Paper]](https://arxiv.org/pdf/2202.06817.pdf), [[Code]](https://github.com/SunghwanHong/Cost-Aggregation-transformers)
- (arXiv 2022.03) MatchFormer: Interleaving Attention in Transformers for Feature Matching, [[Paper]](https://arxiv.org/pdf/2203.09645.pdf), [[Code]](https://github.com/jamycheung/MatchFormer)
- (arXiv 2022.05) TransforMatcher: Match-to-Match Attention for Semantic Correspondence, [[Paper]](https://arxiv.org/pdf/2205.11634.pdf), [[Code]](http://cvlab.postech.ac.kr/research/TransforMatcher)
- (arXiv 2022.07) Deep Laparoscopic Stereo Matching with Transformers, [[Paper]](https://arxiv.org/pdf/2207.12152.pdf)
- (arXiv 2022.08) ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer, [[Paper]](https://arxiv.org/pdf/2208.14201.pdf), [[Project]](https://aspanformer.github.io/)
- (arXiv 2023.01) DeepMatcher: A Deep Transformer-based Network for Robust and Accurate Local Feature Matching, [[Paper]](https://arxiv.org/pdf/2301.02993.pdf), [[Code]](https://github.com/XT-1997/DeepMatcher)
- (arXiv 2023.03) ParaFormer: Parallel Attention Transformer for Efficient Feature Matching, [[Paper]](https://arxiv.org/pdf/2303.00941.pdf)
- (arXiv 2023.03) Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints, [[Paper]](https://arxiv.org/pdf/2303.02885.pdf)
- (arXiv 2023.03) Adaptive Spot-Guided Transformer for Consistent Local Feature Matching, [[Paper]](https://arxiv.org/pdf/2303.16624.pdf), [[Code]](https://astr2023.github.io/)
- (arXiv 2023.05) AMatFormer: Efficient Feature Matching via Anchor Matching Transformer, [[Paper]](https://arxiv.org/pdf/2305.19205.pdf)
- (arXiv 2023.08) Multi-scale Alternated Attention Transformer for Generalized Stereo Matching, [[Paper]](https://arxiv.org/pdf/2308.03048.pdf)
- (arXiv 2023.10) FMRT: Learning Accurate Feature Matching with Reconciliatory Transformer, [[Paper]](https://arxiv.org/pdf/2310.13605.pdf)
- (arXiv 2023.11) LGFCTR: Local and Global Feature Convolutional Transformer for Image Matching, [[Paper]](https://arxiv.org/pdf/2311.17571.pdf), [[Code]](https://github.com/zwh0527/LGFCTR)
- (arXiv 2023.12) Latent Space Editing in Transformer-Based Flow Matching, [[Paper]](https://arxiv.org/pdf/2312.10825.pdf), [[Code]](https://taohu.me/lfm/)

### Matting
- (arXiv 2022.03) MatteFormer: Transformer-Based Image Matting via Prior-Tokens, [[Paper]](https://arxiv.org/pdf/2203.15662.pdf), [[Code]](https://github.com/webtoon/matteformer)
- (arXiv 2022.08) TransMatting: Enhancing Transparent Objects Matting with Transformers, [[Paper]](https://arxiv.org/pdf/2208.03007.pdf), [[Code]](https://github.com/AceCHQ/TransMatting)
- (arXiv 2022.08) VMFormer: End-to-End Video Matting with Transformer, [[Paper]](https://arxiv.org/pdf/2208.12801.pdf), [[Project]](https://chrisjuniorli.github.io/project/VMFormer/)
- (arXiv 2023.03) TransMatting: Tri-token Equipped Transformer Model for Image Matting, [[Paper]](https://arxiv.org/pdf/2303.06476.pdf), [[Project]](https://github.com/AceCHQ/TransMatting)
- (arXiv 2023.05) ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.15272.pdf)
- (arXiv 2023.08) EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting, [[Paper]](https://arxiv.org/pdf/2308.12831.pdf)

### Medical
- (arXiv 2021.02) TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation, [[Paper]](https://arxiv.org/abs/2102.04306), [[Code]](https://github.com/Beckschen/TransUNet)
- (arXiv 2021.02) Medical Transformer: Gated Axial-Attention for Medical Image Segmentation, [[Paper]](https://arxiv.org/abs/2102.10662), [[Code]](https://github.com/jeya-maria-jose/Medical-Transformer)
- (arXiv 2021.03) SpecTr: Spectral Transformer for Hyperspectral Pathology Image Segmentation, [[Paper]](https://arxiv.org/abs/2103.03604), [[Code]](https://github.com/hfut-xc-yun/SpecTr)
- (arXiv 2021.03) TransBTS: Multimodal Brain Tumor Segmentation Using Transformer, [[Paper]](https://arxiv.org/abs/2103.04430), [[Code]](https://github.com/Wenxuan-1119/TransBTS)
- (arXiv 2021.03) TransMed: Transformers Advance Multi-modal Medical Image Classification, [[Paper]](https://arxiv.org/abs/2103.05940)
- (arXiv 2021.03) U-Net Transformer: Self and Cross Attention for Medical Image Segmentation, [[Paper]](https://arxiv.org/abs/2103.06104)
- (arXiv 2021.03) SUNETR: Transformers for 3D Medical Image Segmentation, [[Paper]](https://arxiv.org/abs/2103.10504)
- (arXiv 2021.04) DeepProg: A Multi-modal Transformer-based End-to-end Framework for Predicting Disease Prognosis, [[Paper]](https://arxiv.org/pdf/2104.03642.pdf)
- (arXiv 2021.04) Vision Transformer using Low-level Chest X-ray Feature Corpus for COVID-19 Diagnosis and Severity Quantification, [[Paper]](https://arxiv.org/pdf/2104.07235.pdf)
- (arXiv 2021.04) Shoulder Implant X-Ray Manufacturer Classification: Exploring with Vision Transformer, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2104/2104.07667.pdf)
- (arXiv 2021.04) Medical Transformer: Universal Brain Encoder for 3D MRI Analysis, [[Paper]](https://arxiv.org/pdf/2104.13633.pdf)
- (arXiv 2021.04) Crossmodal Matching Transformer for Interventional in TEVAR, [[Paper]](https://arxiv.org/pdf/2104.14273.pdf)
- (arXiv 2021.04) GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification, [[Paper]](https://arxiv.org/pdf/2104.14528.pdf)
- (arXiv 2021.04) Pyramid Medical Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2104/2104.14702.pdf)
- (arXiv 2021.05) Anatomy-Guided Parallel Bottleneck Transformer Network for Automated Evaluation of Root Canal Therapy, [[Paper]](https://arxiv.org/abs/2105.00381)
- (arXiv 2021.05) Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2105.05537.pdf), [[Code]](https://github.com/HuCaoFighting/Swin-Unet)
- (arXiv 2021.05) Is Image Size Important? A Robustness Comparison of Deep Learning Methods for Multi-scale Cell Image Classification Tasks: from Convolutional Neural Networks to Visual Transformers, [[Paper]](https://arxiv.org/pdf/2105.07402.pdf)
- (arXiv 2021.05) Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers, [[Paper]](https://arxiv.org/pdf/2105.08059.pdf)
- (arXiv 2021.05) Medical Image Segmentation using Squeeze-and-Expansion Transformers, [[Paper]](https://arxiv.org/pdf/2105.09511.pdf), [[Code]](https://github.com/askerlee/segtran)
- (arXiv 2021.05) POCFormer: A Lightweight Transformer Architecture for Detection of COVID-19 Using Point of Care Ultrasound, [[Paper]](https://arxiv.org/pdf/2105.09913.pdf)
- (arXiv 2021.05) COTR: Convolution in Transformer Network for End to End Polyp Detection, [[Paper]](https://arxiv.org/pdf/2105.10925.pdf)
- (arXiv 2021.05) PTNet: A High-Resolution Infant MRI Synthesizer Based on Transformer, [[Paper]](https://arxiv.org/pdf/2105.13993.pdf)
- (arXiv 2021.06) TED-net: Convolution-free T2T Vision Transformerbased Encoder-decoder Dilation network for Low-dose CT Denoising, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2106/2106.04650.pdf)
- (arXiv 2021.06) A Multi-Branch Hybrid Transformer Network for Corneal Endothelial Cell Segmentation, [[Paper]](https://arxiv.org/pdf/2106.07557.pdf)
- (arXiv 2021.06) Task Transformer Network for Joint MRI Reconstruction and Super-Resolution, [[Paper]](https://arxiv.org/pdf/2106.06742.pdf), [[Code]](https://github.com/chunmeifeng/T2Net)
- (arXiv 2021.06) DS-TransUNet: Dual Swin Transformer U-Net for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2106.06716.pdf)
- (arXiv 2021.06) More than Encoder: Introducing Transformer Decoder to Upsample, [[Paper]](https://arxiv.org/pdf/2106.10637.pdf)
- (arXiv 2021.06) Instance-based Vision Transformer for Subtyping of Papillary Renal Cell Carcinoma in Histopathological Image, [[Paper]](https://arxiv.org/pdf/2106.12265.pdf)
- (arXiv 2021.06) MTrans: Multi-Modal Transformer for Accelerated MR Imaging, [[Paper]](https://arxiv.org/pdf/2106.14248.pdf), [[Code]](https://github.com/chunmeifeng/MTrans)
- (arXiv 2021.06) Multi-Compound Transformer for Accurate Biomedical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2106.14385.pdf), [[Code]](https://github.com/JiYuanFeng/MCTrans)
- (arXiv 2021.07) ResViT: Residual vision transformers for multi-modal medical image synthesis, [[Paper]](https://arxiv.org/pdf/2106.16031.pdf)
- (arXiv 2021.07) E-DSSR: Efficient Dynamic Surgical Scene Reconstruction with Transformer-based Stereoscopic Depth Perception, [[Paper]](https://arxiv.org/pdf/2107.00229.pdf)
- (arXiv 2021.07) UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2107.00781.pdf)
- (arXiv 2021.07) COVID-VIT: Classification of Covid-19 from CT chest images based on vision transformer models, [[Paper]](https://arxiv.org/pdf/2107.01682.pdf)
- (arXiv 2021.07) RATCHET: Medical Transformer for Chest X-ray Diagnosis and Reporting, [[Paper]](https://arxiv.org/pdf/2107.01682.pdf), [[Code]](http://www.github.com/farrell236/RATCHET)
- (arXiv 2021.07) Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation, [[Paper]](https://arxiv.org/pdf/2107.02655.pdf)
- (arXiv 2021.07) Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries, [[Paper]](https://arxiv.org/pdf/2107.03035.pdf)
- (arXiv 2021.07) EEG-ConvTransformer for Single-Trial EEG based Visual Stimuli Classification, [[Paper]](https://arxiv.org/pdf/2107.03983.pdf)
- (arXiv 2021.07) Visual Transformer with Statistical Test for COVID-19 Classification, [[Paper]](https://arxiv.org/pdf/2107.05334.pdf)
- (arXiv 2021.07) TransAttUnet: Multi-level Attention-guided U-Net with Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2107.05274.pdf)
- (arXiv 2021.07) Few-Shot Domain Adaptation with Polymorphic Transformers, [[Paper]](https://arxiv.org/pdf/2107.04805.pdf), [[Code]](https://github.com/askerlee/segtran)
- (arXiv 2021.07) TransClaw U-Net: Claw U-Net with Transformers for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2107.05188.pdf)
- (arXiv 2021.07) Surgical Instruction Generation with Transformers, [[Paper]](https://arxiv.org/pdf/2107.06964.pdf)
- (arXiv 2021.07) LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2107.08623.pdf), [[Code]](https://github.com/apple1986/LeViT_UNet)
- (arXiv 2021.07) TEDS-Net: Enforcing Diffeomorphisms in Spatial Transformers to Guarantee Topology Preservation in Segmentations, [[Paper]](https://arxiv.org/pdf/2107.13542.pdf), [[Code]](https://www.github.com/mwyburd/TEDS-Net)
- (arXiv 2021.08) Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.06932.pdf), [[Code]](https://github.com/DengPingFan/Polyp-PVT)
- (arXiv 2021.08) Is it Time to Replace CNNs with Transformers for Medical Images, [[Paper]](https://arxiv.org/pdf/2108.09038.pdf), [[Code]](https://github.com/ChrisMats/medical_transformers)
- (arXiv 2021.09) nnFormer: Interleaved Transformer for Volumetric Segmentation, [[Paper]](https://arxiv.org/pdf/2109.03201.pdf), [[Code]](https://github.com/282857341/nnFormer)
- (arXiv 2021.09) UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer, [[Paper]](https://arxiv.org/pdf/2109.04335.pdf), [[Code]](https://github.com/McGregorWwww/UCTransNet)
- (arXiv 2021.09) MISSFormer: An Effective Medical Image Segmentation Transformer, [[Paper]](https://arxiv.org/pdf/2109.07162.pdf)
- (arXiv 2021.09) Eformer: Edge Enhancement based Transformer for Medical Image Denoising, [[Paper]](https://arxiv.org/pdf/2109.08044.pdf)
- (arXiv 2021.09) Transformer-Unet: Raw Image Processing with Unet, [[Paper]](https://arxiv.org/pdf/2109.08417.pdf)
- (arXiv 2021.09) BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2109.12271.pdf)
- (arXiv 2021.09) GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation, [[Paper]](https://arxiv.org/pdf/2109.14813.pdf)
- (arXiv 2021.10) Transformer Assisted Convolutional Network for Cell Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2110.02270.pdf)
- (arXiv 2021.10) A transformer-based deep learning approach for classifying brain metastases into primary organ sites using clinical whole brain MRI images, [[Paper]](https://arxiv.org/pdf/2110.03588.pdf)
- (arXiv 2021.10) Boundary-aware Transformers for Skin Lesion Segmentation, [[Paper]](https://arxiv.org/pdf/2110.03864.pdf), [[Code]](https://github.com/jcwang123/BA-Transformer)
- (arXiv 2021.10) Vision Transformer based COVID-19 Detection using Chest X-rays, [[Paper]](https://arxiv.org/pdf/2110.04458.pdf)
- (arXiv 2021.10) Combining CNNs With Transformer for Multimodal 3D MRI Brain Tumor Segmentation With Self-Supervised Pretraining, [[Paper]](https://arxiv.org/pdf/2110.07919.pdf), [[Code]](https://github.com/ucuapps/BraTS2021_Challenge)
- (arXiv 2021.10) CAE-Transformer: Transformer-based Model to Predict Invasiveness of Lung Adenocarcinoma Subsolid Nodules from Non-thin Section 3D CT Scans, [[Paper]](https://arxiv.org/pdf/2110.08721.pdf), [[Code]](https://github.com/ucuapps/BraTS2021_Challenge)
- (arXiv 2021.10) COVID-19 Detection in Chest X-ray Images Using Swin-Transformer and Transformer in Transformer, [[Paper]](https://arxiv.org/pdf/2110.08427.pdf), [[Code]](https://github.com/ucuapps/BraTS2021_Challenge)
- (arXiv 2021.10) Bilateral-ViT for Robust Fovea Localization, [[Paper]](https://arxiv.org/pdf/2110.09860.pdf)
- (arXiv 2021.10) AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2110.10403.pdf)
- (arXiv 2021.10) Vision Transformer for Classification of Breast Ultrasound Images, [[Paper]](https://arxiv.org/pdf/2110.14731.pdf)
- (arXiv 2021.11) Federated Split Vision Transformer for COVID-19CXR Diagnosis using Task-Agnostic Training, [[Paper]](https://arxiv.org/pdf/2111.01338.pdf)
- (arXiv 2021.11) Hepatic vessel segmentation based on 3D swin-transformer with inductive biased multi-head self-attention, [[Paper]](https://arxiv.org/pdf/2111.03368.pdf)
- (arXiv 2021.11) Lymph Node Detection in T2 MRI with Transformers, [[Paper]](https://arxiv.org/pdf/2111.04885.pdf)
- (arXiv 2021.11) Mixed Transformer U-Net For Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2111.04734.pdf), [[Code]](https://github.com/Dootmaan/MT-UNet)
- (arXiv 2021.11) Transformer for Polyp Detection, [[Paper]](https://arxiv.org/pdf/2111.07918.pdf)
- (arXiv 2021.11) DuDoTrans: Dual-Domain Transformer Provides More Attention for Sinogram Restoration in Sparse-View CT Reconstruction, [[Paper]](https://arxiv.org/pdf/2111.10790.pdf), [[Code]](https://github.com/DuDoTrans/CODE)
- (arXiv 2021.11) A Volumetric Transformer for Accurate 3D Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2111.13300.pdf), [[Code]](https://github.com/himashi92/vt-unet)
- (arXiv 2021.11) Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis, [[Paper]](https://arxiv.org/pdf/2111.14791.pdf), [[Code]](https://github.com/Project-MONAI/research-contributions/tree/master/SwinUNETR)
- (arXiv 2021.11) MIST-net: Multi-domain Integrative Swin Transformer network for Sparse-View CT Reconstruction, [[Paper]](https://arxiv.org/pdf/2111.14831.pdf)
- (arXiv 2021.12) MT-TransUNet: Mediating Multi-Task Tokens in Transformers for Skin Lesion Segmentation and Classification, [[Paper]](https://arxiv.org/pdf/2112.01767.pdf), [[Code]](https://github.com/jingyechen/mt-transunet)
- (arXiv 2021.12) 3D Medical Point Transformer: Introducing Convolution to Attention Networks for Medical Point Cloud Analysis, [[Paper]](https://arxiv.org/pdf/2112.04863.pdf), [[Code]](https://github.com/crane-papercode/3DMedPT)
- (arXiv 2021.12) Semi-Supervised Medical Image Segmentation via Cross Teaching between CNN and Transformer, [[Paper]](https://arxiv.org/pdf/2112.04894.pdf), [[Code]](https://github.com/HiLab-git/SSL4MIS)
- (arXiv 2021.12) Pre-training and Fine-tuning Transformers for fMRI Prediction Tasks, [[Paper]](https://arxiv.org/pdf/2112.05761.pdf), [[Code]](https://github.com/GonyRosenman/TFF)
- (arXiv 2021.12) MSHT: Multi-stage Hybrid Transformer for the ROSE Image Analysis of Pancreatic Cancer, [[Paper]](https://arxiv.org/pdf/2112.13513.pdf), [[Code]](https://github.com/sagizty/Multi-Stage-Hybrid-Transformer)
- (arXiv 2022.01) D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2201.00462.pdf)
- (arXiv 2022.01) Swin UNETR: Swin Transformers for ation of Brain Tumors in MRI Images, [[Paper]](https://arxiv.org/pdf/2201.01266.pdf), [[Code]](https://github.com/Project-MONAI/research-contributions/tree/master/SwinUNETR)
- (arXiv 2022.01) Swin Transformer for Fast MRI, [[Paper]](https://arxiv.org/pdf/2201.01266.pdf), [[Code]](https://github.com/ayanglab/SwinMR)
- (arXiv 2022.01) ViTBIS: Vision Transformer for Biomedical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2201.05920.pdf)
- (arXiv 2022.01) Improving Across-Dataset Brain Tissue Segmentation Using Transformer, [[Paper]](https://arxiv.org/pdf/2201.08741.pdf), [[Code]](https://github.com/raovish6/TABS)
- (arXiv 2022.01) SegTransVAE: Hybrid CNN -- Transformer with Regularization for medical image segmentation, [[Paper]](https://arxiv.org/pdf/2201.08582.pdf), [[Code]](https://github.com/itruonghai/SegTransVAE)
- (arXiv 2022.01) ReconFormer: Accelerated MRI Reconstruction Using Recurrent Transformer, [[Paper]](https://arxiv.org/pdf/2201.09376.pdf), [[Code]](https://github.com/guopengf/ReconFormer)
- (arXiv 2022.01) Fast MRI Reconstruction: How Powerful Transformers Are, [[Paper]](https://arxiv.org/pdf/2201.09400.pdf)
- (arXiv 2022.01) Class-Aware Generative Adversarial Transformers for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2201.10737.pdf)
- (arXiv 2022.01) RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation, [[Paper]](https://arxiv.org/pdf/2201.11037.pdf)
- (arXiv 2022.01) Joint Liver and Hepatic Lesion Segmentation using a Hybrid CNN with Transformer Layers, [[Paper]](https://arxiv.org/pdf/2201.10981.pdf)
- (arXiv 2022.01) DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction, [[Paper]](https://arxiv.org/pdf/2201.10981.pdf)
- (arXiv 2022.01) TransPPG: Two-stream Transformer for Remote Heart Rate Estimate, [[Paper]](https://arxiv.org/pdf/2201.10873.pdf)
- (arXiv 2022.01) TransBTSV2: Wider Instead of Deeper Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2201.12785.pdf), [[Code]](https://github.com/Wenxuan-1119/TransBTS)
- (arXiv 2022.01) Brain Cancer Survival Prediction on Treatment-na ive MRI using Deep Anchor Attention Learning with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2202.01857.pdf)
- (arXiv 2022.02) Indication as Prior Knowledge for Multimodal Disease Classification in Chest Radiographs with Transformers, [[Paper]](https://arxiv.org/pdf/2202.06076.pdf), [[Code]](https://github.com/jacenkow/mmbt)
- (arXiv 2022.02) AI can evolve without labels: self-evolving vision transformer for chest X-ray diagnosis through knowledge distillation, [[Paper]](https://arxiv.org/pdf/2202.06431.pdf)
- (arXiv 2022.02) ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification, [[Paper]](https://arxiv.org/pdf/2202.07570.pdf)
- (arXiv 2022.02) A hybrid 2-stage vision transformer for AI-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies, [[Paper]](https://arxiv.org/pdf/2202.08510.pdf)
- (arXiv 2022.02) TraSeTR: Track-to-Segment Transformer with Contrastive Query for Instance-level Instrument Segmentation in Robotic Surgery, [[Paper]](https://arxiv.org/pdf/2202.08453.pdf)
- (arXiv 2022.02) RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-guided Disease Classification, [[Paper]](https://arxiv.org/pdf/2202.11781.pdf)
- (arXiv 2022.03) Using Multi-scale SwinTransformer-HTC with Data augmentation in CoNIC Challenge, [[Paper]](https://arxiv.org/pdf/2202.13588.pdf)
- (arXiv 2022.03) CTformer: Convolution-free Token2Token Dilated Vision Transformer for Low-dose CT Denoising, [[Paper]](https://arxiv.org/pdf/2202.13517.pdf), [[Code]](https://github.com/wdayang/CTformer)
- (arXiv 2022.03) Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology, [[Paper]](https://arxiv.org/pdf/2203.00585.pdf), [[Code]](https://github.com/Richarizardd/Self-Supervised-ViT-Path)
- (arXiv 2022.03) A Multi-scale Transformer for Medical Image Segmentation: Architectures, Model Efficiency, and Benchmarks, [[Paper]](https://arxiv.org/pdf/2203.00131.pdf), [[Code]](https://github.com/yhygao/CBIM-Medical-Image-Segmentation)
- (arXiv 2022.03) Tempera: Spatial Transformer Feature Pyramid Network for Cardiac MRI Segmentation, [[Paper]](https://arxiv.org/pdf/2203.00355.pdf)
- (arXiv 2022.03) Contextual Attention Network: Transformer Meets U-Net, [[Paper]](https://arxiv.org/pdf/2203.01932.pdf), [[Code]](https://github.com/rezazad68/TMUnet)
- (arXiv 2022.03) Characterizing Renal Structures with 3D Block Aggregate Transformers, [[Paper]](https://arxiv.org/pdf/2203.02430.pdf)
- (arXiv 2022.03) Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification, [[Paper]](https://arxiv.org/pdf/2203.04614.pdf)
- (arXiv 2022.03) Active Phase-Encode Selection for Slice-Specific Fast MR Scanning Using a Transformer-Based Deep Reinforcement Learning Framework, [[Paper]](https://arxiv.org/pdf/2203.05756.pdf)
- (arXiv 2022.03) Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4, [[Paper]](https://arxiv.org/pdf/2203.06649.pdf)
- (arXiv 2022.03) SATr: Slice Attention with Transformer for Universal Lesion Detection, [[Paper]](https://arxiv.org/pdf/2203.07373.pdf)
- (arXiv 2022.03) Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images, [[Paper]](https://arxiv.org/pdf/2203.09207.pdf)
- (arXiv 2022.03) TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2203.10726.pdf)
- (arXiv 2022.03) Adaptively Re-weighting Multi-Loss Untrained Transformer for Sparse-View Cone-Beam CT Reconstruction, [[Paper]](https://arxiv.org/pdf/2203.12476.pdf)
- (arXiv 2022.03) Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection, [[Paper]](https://arxiv.org/pdf/2203.12121.pdf)
- (arXiv 2022.03) Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution, [[Paper]](https://arxiv.org/pdf/2203.13963.pdf), [[Code]](https://github.com/XAIMI-Lab/McMRSR)
- (arXiv 2022.03) Cross-Modality High-Frequency Transformer for MR Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2203.15314.pdf)
- (arXiv 2022.03) CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI, [[Paper]](https://arxiv.org/pdf/2203.15163.pdf)
- (arXiv 2022.04) UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2204.00631.pdf), [[Code]](https://github.com/Project-MONAI/research-contributions)
- (arXiv 2022.04) Data and Physics Driven Learning Models for Fast MRI -- Fundamentals and Methodologies from CNN, GAN to Attention and Transformers, [[Paper]](https://arxiv.org/pdf/2204.01706.pdf)
- (arXiv 2022.04) CCAT-NET: A Novel Transformer Based Semi-supervised Framework for Covid-19 Lung Lesion Segmentation, [[Paper]](https://arxiv.org/pdf/2204.02839.pdf)
- (arXiv 2022.04) Surface Vision Transformers: Flexible Attention-Based Modelling of Biomedical Surfaces, [[Paper]](https://arxiv.org/pdf/2204.03408.pdf), [[Code]](https://github.com/metrics-lab/surface-vision-transformers)
- (arXiv 2022.04) Low-Dose CT Denoising via Sinogram Inner-Structure Transformer, [[Paper]](https://arxiv.org/pdf/2204.03163.pdf)
- (arXiv 2022.04) 3D Shuffle-Mixer: An Efficient Context-Aware Vision Learner of Transformer-MLP Paradigm for Dense Prediction in Medical Volume, [[Paper]](https://arxiv.org/pdf/2204.06779.pdf)
- (arXiv 2022.04) Continual Hippocampus Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2204.08043.pdf)
- (arXiv 2022.04) TranSiam: Fusing Multimodal Visual Features Using Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2204.12185.pdf)
- (arXiv 2022.05) Noise-reducing attention cross fusion learning transformer for histological image classification of osteosarcoma, [[Paper]](https://arxiv.org/pdf/2204.13838.pdf)
- (arXiv 2022.05) One Model to Synthesize Them All: Multi-contrast Multi-scale Transformer for Missing Data Imputation, [[Paper]](https://arxiv.org/pdf/2204.13738.pdf)
- (arXiv 2022.05) Unsupervised Contrastive Learning based Transformer for Lung Nodule Detection, [[Paper]](https://arxiv.org/pdf/2205.00122.pdf)
- (arXiv 2022.05) Understanding Transfer Learning for Chest Radiograph Clinical Report Generation with Modified Transformer Architectures, [[Paper]](https://arxiv.org/pdf/2205.02841.pdf)
- (arXiv 2022.05) Masked Co-attentional Transformer reconstructs 100x ultra-fast/low-dose whole-body PET from longitudinal images and anatomically guided MRI, [[Paper]](https://arxiv.org/pdf/2205.04044.pdf)
- (arXiv 2022.05) Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction, [[Paper]](https://arxiv.org/pdf/2205.06672.pdf)
- (arXiv 2022.05) A microstructure estimation Transformer inspired by sparse representation for diffusion MRI, [[Paper]](https://arxiv.org/pdf/2205.06450.pdf)
- (arXiv 2022.05) An Effective Transformer-based Solution for RSNA Intracranial Hemorrhage Detection Competition, [[Paper]](https://arxiv.org/pdf/2205.07556.pdf),[[Code]](https://aistudio.baidu.com/aistudio/projectdetail/3995861)
- (arXiv 2022.05) HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images, [[Paper]](https://arxiv.org/pdf/2205.08390.pdf)
- (arXiv 2022.05) ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation, [[Paper]](https://arxiv.org/pdf/2205.08473.pdf)
- (arXiv 2022.05) Transformer based multiple instance learning for weakly supervised histopathology image segmentation, [[Paper]](https://arxiv.org/pdf/2205.08878.pdf)
- (arXiv 2022.05) A graph-transformer for whole slide image classification, [[Paper]](https://arxiv.org/pdf/2205.09671.pdf)
- (arXiv 2022.05) BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video, [[Paper]](https://arxiv.org/pdf/2205.09382.pdf),[[Code]](https://github.com/SanoScience/BabyNet)
- (arXiv 2022.05) Transformer based Generative Adversarial Network for Liver Segmentation, [[Paper]](https://arxiv.org/pdf/2205.10663.pdf)
- (arXiv 2022.05) A Comparative Study of Gastric Histopathology Sub-size Image Classification: from Linear Regression to Visual Transformer, [[Paper]](https://arxiv.org/pdf/2205.12843.pdf),[[Code]](https://github.com/SanoScience/BabyNet)
- (arXiv 2022.05) Zero-Shot and Few-Shot Learning for Lung Cancer Multi-Label Classification using Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.15290.pdf)
- (arXiv 2022.06) The Fully Convolutional Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2206.00566.pdf),[[Code]](https://github.com/Thanos-DB/FullyConvolutionalTransformer)
- (arXiv 2022.06) CellCentroidFormer: Combining Self-attention and Convolution for Cell Detection, [[Paper]](https://arxiv.org/pdf/2206.00338.pdf),[[Code]](https://github.com/roydenwa/cell-centroid-former)
- (arXiv 2022.06) Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives, [[Paper]](https://arxiv.org/pdf/2206.01136.pdf)
- (arXiv 2022.06) CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron, [[Paper]](https://arxiv.org/pdf/2206.00971.pdf)
- (arXiv 2022.06) MISSU: 3D Medical Image Segmentation via Self-distilling TransUNet, [[Paper]](https://arxiv.org/pdf/2206.00902.pdf),[[Code]](https://github.com/wangn123/MISSU.git)
- (arXiv 2022.06) mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2206.02425.pdf),[[Code]](https://github.com/YaoZhang93/mmFormer)
- (arXiv 2022.06) Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2206.01741.pdf)
- (arXiv 2022.06) Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation, [[Paper]](https://arxiv.org/pdf/2206.01988.pdf)
- (arXiv 2022.06) Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans, [[Paper]](https://arxiv.org/pdf/2206.03049.pdf),[[Code]](https://github.com/liaw05/STMixer)
- (arXiv 2022.06) Transformer-based Personalized Attention Mechanism (PersAM) for Medical Images with Clinical Records, [[Paper]](https://arxiv.org/pdf/2206.03003.pdf)
- (arXiv 2022.06) SwinCheX: Multi-label classification on chest X-ray images with transformers, [[Paper]](https://arxiv.org/pdf/2206.04246.pdf)
- (arXiv 2022.06) RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans, [[Paper]](https://arxiv.org/pdf/2206.06253.pdf),[[Code]](https://github.com/smilenaxx/RPLHR-CT)
- (arXiv 2022.06) Transformer Lesion Tracker, [[Paper]](https://arxiv.org/pdf/2206.06252.pdf),[[Code]](https://github.com/TangWen920812/TLT)
- (arXiv 2022.06) SeATrans: Learning Segmentation-Assisted diagnosis model via Transforme, [[Paper]](https://arxiv.org/pdf/2206.05763.pdf)
- (arXiv 2022.06) K-Space Transformer for Fast MRIReconstruction with Implicit Representation, [[Paper]](https://arxiv.org/pdf/2206.06947.pdf),[[Code]](https://zhaoziheng.github.io/Website/K-Space-Transformer)
- (arXiv 2022.06) XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention, [[Paper]](https://arxiv.org/pdf/2206.07349.pdf),[[Code]](https://github.com/Solemoon/XMorpher)
- (arXiv 2022.06) A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects, [[Paper]](https://arxiv.org/pdf/2206.07219.pdf)
- (arXiv 2022.06) Rectify ViT Shortcut Learning by Visual Saliency, [[Paper]](https://arxiv.org/pdf/2206.08567.pdf)
- (arXiv 2022.06) Neural Transformers for Intraductal Papillary Mucosal Neoplasms (IPMN) Classification in MRI images, [[Paper]](https://arxiv.org/pdf/2206.10531.pdf)
- (arXiv 2022.06) Toward Unpaired Multi-modal Medical Image Segmentation via Learning Structured Semantic Consistency, [[Paper]](https://arxiv.org/pdf/2206.10571.pdf),[[Code]](https://github.com/YangJie18/LSSC)
- (arXiv 2022.06) TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation, [[Paper]](https://arxiv.org/pdf/2206.08985.pdf),[[Code]](https://github.com/nikhilroxtomar/TransResUNet)
- (arXiv 2022.06) SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI, [[Paper]](https://arxiv.org/pdf/2206.10802.pdf),[[Code]](https://github.com/daviddmc/SVoRT)
- (arXiv 2022.06) ICOS Protein Expression Segmentation: Can Transformer Networks Give Better Results, [[Paper]](https://arxiv.org/pdf/2206.11520.pdf)
- (arXiv 2022.06) Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification, [[Paper]](https://arxiv.org/pdf/2206.13156.pdf),[[Code]](https://github.com/zhengyushan/kat)
- (arXiv 2022.06) Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading, [[Paper]](https://arxiv.org/pdf/2206.13173.pdf)
- (arXiv 2022.06) The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning, [[Paper]](https://arxiv.org/pdf/2206.14413.pdf),[[Code]](https://github.com/xianlin7/APFormer)
- (arXiv 2022.06) C2FTrans: Coarse-to-Fine Transformers for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2206.14409.pdf),[[Code]](https://github.com/xianlin7/C2FTrans)
- (arXiv 2022.06) LViT: Language meets Vision Transformer in Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2206.14718.pdf),[[Code]](https://github.com/HUANGLIZI/LViT)
- (arXiv 2022.06) PVT-COV19D: Pyramid Vision Transformer for COVID-19 Diagnosis, [[Paper]](https://arxiv.org/pdf/2206.15069.pdf)
- (arXiv 2022.07) Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches, [[Paper]](https://arxiv.org/pdf/2207.00113.pdf),[[Code]](https://github.com/XuMengyaAmy/SwinMLP_TranCAP)
- (arXiv 2022.07) Efficient Lung Cancer Image Classification and Segmentation Algorithm Based on Improved Swin Transformer, [[Paper]](https://arxiv.org/pdf/2207.01527.pdf)
- (arXiv 2022.07) Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans, [[Paper]](https://arxiv.org/pdf/2207.01579.pdf)
- (arXiv 2022.07) Adaptive GLCM sampling for transformer-based COVID-19 detection on CT, [[Paper]](https://arxiv.org/pdf/2207.01520.pdf)
- (arXiv 2022.07) CNN-based Local Vision Transformer for COVID-19 Diagnosis, [[Paper]](https://arxiv.org/pdf/2207.02027.pdf)
- (arXiv 2022.07) Transformer based Models for Unsupervised Anomaly Segmentation in Brain MR Images, [[Paper]](https://arxiv.org/pdf/2207.02059.pdf),[[Code]](https://github.com/ahmedgh970/Transformers_Unsupervised_Anomaly_Segmentation.git)
- (arXiv 2022.07) CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis, [[Paper]](https://arxiv.org/pdf/2207.02091.pdf)
- (arXiv 2022.07) Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI, [[Paper]](https://arxiv.org/pdf/2207.02390.pdf),[[Code]](https://github.com/ayanglab/SDAUT)
- (arXiv 2022.07) Multi-Label Retinal Disease Classification using Transformers, [[Paper]](https://arxiv.org/pdf/2207.02335.pdf),[[Code]](https://github.com/manuel-rdz/C-Tran),[[Dataset]](https://bit.ly/3utt9ZD)
- (arXiv 2022.07) TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.02327.pdf)
- (arXiv 2022.07) Learning Apparent Diffusion Coefficient Maps from Undersampled Radial k-Space Diffusion-Weighted MRI in Mice using a Deep CNN-Transformer Model in Conjunction with a Monoexponential Model, [[Paper]](https://arxiv.org/pdf/2207.02399.pdf)
- (arXiv 2022.07) TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2207.03450.pdf),[[Code]](https://github.com/HUANGLIZI/TFCNs)
- (arXiv 2022.07) Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays, [[Paper]](https://arxiv.org/pdf/2207.04394.pdf)
- (arXiv 2022.07) RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2207.06177.pdf)
- (arXiv 2022.07) CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer with Modality-Correlated Cross-Attention for Brain Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2207.07370.pdf)
- (arXiv 2022.07) Mobile Keystroke Biometrics Using Transformers, [[Paper]](https://arxiv.org/pdf/2207.07596.pdf)
- (arXiv 2022.07) Multi-head Cascaded Swin Transformers with Attention to k-space Sampling Pattern for Accelerated MRI Reconstruction, [[Paper]](https://arxiv.org/pdf/2207.08412.pdf)
- (arXiv 2022.07) HiFormer: Hierarchical Multi-scale Representations Using Transformers for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2207.08518.pdf),[[Code]](https://github.com/amirhossein-kz/HiFormer)
- (arXiv 2022.07) Focused Decoding Enables 3D Anatomical Detection by Transformers, [[Paper]](https://arxiv.org/pdf/2207.10774.pdf),[[Code]](https://github.com/bwittmann/transoar)
- (arXiv 2022.07) High-Resolution Swin Transformer for Automatic Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2207.11553.pdf),[[Code]](https://github.com/auroua/HRSTNet)
- (arXiv 2022.07) Improved Super Resolution of MR Images Using CNNs and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.11748.pdf),[[Code]](https://github.com/auroua/HRSTNet)
- (arXiv 2022.07) TransNorm: Transformer Provides a Strong Spatial Normalization Mechanism for a Deep Segmentation Model, [[Paper]](https://arxiv.org/pdf/2207.13415.pdf),[[Code]](https://github.com/rezazad68/transnorm)
- (arXiv 2022.07) ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise Perspective for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2207.14552.pdf),[[Code]](https://github.com/ZJUGiveLab/ScaleFormer)
- (arXiv 2022.08) TransDeepLab: Convolution-Free Transformer-based DeepLab v3+ for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2208.00713.pdf),[[Code]](https://github.com/rezazad68/transdeeplab)
- (arXiv 2022.08) Multi-Feature Vision Transformer via Self-Supervised Representation Learning for Improvement of COVID-19 Diagnosis, [[Paper]](https://arxiv.org/pdf/2208.01843.pdf),[[Code]](https://github.com/endiqq/Multi-Feature-ViT)
- (arXiv 2022.08) Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification, [[Paper]](https://arxiv.org/pdf/2208.02851.pdf),[[Code]](https://github.com/faresmalik/SEViT)
- (arXiv 2022.08) BrainFormer: A Hybrid CNN-Transformer Model for Brain fMRI Data Classification, [[Paper]](https://arxiv.org/pdf/2208.03028.pdf),[[Code]](https://github.com/ZiyaoZhangforPCL/BrainFormer)
- (arXiv 2022.08) U-Net vs Transformer: Is U-Net Outdated in Medical Image Registration, [[Paper]](https://arxiv.org/pdf/2208.04939.pdf),[[Code]](https://github.com/xi-jia/LKU-Net)
- (arXiv 2022.08) Shifted Windows Transformers for Medical Image Quality Assessment, [[Paper]](https://arxiv.org/pdf/2208.06034.pdf),[[Code]](https://github.com/canerozer/qct)
- (arXiv 2022.08) Shuffle Instances-based Vision Transformer for Pancreatic Cancer ROSE Image Classification, [[Paper]](https://arxiv.org/pdf/2208.06833.pdf),[[Code]](https://github.com/sagizty/MIL-SI)
- (arXiv 2022.08) When CNN Meet with ViT: Towards Semi-Supervised Learning for Multi-Class Medical Image ation, [[Paper]](https://arxiv.org/pdf/2208.06449.pdf), [[Code]](https://github.com/ziyangwang007/CV-SSL-MIS)
- (arXiv 2022.08) Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2208.08315.pdf), [[Code]](https://github.com/SimonZeng7108/Video-TransUNet)
- (arXiv 2022.08) FCN-Transformer Feature Fusion for Polyp Segmentation, [[Paper]](https://arxiv.org/pdf/2208.08352.pdf), [[Code]](https://github.com/CVML-UCLan/FCBFormer)
- (arXiv 2022.08) A Medical Semantic-Assisted Transformer for Radiographic Report Generation, [[Paper]](https://arxiv.org/pdf/2208.10358.pdf), [[Code]](https://github.com/CVML-UCLan/FCBFormer)
- (arXiv 2022.08) Multiple Instance Neuroimage Transformer, [[Paper]](https://arxiv.org/pdf/2208.09567.pdf), [[Code]](https://github.com/singlaayush/MINIT)
- (arXiv 2022.08) Cats: Complementary CNN and Transformer Encoders for Segmentation, [[Paper]](https://arxiv.org/pdf/2208.11572.pdf)
- (arXiv 2022.08) Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2208.13113.pdf)
- (arXiv 2022.08) SB-SSL: Slice-Based Self-Supervised Transformers for Knee Abnormality Classification from MRI, [[Paper]](https://arxiv.org/pdf/2208.13923.pdf)
- (arXiv 2022.08) NestedFormer: Nested Modality-Aware Transformer for Brain Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2208.14876.pdf), [[Code]](https://github.com/920232796/NestedFormer)
- (arXiv 2022.08) ARST: Auto-Regressive Surgical Transformer for Phase Recognition from Laparoscopic Videos, [[Paper]](https://arxiv.org/pdf/2209.01148.pdf)
- (arXiv 2022.09) Time-distance vision transformers in lung cancer diagnosis from longitudinal computed tomography, [[Paper]](https://arxiv.org/pdf/2209.01676.pdf), [[Code]](https://github.com/tom1193/time-distance-transformer)
- (arXiv 2022.09) Masked Sinogram Model with Transformer for ill-Posed Computed Tomography Reconstruction: a Preliminary Study, [[Paper]](https://arxiv.org/pdf/2209.01356.pdf), [[Code]](https://github.com/lzhengchun/TomoTx)
- (arXiv 2022.09) Spach Transformer: Spatial and Channel-wise Transformer Based on Local and Global Self-attentions for PET Image Denoising, [[Paper]](https://arxiv.org/pdf/2209.03300.pdf)
- (arXiv 2022.09) View-Disentangled Transformer for Brain Lesion Detection, [[Paper]](https://arxiv.org/pdf/2209.09657.pdf), [[Code]](https://github.com/lhaof/ISBI-VDFormer)
- (arXiv 2022.09) CCTCOVID: COVID-19 Detection from Chest X-Ray Images Using Compact Convolutional Transformers, [[Paper]](https://arxiv.org/pdf/2209.13399.pdf)
- (arXiv 2022.09) Medical Image Captioning via Generative Pretrained Transformers, [[Paper]](https://arxiv.org/pdf/2209.13983.pdf)
- (arXiv 2022.09) UNesT: Local Spatial Representation Learning with Hierarchical Transformer for Efficient Medical Segmentation, [[Paper]](https://arxiv.org/pdf/2209.14378.pdf), [[Code]](https://github.com/Project-MONAI/model-zoo/tree/dev/models)
- (arXiv 2022.10) 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2209.15076.pdf), [[Code]](https://github.com/MASILab/3DUX-Net)
- (arXiv 2022.10) Gastrointestinal Disorder Detection with a Transformer Based Approach, [[Paper]](https://arxiv.org/pdf/2210.03168.pdf)
- (arXiv 2022.10) LAPFormer: A Light and Accurate Polyp Segmentation Transformer, [[Paper]](https://arxiv.org/pdf/2210.04393.pdf)
- (arXiv 2022.10) Memory transformers for full context and high-resolution 3D Medical Segmentation, [[Paper]](https://arxiv.org/pdf/2210.05313.pdf)
- (arXiv 2022.10) ConvTransSeg: A Multi-resolution Convolution-Transformer Network for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2210.07072.pdf)
- (arXiv 2022.10) Brain Network Transformer, [[Paper]](https://arxiv.org/pdf/2210.06681.pdf), [[Code]](https://github.com/Wayfear/BrainNetworkTransformer)
- (arXiv 2022.10) Wide Range MRI Artifact Removal with Transformers, [[Paper]](https://arxiv.org/pdf/2210.07976.pdf)
- (arXiv 2022.10) Optimizing Vision Transformers for Medical Image Segmentation and Few-Shot Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2210.08066.pdf)
- (arXiv 2022.10) SimpleClick: Interactive Image Segmentation with Simple Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.11006.pdf)
- (arXiv 2022.10) Adversarial Transformer for Repairing Human Airway Segmentation, [[Paper]](https://arxiv.org/pdf/2210.12029.pdf)
- (arXiv 2022.10) Clinically-Inspired Multi-Agent Transformers for Disease Trajectory Forecasting from Multimodal Data, [[Paper]](https://arxiv.org/pdf/2210.13889.pdf), [[Code]](https://github.com/Oulu-IMEDS/CLIMATv2)
- (arXiv 2022.10) Automatic Diagnosis of Myocarditis Disease in Cardiac MRI Modality using Deep Transformers and Explainable Artificial Intelligence, [[Paper]](https://arxiv.org/pdf/2210.14611.pdf)
- (arXiv 2022.10) Spatio-Temporal Hybrid Fusion of CAE and SWIn Transformers for Lung Cancer Malignancy Prediction, [[Paper]](https://arxiv.org/pdf/2210.15297.pdf)
- (arXiv 2022.10) Hyper-Connected Transformer Network for Co-Learning Multi-Modality PET-CT Features, [[Paper]](https://arxiv.org/pdf/2210.15808.pdf)
- (arXiv 2022.10) ImplantFormer: Vision Transformer based Implant Position Regression Using Dental CBCT Data, [[Paper]](https://arxiv.org/pdf/2210.16467.pdf)
- (arXiv 2022.10) Attention Swin U-Net: Cross-Contextual Attention Mechanism for Skin Lesion Segmentation, [[Paper]](https://arxiv.org/pdf/2210.16898.pdf), [[Code]](https://github.com/nitr098/attswinunet)
- (arXiv 2022.10) TFormer: 3D Tooth Segmentation in Mesh Scans with Geometry Guided Transformer, [[Paper]](https://arxiv.org/pdf/2210.16627.pdf), [[Code]](https://github.com/nitr098/attswinunet)
- (arXiv 2022.10) ViTASD: Robust Vision Transformer Baselines for Autism Spectrum Disorder Facial Diagnosis, [[Paper]](https://arxiv.org/pdf/2210.16943.pdf), [[Code]](https://github.com/irohxu/vitasd)
- (arXiv 2022.11) ViT-DeiT: An Ensemble Model for Breast Cancer Histopathological Images Classification, [[Paper]](https://arxiv.org/pdf/2211.00749.pdf)
- (arXiv 2022.11) RadFormer: Transformers with Global-Local Attention for Interpretable and Accurate Gallbladder Cancer Detection, [[Paper]](https://arxiv.org/pdf/2211.04793.pdf), [[Code]](https://github.com/sbasu276/RadFormer)
- (arXiv 2022.11) MultiCrossViT: Multimodal Vision Transformer for Schizophrenia Prediction using Structural MRI and Functional Network Connectivity Data, [[Paper]](https://arxiv.org/pdf/2211.06726.pdf)
- (arXiv 2022.11) ConvFormer: Combining CNN and Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2211.08564.pdf)
- (arXiv 2022.11) SWIN-SFTNet : Spatial Feature Expansion and Aggregation using Swin Transformer For Whole Breast micro-mass segmentation, [[Paper]](https://arxiv.org/pdf/2211.08717.pdf)
- (arXiv 2022.11) Parameter-Efficient Transformer with Hybrid Axial-Attention for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2211.09533.pdf)
- (arXiv 2022.11) TFormer: A throughout fusion transformer for multi-modal skin lesion diagnosis, [[Paper]](https://arxiv.org/pdf/2211.11393.pdf)
- (arXiv 2022.11) Unsupervised Echocardiography Registration through Patch-based MLPs and Transformers, [[Paper]](https://arxiv.org/pdf/2211.11687.pdf), [[Code]](https://gitlab.inria.fr/epione/mlp_transformer_registration)
- (arXiv 2022.11) Towards Automated Polyp Segmentation Using Weakly- and Semi-Supervised Learning and Deformable Transformers, [[Paper]](https://arxiv.org/pdf/2211.11847.pdf)
- (arXiv 2022.11) Cross-Field Transformer for Diabetic Retinopathy Grading on Two-field Fundus Images, [[Paper]](https://arxiv.org/pdf/2211.14552.pdf), [[Code]](https://github.com/FDU-VTS/DRTiD)
- (arXiv 2022.11) Hierarchical Transformer for Survival Prediction Using Multimodality Whole Slide Images and Genomics, [[Paper]](https://arxiv.org/pdf/2211.16632.pdf)
- (arXiv 2022.12) SLMT-Net: A Self-supervised Learning based Multi-scale Transformer Network for Cross-Modality MR Image Synthesis, [[Paper]](https://arxiv.org/pdf/2212.01108.pdf), [[Code]](https://github.com/lyhkevin/SLMT-Net)
- (arXiv 2022.12) CTT-Net: A Multi-view Cross-token Transformer for Cataract Postoperative Visual Acuity Prediction, [[Paper]](https://arxiv.org/pdf/2212.05794.pdf), [[Code]](https://github.com/wjh892521292/Cataract_OCT)
- (arXiv 2022.12) Two-stage Contextual Transformer-based Convolutional Neural Network for Airway Extraction from CT Images, [[Paper]](https://arxiv.org/pdf/2212.07651.pdf), [[Code]](https://github.com/zhaozsq/airway_segmentation)
- (arXiv 2022.12) Visual Transformers for Primates Classification and Covid Detection, [[Paper]](https://arxiv.org/pdf/2212.10093.pdf)
- (arXiv 2022.12) Conditioned Generative Transformers for Histopathology Image Synthetic Augmentation, [[Paper]](https://arxiv.org/pdf/2212.09977.pdf)
- (arXiv 2022.12) DuAT: Dual-Aggregation Transformer Network for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2212.11677.pdf)
- (arXiv 2022.12) Transformer and GAN Based Super-Resolution Reconstruction Network for Medical Images, [[Paper]](https://arxiv.org/pdf/2212.13068.pdf)
- (arXiv 2022.12) DAE-Former: Dual Attention-guided Efficient Transformer for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2212.13504.pdf), [[Code]](https://github.com/mindflow-institue/DAEFormer)
- (arXiv 2023.01) A New Perspective to Boost Vision Transformer for Medical Image Classification, [[Paper]](https://arxiv.org/pdf/2301.00989.pdf)
- (arXiv 2023.01) Detecting Severity of Diabetic Retinopathy from Fundus Images using Ensembled Transformers, [[Paper]](https://arxiv.org/pdf/2301.00973.pdf)
- (arXiv 2023.01) MS-DINO: Efficient Distributed Training of Vision Transformer Foundation Model in Medical Domain through Masked Sampling, [[Paper]](https://arxiv.org/pdf/2301.02064.pdf)
- (arXiv 2023.01) Cooperation Learning Enhanced Colonic Polyp Segmentation Based on TransformerCNN Fusion, [[Paper]](https://arxiv.org/pdf/2301.06892.pdf)
- (arXiv 2023.01) ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised Medical Image Representations, [[Paper]](https://arxiv.org/pdf/2301.07382.pdf)
- (arXiv 2023.01) Fully transformer-based biomarker prediction from colorectal cancer histology: a large-scale multicentric study, [[Paper]](https://arxiv.org/pdf/2301.09617.pdf)
- (arXiv 2023.01) MultiNet with Transformers: A Model for Cancer Diagnosis Using Images, [[Paper]](https://arxiv.org/pdf/2301.09007.pdf)
- (arXiv 2023.01) TranSOP: Transformer-based Multimodal Classification for Stroke Treatment Outcome Prediction, [[Paper]](https://arxiv.org/pdf/2301.10829.pdf)
- (arXiv 2023.01) MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer, [[Paper]](https://arxiv.org/pdf/2301.11798.pdf), [[Code]](https://github.com/WuJunde/MedSegDiff)
- (arXiv 2023.01) Enhancing Medical Image Segmentation with TransCeption: A Multi-Scale Feature Fusion Approach, [[Paper]](https://arxiv.org/pdf/2301.10847.pdf), [[Code]](https://github.com/mindflow-institue/TransCeption)
- (arXiv 2023.02) Efficient Scopeformer: Towards Scalable and Rich Feature Extraction for Intracranial Hemorrhage Detection, [[Paper]](https://arxiv.org/pdf/2302.00220.pdf)
- (arXiv 2023.02) LesionAid: Vision Transformers-based Skin Lesion Generation and Classification, [[Paper]](https://arxiv.org/pdf/2302.01104.pdf)
- (arXiv 2023.02) FCB-SwinV2 Transformer for Polyp Segmentation, [[Paper]](https://arxiv.org/pdf/2302.01027.pdf)
- (arXiv 2023.02) Longformer: Longitudinal Transformer for Alzheimer's Disease Classification with Structural MRIs, [[Paper]](https://arxiv.org/pdf/2302.00901.pdf), [[Code]](https://github.com/Qybc/LongFormer)
- (arXiv 2023.02) SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images, [[Paper]](https://arxiv.org/pdf/2302.03861.pdf)
- (arXiv 2023.02) Adapting Pre-trained Vision Transformers from 2D to 3D through Weight Inflation Improves Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2302.04303.pdf),[[Code]](https://github.com/yuhui-zh15/TransSeg)
- (arXiv 2023.02) Bilateral-Fuser: A Novel Multi-cue Fusion Architecture with Anatomical-aware Tokens for Fovea Localization, [[Paper]](https://arxiv.org/pdf/2302.06961.pdf)
- (arXiv 2023.02) MedViT: A Robust Vision Transformer for Generalized Medical Image Classification, [[Paper]](https://arxiv.org/pdf/2302.09462.pdf)
- (arXiv 2023.02) SF2Former: Amyotrophic Lateral Sclerosis Identification From Multi-center MRI Data Using Spatial and Frequency Fusion Transformer,[[Paper]](https://arxiv.org/pdf/2302.10859.pdf)
- (arXiv 2023.02) Magnification Invariant Medical Image Analysis: A Comparison of Convolutional Networks, Vision Transformers, and Token Mixers, [[Paper]](https://arxiv.org/pdf/2302.11488.pdf)
- (arXiv 2023.02) A residual dense vision transformer for medical image super-resolution with segmentation-based perceptual loss fine-tuning, [[Paper]](https://arxiv.org/pdf/2302.11184.pdf)
- (arXiv 2023.02) StudyFormer : Attention-Based and Dynamic Multi View Classifier for X-ray images, [[Paper]](https://arxiv.org/pdf/2302.11840.pdf)
- (arXiv 2023.03) Meta-information-aware Dual-path Transformer for Differential Diagnosis of Multi-type Pancreatic Lesions in Multi-phase CT, [[Paper]](https://arxiv.org/pdf/2303.00942.pdf)
- (arXiv 2023.03) TRUSformer: Improving Prostate Cancer Detection from Micro-Ultrasound Using Attention and Self-Supervision, [[Paper]](https://arxiv.org/pdf/2303.02128.pdf),[[Code]](https://github.com/med-i-lab/TRUSFormer)
- (arXiv 2023.03) UT-Net: Combining U-Net and Transformer for Joint Optic Disc and Cup Segmentation and Glaucoma Detection, [[Paper]](https://arxiv.org/pdf/2303.04939.pdf)
- (arXiv 2023.03) Generalized Diffusion MRI Denoising and Super-Resolution using Swin Transformers, [[Paper]](https://arxiv.org/pdf/2303.05686.pdf),[[Code]](https://github.com/ucsfncl/dmri-swin)
- (arXiv 2023.03) Pretrained ViTs Yield Versatile Representations For Medical Images, [[Paper]](https://arxiv.org/pdf/2303.07034.pdf)
- (arXiv 2023.03) Deformable Cross-Attention Transformer for Medical Image Registration, [[Paper]](https://arxiv.org/pdf/2303.06179.pdf)
- (arXiv 2023.03) Endoscopy Classification Model Using Swin Transformer and Saliency Map, [[Paper]](https://arxiv.org/pdf/2303.06736.pdf)
- (arXiv 2023.03) TransNetR: Transformer-based Residual Network for Polyp Segmentation with Multi-Center Out-of-Distribution Testing, [[Paper]](https://arxiv.org/pdf/2303.07428.pdf),[[Code]](https://github.com/DebeshJha)
- (arXiv 2023.03) Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer's Disease Detection, [[Paper]](https://arxiv.org/pdf/2303.08216.pdf)
- (arXiv 2023.03) MATIS: Masked-Attention Transformers for Surgical Instrument Segmentation, [[Paper]](https://arxiv.org/pdf/2303.09514.pdf)
- (arXiv 2023.03) SwinVFTR: A Novel Volumetric Feature-learning Transformer for 3D OCT Fluid Segmentation, [[Paper]](https://arxiv.org/pdf/2303.09233.pdf)
- (arXiv 2023.03) MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2303.09975.pdf)
- (arXiv 2023.03) GNNFormer: A Graph-based Framework for Cytopathology Report Generation, [[Paper]](https://arxiv.org/pdf/2303.09956.pdf)
- (arXiv 2023.03) Shifted-Windows Transformers for the Detection of Cerebral Aneurysms in Microsurgery, [[Paper]](https://arxiv.org/pdf/2303.09648.pdf)
- (arXiv 2023.03) CerviFormer: A Pap-smear based cervical cancer classification method using cross attention and latent transformer, [[Paper]](https://arxiv.org/pdf/2303.10222.pdf)
- (arXiv 2023.03) Convolutions, Transformers, and their Ensembles for the Segmentation of Organs at Risk in Radiation Treatment of Cervical Cancer, [[Paper]](https://arxiv.org/pdf/2303.11501.pdf)
- (arXiv 2023.03) HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals, [[Paper]](https://arxiv.org/pdf/2303.11340.pdf)
- (arXiv 2023.03) 3D Mitochondria Instance Segmentation with Spatio-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2303.12073.pdf),[[Code]](https://github.com/OmkarThawakar/STT-UNET)
- (arXiv 2023.03) Vision Transformer-based Model for Severity Quantification of Lung Pneumonia Using Chest X-ray Images, [[Paper]](https://arxiv.org/pdf/2303.11935.pdf),[[Code]](https://github.com/bouthainas/ViTReg-IP)
- (arXiv 2023.03) Prior-RadGraphFormer: A Prior-Knowledge-Enhanced Transformer for Generating Radiology Graphs from X-Rays, [[Paper]](https://arxiv.org/pdf/2303.13818.pdf)
- (arXiv 2023.03) Few Shot Medical Image Segmentation with Cross Attention Transformer, [[Paper]](https://arxiv.org/pdf/2303.13867.pdf)
- (arXiv 2023.03) D-TrAttUnet: Dual-Decoder Transformer-Based Attention Unet Architecture for Binary and Multi-classes Covid-19 Infection Segmentation, [[Paper]](https://arxiv.org/pdf/2303.15576.pdf)
- (arXiv 2023.03) MoViT: Memorizing Vision Transformers for Medical Image Analysis, [[Paper]](https://arxiv.org/pdf/2303.15553.pdf)
- (arXiv 2023.03) Multi-scale Hierarchical Vision Transformer with Cascaded Attention Decoding for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2303.16892.pdf)
- (arXiv 2023.04) Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization, [[Paper]](https://arxiv.org/pdf/2304.00212.pdf)
- (arXiv 2023.04) EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition, [[Paper]](https://arxiv.org/pdf/2304.01508.pdf),[[Code]](https://github.com/SiyuanYan1/EPVT)
- (arXiv 2023.04) U-Netmer: U-Net meets Transformer for medical image segmentation, [[Paper]](https://arxiv.org/pdf/2304.01401.pdf)
- (arXiv 2023.04) METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens, [[Paper]](https://arxiv.org/pdf/2304.02211.pdf)
- (arXiv 2023.04) HST-MRF: Heterogeneous Swin Transformer with Multi-Receptive Field for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2304.04614.pdf)
- (arXiv 2023.04) ForamViT-GAN: Exploring New Paradigms in Deep Learning for Micropaleontological Image Analysis, [[Paper]](https://arxiv.org/pdf/2304.04291.pdf)
- (arXiv 2023.04) Towards Evaluating Explanations of Vision Transformers for Medical Imaging, [[Paper]](https://arxiv.org/pdf/2304.06133.pdf)
- (arXiv 2023.04) Cross Attention Transformers for Multi-modal Unsupervised Whole-Body PET Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2304.07147.pdf)
- (arXiv 2023.04) CAD-RADS scoring of coronary CT angiography with Multi-Axis Vision Transformer: a clinically-inspired deep learning pipeline, [[Paper]](https://arxiv.org/pdf/2304.07277.pdf)
- (arXiv 2023.04) Transformer with Selective Shuffled Position Embedding using ROI-Exchange Strategy for Early Detection of Knee Osteoarthritis, [[Paper]](https://arxiv.org/pdf/2304.08364.pdf)
- (arXiv 2023.04) Masked Pre-Training of Transformers for Histology Image Analysis, [[Paper]](https://arxiv.org/pdf/2304.07434.pdf),[[Code]](https://github.com/BMIRDS/WSI-PLP/tree/maskhit)
- (arXiv 2023.04) Fibroglandular Tissue Segmentation in Breast MRI using Vision Transformers -- A multi-institutional evaluation, [[Paper]](https://arxiv.org/pdf/2304.08972.pdf)
- (arXiv 2023.04) Cross-Reference Transformer for Few-shot Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2304.09630.pdf)
- (arXiv 2023.04) DeformableFormer: Classification of Endoscopic Ultrasound Guided Fine Needle Biopsy in Pancreatic Diseases, [[Paper]](https://arxiv.org/pdf/2304.10791.pdf)
- (arXiv 2023.04) Vision Transformer for Efficient Chest X-ray and Gastrointestinal Image Classification, [[Paper]](https://arxiv.org/pdf/2304.11529.pdf)
- (arXiv 2023.04) Dilated-UNet: A Fast and Accurate Medical Image Segmentation Approach using a Dilated Transformer and U-Net Architecture, [[Paper]](https://arxiv.org/pdf/2304.11450.pdf),[[Code]](https://github.com/Omid-Nejati/Dilated_Unet)
- (arXiv 2023.04) STM-UNet: An Efficient U-shaped Architecture Based on Swin Transformer and Multi-scale MLP for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2304.12615.pdf)
- (arXiv 2023.05) 3D Brainformer: 3D Fusion Transformer for Brain Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2304.14508.pdf)
- (arXiv 2023.05) Transformer-based interpretable multi-modal data fusion for skin lesion classification, [[Paper]](https://arxiv.org/pdf/2304.14505.pdf)
- (arXiv 2023.05) Cross-Shaped Windows Transformer with Self-supervised Pretraining for Clinically Significant Prostate Cancer Detection in Bi-parametric MRI, [[Paper]](https://arxiv.org/pdf/2305.00385.pdf)
- (arXiv 2023.05) Transformer-Based Hierarchical Clustering for Brain Network Analysis, [[Paper]](https://arxiv.org/pdf/2305.04142.pdf),[[Code]](https://github.com/DDVD233/THC)
- (arXiv 2023.05) Brain Tumor Detection using Swin Transformers, [[Paper]](https://arxiv.org/pdf/2305.06025.pdf)
- (arXiv 2023.05) Transformers for CT Reconstruction From Monoplanar and Biplanar Radiographs, [[Paper]](https://arxiv.org/pdf/2305.06965.pdf)
- (arXiv 2023.05) Cascaded Cross-Attention Networks for Data-Efficient Whole-Slide Image Classification Using Transformers, [[Paper]](https://arxiv.org/pdf/2305.06965.pdf)
- (arXiv 2023.05) MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2305.08396.pdf)
- (arXiv 2023.05) LoViT: Long Video Transformer for Surgical Phase Recognition, [[Paper]](https://arxiv.org/pdf/2305.08989.pdf)
- (arXiv 2023.05) CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images, [[Paper]](https://arxiv.org/pdf/2305.09211.pdf)
- (arXiv 2023.05) Multi-resolution Spatiotemporal Enhanced Transformer Denoising with Functional Diffusive GANs for Constructing Brain Effective Connectivity in MCI analysis, [[Paper]](https://arxiv.org/pdf/2305.10754.pdf)
- (arXiv 2023.05) Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery, [[Paper]](https://arxiv.org/pdf/2305.11692.pdf),[[Code]](https://github.com/longbai1006/Surgical-VQLA)
- (arXiv 2023.05) Coordinated Transformer with Position & Sample-aware Central Loss for Anatomical Landmark Detection, [[Paper]](https://arxiv.org/pdf/2305.11338.pdf)
- (arXiv 2023.05) HGT: A Hierarchical GCN-Based Transformer for Multimodal Periprosthetic Joint Infection Diagnosis Using CT Images and Text, [[Paper]](https://arxiv.org/pdf/2305.18022.pdf)
- (arXiv 2023.05) Prompt-based Tuning of Transformer Models for Multi-Center Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2305.18948.pdf)
- (arXiv 2023.05) XTransCT: Ultra-Fast Volumetric CT Reconstruction using Two Orthogonal X-Ray Projections via a Transformer Network, [[Paper]](https://arxiv.org/pdf/2305.19621.pdf)
- (arXiv 2023.06) Prediction of Post-Operative Renal and Pulmonary Complication Using Transformers, [[Paper]](https://arxiv.org/pdf/2306.00698.pdf)
- (arXiv 2023.06) A Transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics, [[Paper]](https://arxiv.org/pdf/2306.00864.pdf),[[Code]](https://github.com/RL4M/IRENE)
- (arXiv 2023.06) A Novel Vision Transformer with Residual in Self-attention for Biomedical Image Classification, [[Paper]](https://arxiv.org/pdf/2306.01594.pdf)
- (arXiv 2023.06) Transformer-based Annotation Bias-aware Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2306.01340.pdf)
- (arXiv 2023.06) Inflated 3D Convolution-Transformer for Weakly-supervised Carotid Stenosis Grading with Ultrasound Videos, [[Paper]](https://arxiv.org/abs/2306.02548)
- (arXiv 2023.06) CiT-Net: Convolutional Neural Networks Hand in Hand with Vision Transformers for Medical Image Segmentation, [[Paper]](https://arxiv.org/abs/2306.03373),[[Code]](https://github.com/SR0920/CiT-Net)
- (arXiv 2023.06) TEC-Net: Vision Transformer Embrace Convolutional Neural Networks for Medical Image Segmentation, [[Paper]](https://arxiv.org/abs/2306.04086),[[Code]](https://github.com/SR0920/TEC-Net)
- (arXiv 2023.06) Enhancing COVID-19 Diagnosis through Vision Transformer-Based Analysis of Chest X-ray Images, [[Paper]](https://arxiv.org/abs/2306.06914)
- (arXiv 2023.06) TransMRSR: Transformer-based Self-Distilled Generative Prior for Brain MRI Super-Resolution, [[Paper]](https://arxiv.org/abs/2306.06669),[[Code]](https://github.com/goddesshs/TransMRSR.git)
- (arXiv 2023.06) Multimodal Optimal Transport-based Co-Attention Transformer with Global Structure Consistency for Survival Prediction, [[Paper]](https://arxiv.org/pdf/2306.08330.pdf),[[Code]](https://github.com/Innse/MOTCat)
- (arXiv 2023.06) SegT: A Novel Separated Edge-guidance Transformer Network for Polyp Segmentation, [[Paper]](https://arxiv.org/pdf/2306.10773.pdf)
- (arXiv 2023.06) KiUT: Knowledge-injected U-Transformer for Radiology Report Generation, [[Paper]](https://arxiv.org/pdf/2306.11345.pdf)
- (arXiv 2023.06) Concurrent ischemic lesion age estimation and segmentation of CT brain using a Transformer-based network, [[Paper]](https://arxiv.org/pdf/2306.12242.pdf)
- (arXiv 2023.06) CST-YOLO: A Novel Method for Blood Cell Detection Based on Improved YOLOv7 and CNN-Swin Transformer, [[Paper]](https://arxiv.org/pdf/2306.14590.pdf),[[Code]](https://github.com/mkang315/CST-YOLO)
- (arXiv 2023.06) Taming Detection Transformers for Medical Object Detection, [[Paper]](https://arxiv.org/pdf/2306.15472.pdf)
- (arXiv 2023.06) CellViT: Vision Transformers for Precise Cell Segmentation and Classification, [[Paper]](https://arxiv.org/pdf/2306.15350.pdf),[[Code]](https://github.com/TIO-IKIM/CellViT)
- (arXiv 2023.06) HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image, [[Paper]](https://arxiv.org/pdf/2306.17373.pdf),[[Code]](https://github.com/szc19990412/HVTSurv)
- (arXiv 2023.07) MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets, [[Paper]](https://arxiv.org/pdf/2307.02100.pdf),[[Code]](https://github.com/siyi-wind/MDViT)
- (arXiv 2023.07) Multi-Scale Prototypical Transformer for Whole Slide Image Classification, [[Paper]](https://arxiv.org/pdf/2307.02308.pdf)
- (arXiv 2023.07) Pretraining is All You Need: A Multi-Atlas Enhanced Transformer Framework for Autism Spectrum Disorder Classification, [[Paper]](https://arxiv.org/pdf/2307.01759.pdf),[[Code]](https://github.com/Lugges991/METAFormer)
- (arXiv 2023.07) H-DenseFormer: An Efficient Hybrid Densely Connected Transformer for Multimodal Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2307.01486.pdf),[[Code]](https://github.com/shijun18/H-DenseFormer)
- (arXiv 2023.07) Merging-Diverging Hybrid Transformer Networks for Survival Prediction in Head and Neck Cancer, [[Paper]](https://arxiv.org/pdf/2307.03427.pdf)
- (arXiv 2023.07) Source-Free Open-Set Domain Adaptation for Histopathological Images via Distilling Self-Supervised Vision Transformer, [[Paper]](https://arxiv.org/pdf/2307.04596.pdf),[[Code]](https://github.com/LTS5/Proto-SF-OSDA)
- (arXiv 2023.07) Automatic diagnosis of knee osteoarthritis severity using Swin transformer, [[Paper]](https://arxiv.org/pdf/2307.04442.pdf)
- (arXiv 2023.07) Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering, [[Paper]](https://arxiv.org/pdf/2307.05314.pdf),[[Code]](https://github.com/pengfeiliHEU/MUMC)
- (arXiv 2023.07) SwiFT: Swin 4D fMRI Transformer, [[Paper]](https://arxiv.org/pdf/2307.05916.pdf)
- (arXiv 2023.07) A Hierarchical Transformer Encoder to Improve Entire Neoplasm Segmentation on Whole Slide Image of Hepatocellular Carcinoma, [[Paper]](https://arxiv.org/pdf/2307.05800.pdf)
- (arXiv 2023.07) UGCANet: A Unified Global Context-Aware Transformer-based Network with Feature Alignment for Endoscopic Image Analysis, [[Paper]](https://arxiv.org/pdf/2307.06260.pdf)
- (arXiv 2023.07) RaBiT: An Efficient Transformer using Bidirectional Feature Pyramid Network with Reverse Attention for Colon Polyp Segmentation, [[Paper]](https://arxiv.org/pdf/2307.06420.pdf)
- (arXiv 2023.07) Transformer-based end-to-end classification of variable-length volumetric data, [[Paper]](https://arxiv.org/pdf/2307.06666.pdf),[[Code]](https://github.com/marziehoghbaie/VLFAT)
- (arXiv 2023.07) TriFormer: A Multi-modal Transformer Framework For Mild Cognitive Impairment Conversion Prediction, [[Paper]](https://arxiv.org/pdf/2307.07177.pdf)
- (arXiv 2023.07) MUVF-YOLOX: A Multi-modal Ultrasound Video Fusion Network for Renal Tumor Diagnosis, [[Paper]](https://arxiv.org/pdf/2307.07807.pdf),[[Code]](https://github.com/JeunyuLi/MUAF)
- (arXiv 2023.07) Study of Vision Transformers for Covid-19 Detection from Chest X-rays, [[Paper]](https://arxiv.org/pdf/2307.09402.pdf)
- (arXiv 2023.07) TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition, [[Paper]](https://arxiv.org/pdf/2307.09997.pdf)
- (arXiv 2023.07) GLSFormer : Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos, [[Paper]](https://arxiv.org/pdf/2307.11081.pdf)
- (arXiv 2023.07) Dense Transformer based Enhanced Coding Network for Unsupervised Metal Artifact Reduction, [[Paper]](https://arxiv.org/pdf/2307.12717.pdf)
- (arXiv 2023.07) SwinMM: Masked Multi-view with Swin Transformers for 3D Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2307.12591.pdf), [[Project]](https://github.com/UCSC-VLAA/SwinMM/)
- (arxiv 2023.07) Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction, [[Paper]](https://arxiv.org/pdf/2307.11952.pdf)
- (arxiv 2023.07) SCPAT-GAN: Structural Constrained and Pathology Aware Convolutional Transformer-GAN for Virtual Histology Staining of Human Coronary OCT images, [[Paper]](https://arxiv.org/pdf/2307.12138.pdf)
- (arxiv 2023.07) Simulation of Arbitrary Level Contrast Dose in MRI Using an Iterative Global Transformer Model, [[Paper]](https://arxiv.org/pdf/2307.11980.pdf)
- (arXiv 2023.07) AViT: Adapting Vision Transformers for Small Skin Lesion Segmentation Datasets, [[Paper]](https://arxiv.org/pdf/2307.13897.pdf)
- (arXiv 2023.07) CoVid-19 Detection leveraging Vision Transformers and Explainable AI, [[Paper]](https://arxiv.org/pdf/2307.16033.pdf)
- (arXiv 2023.08) ViT2EEG: Leveraging Hybrid Pretrained Vision Transformers for EEG Data, [[Paper]](https://arxiv.org/pdf/2308.00454.pdf)
- (arXiv 2023.08) Ensemble Learning with Residual Transformer for Brain Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2308.00128.pdf)
- (arXiv 2023.08) DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification, [[Paper]](https://arxiv.org/pdf/2308.00475.pdf)
- (arXiv 2023.08) Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network, [[Paper]](https://arxiv.org/pdf/2308.02101.pdf)
- (arXiv 2023.08) IIHT: Medical Report Generation with Image-to-Indicator Hierarchical Transformer, [[Paper]](https://arxiv.org/pdf/2308.05633.pdf)
- (arXiv 2023.08) TriDo-Former: A Triple-Domain Transformer for Direct PET Reconstruction from Low-Dose Sinograms, [[Paper]](https://arxiv.org/pdf/2308.05365.pdf)
- (arXiv 2023.08) From CNN to Transformer: A Review of Medical Image Segmentation Models, [[Paper]](https://arxiv.org/pdf/2308.05305.pdf)
- (arXiv 2023.08) CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification, [[Paper]](https://arxiv.org/pdf/2308.04008.pdf),[[Code]](https://github.com/dongkyuk/CXR-LT-public-solution)
- (arXiv 2023.08) SDLFormer: A Sparse and Dense Locality-enhanced Transformer for Accelerated MR Image Reconstruction, [[Paper]](https://arxiv.org/pdf/2308.04262.pdf),[[Code]](https://github.com/rahul-gs-16/sdlformer.git)
- (arXiv 2023.08) SEDA: Self-Ensembling ViT with Defensive Distillation and Adversarial Training for robust Chest X-rays Classification, [[Paper]](https://arxiv.org/pdf/2308.07874.pdf)
- (arXiv 2023.08) SkinDistilViT: Lightweight Vision Transformer for Skin Lesion Classification, [[Paper]](https://arxiv.org/pdf/2308.08669.pdf),[[Code]](https://github.com/Longman-Stan/SkinDistilVit)
- (arXiv 2023.08) Dense Error Map Estimation for MRI-Ultrasound Registration in Brain Tumor Surgery Using Swin UNETR, [[Paper]](https://arxiv.org/pdf/2308.10784.pdf)
- (arXiv 2023.08) Towards Hierarchical Regional Transformer-based Multiple Instance Learning, [[Paper]](https://arxiv.org/pdf/2308.12634.pdf)
- (arXiv 2023.08) ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis, [[Paper]](https://arxiv.org/pdf/2308.13324.pdf)
- (arXiv 2023.08) GEMTrans: A General, Echocardiography-based, Multi-Level Transformer Framework for Cardiovascular Diagnosis, [[Paper]](https://arxiv.org/pdf/2308.13217.pdf)
- (arXiv 2023.08) Unlocking Fine-Grained Details with Wavelet-based High-Frequency Enhancement in Transformers, [[Paper]](https://arxiv.org/pdf/2308.13442.pdf),[[Code]](https://github.com/mindflow-institue/WaveFormer)
- (arXiv 2023.08) CircleFormer: Circular Nuclei Detection in Whole Slide Images with Circle Queries and Attention, [[Paper]](https://arxiv.org/pdf/2308.16145.pdf),[[Code]](https://github.com/zhanghx-iim-ahu/CircleFormer)
- (arXiv 2023.08) Towards Optimal Patch Size in Vision Transformers for Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2308.16598.pdf),[[Code]](https://github.com/Ramtin-Mojtahedi/OVTPS)
- (arXiv 2023.09) Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care, [[Paper]](https://arxiv.org/pdf/2309.00252.pdf)
- (arXiv 2023.09) Beyond Self-Attention: Deformable Large Kernel Attention for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2309.00121.pdf),[[Code]](https://github.com/mindflow-institue/deformableLKA)
- (arXiv 2023.09) Laplacian-Former: Overcoming the Limitations of Vision Transformers in Local Texture Detection, [[Paper]](https://arxiv.org/pdf/2309.00108.pdf),[[Code]](https://github.com/mindflow-institue/Laplacian-Former)
- (arXiv 2023.09) Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design, [[Paper]](https://arxiv.org/pdf/2309.01408.pdf)
- (arXiv 2023.09) Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations, [[Paper]](https://arxiv.org/pdf/2309.01823.pdf)
- (arXiv 2023.09) Improving diagnosis and prognosis of lung cancer using vision transformers: A scoping review, [[Paper]](https://arxiv.org/pdf/2309.02783.pdf)
- (arXiv 2023.09) Evaluation Kidney Layer Segmentation on Whole Slide Imaging using Convolutional Neural Networks and Transformers, [[Paper]](https://arxiv.org/pdf/2309.02563.pdf)
- (arXiv 2023.09) 3D Transformer based on deformable patch location for differential diagnosis between Alzheimer's disease and Frontotemporal dementia, [[Paper]](https://arxiv.org/pdf/2309.03183.pdf)
- (arXiv 2023.09) Enhancing Hierarchical Transformers for Whole Brain Segmentation with Intracranial Measurements Integration, [[Paper]](https://arxiv.org/pdf/2309.04071.pdf),[[Code]](https://github.com/MASILab/UNesT/wholebrainSeg)
- (arXiv 2023.09) Phase-Specific Augmented Reality Guidance for Microscopic Cataract Surgery Using Long-Short Spatiotemporal Aggregation Transformer, [[Paper]](https://arxiv.org/pdf/2309.05209.pdf)
- (arXiv 2023.09) Few-Shot Medical Image Segmentation via a Region-enhanced Prototypical Transformer, [[Paper]](https://arxiv.org/pdf/2309.04825.pdf),[[Code]](https://github.com/YazhouZhu19/RPT)
- (arXiv 2023.09) ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2309.05674.pdf),[[Code]](https://github.com/xianlin7/ConvFormer)
- (arXiv 2023.09) UniBrain: Universal Brain MRI Diagnosis with Hierarchical Knowledge-enhanced Pre-training, [[Paper]](https://arxiv.org/pdf/2309.06828.pdf)
- (arXiv 2023.09) SAMUS: Adapting Segment Anything Model for Clinically-Friendly and Generalizable Ultrasound Image Segmentation, [[Paper]](https://arxiv.org/pdf/2309.06824.pdf),[[Code]](https://github.com/xianlin7/SAMUS)
- (arXiv 2023.09) HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis, [[Paper]](https://arxiv.org/pdf/2309.07400.pdf),[[Code]](https://github.com/HKU-MedAI/HIGT)
- (arXiv 2023.09) Cross-Modal Synthesis of Structural MRI and Functional Connectivity Networks via Conditional ViT-GANs, [[Paper]](https://arxiv.org/pdf/2309.08160.pdf)
- (arXiv 2023.09) Image-level supervision and self-training for transformer-based cross-modality tumor segmentation, [[Paper]](https://arxiv.org/pdf/2309.09246.pdf)
- (arXiv 2023.09) MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2309.08842.pdf),[[Code]](https://github.com/cchen-cc/MA-SAM)
- (arXiv 2023.09) Learning Dynamic MRI Reconstruction with Convolutional Network Assisted Reconstruction Swin Transformer, [[Paper]](https://arxiv.org/pdf/2309.10227.pdf)
- (arXiv 2023.09) Speech Audio Synthesis from Tagged MRI and Non-Negative Matrix Factorization via Plastic Transformer, [[Paper]](https://arxiv.org/pdf/2309.14586.pdf)
- (arXiv 2023.09) AiAReSeg: Catheter Detection and Segmentation in Interventional Ultrasound using Transformers, [[Paper]](https://arxiv.org/pdf/2309.14492.pdf)
- (arXiv 2023.09) Cross-Modal Transformer GAN: Brain Structural-Functional Deep Fusing Network for Alzheimer's Disease Analysis, [[Paper]](https://arxiv.org/pdf/2309.16206.pdf)
- (arXiv 2023.10) MVC: A Multi-Task Vision Transformer Network for COVID-19 Diagnosis from Chest X-ray Images, [[Paper]](https://arxiv.org/pdf/2310.00418.pdf)
- (arXiv 2023.10) Pubic Symphysis-Fetal Head Segmentation Using Full Transformer with Bi-level Routing Attention, [[Paper]](https://arxiv.org/pdf/2310.00289.pdf),[[Code]](https://github.com/Caipengzhou/BRAU-Net)
- (arXiv 2023.10) RoFormer for Position Aware Multiple Instance Learning in Whole Slide Image Classification, [[Paper]](https://arxiv.org/pdf/2310.01924.pdf),[[Code]](https://github.com/Sanofi-Public/DDS-RoFormerMIL)
- (arXiv 2023.10) Multi-Dimension-Embedding-Aware Modality Fusion Transformer for Psychiatric Disorder Clasification, [[Paper]](https://arxiv.org/pdf/2310.02690.pdf)
- (arXiv 2023.10) Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet, [[Paper]](https://arxiv.org/pdf/2310.03365.pdf)
- (arXiv 2023.10) Blind CT Image Quality Assessment Using DDPM-derived Content and Transformer-based Evaluator, [[Paper]](https://arxiv.org/pdf/2310.03118.pdf)
- (arXiv 2023.10) A Simple and Robust Framework for Cross-Modality Medical Image Segmentation applied to Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.05572.pdf),[[Code]](https://github.com/matteo-bastico/MI-Seg)
- (arXiv 2023.10) TransCC: Transformer Network for Coronary Artery CCTA Segmentation, [[Paper]](https://arxiv.org/pdf/2310.04779.pdf)
- (arXiv 2023.10) HydraViT: Adaptive Multi-Branch Transformer for Multi-Label Disease Classification from Chest X-ray Images, [[Paper]](https://arxiv.org/pdf/2310.06143.pdf)
- (arXiv 2023.10) COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images, [[Paper]](https://arxiv.org/pdf/2310.08165.pdf)
- (arXiv 2023.10) 3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.07781.pdf),[[Code]](https://github.com/Beckschen/3D-TransUNet)
- (arXiv 2023.10) Faster 3D cardiac CT segmentation with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.09099.pdf),[[Code]](http://github.com/ljollans/TRUNet)
- (arXiv 2023.10) Tackling Heterogeneity in Medical Federated learning via Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.09444.pdf)
- (arXiv 2023.10) A Multi-Scale Spatial Transformer U-Net for Simultaneously Automatic Reorientation and Segmentation of 3D Nuclear Cardiac Images, [[Paper]](https://arxiv.org/pdf/2310.10095.pdf)
- (arXiv 2023.10) SeUNet-Trans: A Simple yet Effective UNet-Transformer Model for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2310.09998.pdf)
- (arXiv 2023.10) Heart Disease Detection using Vision-Based Transformer Models from ECG Images, [[Paper]](https://arxiv.org/pdf/2310.12630.pdf)
- (arXiv 2023.10) Predicting Ovarian Cancer Treatment Response in Histopathology using Hierarchical Vision Transformers and Multiple Instance Learning, [[Paper]](https://arxiv.org/pdf/2310.12866.pdf)
- (arXiv 2023.10) DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2310.12570.pdf)
- (arXiv 2023.10) Skin Lesion Segmentation Improved by Transformer-based Networks with Inter-scale Dependency Modeling, [[Paper]](https://arxiv.org/pdf/2310.13604.pdf),[[Code]](https://github.com/saniaesk/skin-lesion-segmentation)
- (arXiv 2023.10) Prompt-based Grouping Transformer for Nucleus Detection and Classification, [[Paper]](https://arxiv.org/pdf/2310.14176.pdf)
- (arXiv 2023.10) Affine-Consistent Transformer for Multi-Class Cell Nuclei Detection, [[Paper]](https://arxiv.org/pdf/2310.14154.pdf), [[Code]](https://github.com/lhaof/PGT)
- (arXiv 2023.10) Inter-Scale Dependency Modeling for Skin Lesion Segmentation with Transformer-based Networks, [[Paper]](https://arxiv.org/pdf/2310.13727.pdf)
- (arXiv 2023.10) Ophthalmic Biomarker Detection Using Ensembled Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.14005.pdf)
- (arXiv 2023.10) What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning, [[Paper]](https://arxiv.org/pdf/2310.20607.pdf)
- (arXiv 2023.10) MIST: Medical Image Segmentation Transformer with Convolutional Attention Mixing (CAM) Decoder, [[Paper]](https://arxiv.org/pdf/2310.19898.pdf), [[Code]](https://github.com/Rahman-Motiur/MIST)
- (arXiv 2023.10) Muscle volume quantification: guiding transformers with anatomical priors, [[Paper]](https://arxiv.org/pdf/2310.20355.pdf)
- (arXiv 2023.10) fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for Multi-Subject Brain Activity Decoding, [[Paper]](https://arxiv.org/pdf/2311.00342.pdf)
- (arXiv 2023.11) Hybrid-Fusion Transformer for Multisequence MRI, [[Paper]](https://arxiv.org/pdf/2311.01308.pdf)
- (arXiv 2023.11) Capturing Local and Global Features in Medical Images by Using Ensemble CNN-Transformer, [[Paper]](https://arxiv.org/pdf/2311.01731.pdf)
- (arXiv 2023.11) Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data, [[Paper]](https://arxiv.org/pdf/2311.03217.pdf)
- (arXiv 2023.11) Transformer-based Model for Oral Epithelial Dysplasia Segmentation, [[Paper]](https://arxiv.org/pdf/2311.05452.pdf)
- (arXiv 2023.11) TransReg: Cross-transformer as auto-registration module for multi-view mammogram mass detection, [[Paper]](https://arxiv.org/pdf/2311.05192.pdf)
- (arXiv 2023.11) Automatic Report Generation for Histopathology images using pre-trained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.06176.pdf)
- (arXiv 2023.11) SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification, [[Paper]](https://arxiv.org/pdf/2311.07750.pdf)
- (arXiv 2023.11) LT-ViT: A Vision Transformer for multi-label Chest X-ray classification, [[Paper]](https://arxiv.org/pdf/2311.07263.pdf)
- (arXiv 2023.11) Swin UNETR++: Advancing Transformer-Based Dense Dose Prediction Towards Fully Automated Radiation Oncology Treatments, [[Paper]](https://arxiv.org/pdf/2311.06572.pdf)
- (arXiv 2023.11) TTMFN: Two-stream Transformer-based Multimodal Fusion Network for Survival Prediction, [[Paper]](https://arxiv.org/pdf/2311.07033.pdf)
- (arXiv 2023.11) MARformer: An Efficient Metal Artifact Reduction Transformer for Dental CBCT Images, [[Paper]](https://arxiv.org/pdf/2311.09590.pdf)
- (arXiv 2023.11) Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection, [[Paper]](https://arxiv.org/pdf/2311.09942.pdf)
- (arXiv 2023.11) Semi-supervised ViT knowledge distillation network with style transfer normalization for colorectal liver metastases survival prediction, [[Paper]](https://arxiv.org/pdf/2311.10305.pdf)
- (arXiv 2023.11) PMP-Swin: Multi-Scale Patch Message Passing Swin Transformer for Retinal Disease Classification, [[Paper]](https://arxiv.org/pdf/2311.11669.pdf)
- (arXiv 2023.11) MGCT: Mutual-Guided Cross-Modality Transformer for Survival Outcome Prediction using Integrative Histopathology-Genomic Features, [[Paper]](https://arxiv.org/pdf/2311.11659.pdf)
- (arXiv 2023.11) Radiology Report Generation Using Transformers Conditioned with Non-imaging Data, [[Paper]](https://arxiv.org/pdf/2311.11097.pdf)
- (arXiv 2023.11) Enhancing Transformer-Based Segmentation for Breast Cancer Diagnosis using Auto-Augmentation and Search Optimisation Techniques, [[Paper]](https://arxiv.org/pdf/2311.11065.pdf)
- (arXiv 2023.11) TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer, [[Paper]](https://arxiv.org/pdf/2311.13234.pdf), [[Code]](https://github.com/huiminxiong/TSegFormer)
- (arXiv 2023.11) Adapting Segment Anything Model (SAM) through Prompt-based Learning for Enhanced Protein Identification in Cryo-EM Micrographs, [[Paper]](https://arxiv.org/pdf/2311.16140.pdf)
- (arXiv 2023.12) Brainformer: Modeling MRI Brain Functions to Machine Vision, [[Paper]](https://arxiv.org/pdf/2312.00236.pdf)
- (arXiv 2023.12) Event Recognition in Laparoscopic Gynecology Videos with Hybrid Transformers, [[Paper]](https://arxiv.org/pdf/2312.00593.pdf)
- (arXiv 2023.12) MobileUtr: Revisiting the relationship between light-weight CNN and Transformer for efficient medical image segmentation, [[Paper]](https://arxiv.org/pdf/2312.01740.pdf), [[Code]](https://github.com/FengheTan9/MobileUtr)
- (arXiv 2023.12) Automatic Report Generation for Histopathology images using pre-trained Vision Transformers and BERT, [[Paper]](https://arxiv.org/pdf/2312.01435.pdf), [[Code]](https://github.com/FengheTan9/MobileUtr)
- (arXiv 2023.12) Predicting Bone Degradation Using Vision Transformer and Synthetic Cellular Microstructures Dataset, [[Paper]](https://arxiv.org/pdf/2312.03133.pdf)
- (arXiv 2023.12) Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography, [[Paper]](https://arxiv.org/pdf/2312.07052.pdf),[[Code]](https://github.com/maxiao0234/ARTran)
- (arXiv 2023.12) Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images, [[Paper]](https://arxiv.org/pdf/2312.06454.pdf),[[Code]](https://github.com/boyden/PointTransformerFL)
- (arXiv 2023.12) SP-DiffDose: A Conditional Diffusion Model for Radiation Dose Prediction Based on Multi-Scale Fusion of Anatomical Structures, Guided by SwinTransformer and Projector, [[Paper]](https://arxiv.org/pdf/2312.06187.pdf)
- (arXiv 2023.12) Pre-trained Universal Medical Image Transformer, [[Paper]](https://arxiv.org/pdf/2312.07630.pdf),[[Code]](https://github.com/function2-llx/PUMIT)
- (arXiv 2023.12) Vision Transformer-Based Deep Learning for Histologic Classification of Endometrial Cancer, [[Paper]](https://arxiv.org/pdf/2312.08479.pdf)
- (arXiv 2023.12) Brain Diffuser with Hierarchical Transformer for MCI Causality Analysis, [[Paper]](https://arxiv.org/pdf/2312.09022.pdf)
- (arXiv 2023.12) Glioblastoma Tumor Segmentation using an Ensemble of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.11467.pdf)
- (arXiv 2023.12) Hierarchical Vision Transformers for Context-Aware Prostate Cancer Grading in Whole Slide Images, [[Paper]](https://arxiv.org/pdf/2312.12619.pdf)
- (arXiv 2024.01) BRAU-Net++: U-Shaped Hybrid CNN-Transformer Network for Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2401.00722.pdf), [[Code]](https://github.com/Caipengzhou/BRAU-Netplusplus)
- (arXiv 2024.01) Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases, [[Paper]](https://arxiv.org/pdf/2401.00926.pdf)锛孾[Code]](https://github.com/JustlfC03/MFDS-DETR)
- (arXiv 2024.01) A novel method to enhance pneumonia detection via a model-level ensembling of CNN and vision transformer, [[Paper]](https://arxiv.org/pdf/2401.02358.pdf)
- (arXiv 2024.01) Vision Transformers and Bi-LSTM for Alzheimer's Disease Diagnosis from 3D MRI, [[Paper]](https://arxiv.org/pdf/2401.03132.pdf)
- (arXiv 2024.01) Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN, [[Paper]](https://arxiv.org/pdf/2401.05159.pdf)
- (arXiv 2024.01) Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-based Non-invasive Digital System, [[Paper]](https://arxiv.org/pdf/2401.04746.pdf)
- (arXiv 2024.01) Transformer-CNN Fused Architecture for Enhanced Skin Lesion Segmentation, [[Paper]](https://arxiv.org/pdf/2401.05481.pdf)
- (arXiv 2024.01) MedTransformer: Accurate AD Diagnosis for 3D MRI Images through 2D Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.06349.pdf)
- (arXiv 2024.01) D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation, [[Paper]](https://arxiv.org/pdf/2401.06150.pdf)
- (arXiv 2024.01) B-Cos Aligned Transformers Learn Human-Interpretable Features, [[Paper]](https://arxiv.org/pdf/2401.08868.pdf)
- (arXiv 2024.01) Triamese-ViT: A 3D-Aware Method for Robust Brain Age Estimation from MRIs, [[Paper]](https://arxiv.org/pdf/2401.09475.pdf)
- (arXiv 2024.01) MedBlindTuner: Towards Privacy-preserving Fine-tuning on Biomedical Images with Transformers and Fully Homomorphic Encryption, [[Paper]](https://arxiv.org/pdf/2401.09604.pdf)
- (arXiv 2024.01) M2ORT: Many-To-One Regression Transformer for Spatial Transcriptomics Prediction from Histopathology Images, [[Paper]](https://arxiv.org/pdf/2401.10608.pdf), [[Code]](https://github.com/Dootmaan/M2ORT/)
- (arXiv 2024.01) Friends Across Time: Multi-Scale Action Segmentation Transformer for Surgical Phase Recognition, [[Paper]](https://arxiv.org/pdf/2401.11644.pdf)
- (arXiv 2024.01) MAST: Video Polyp Segmentation with a Mixture-Attention Siamese Transformer, [[Paper]](https://arxiv.org/pdf/2401.12439.pdf), [[Code]](https://github.com/Junqing-Yang/MAST)
- (arXiv 2024.01) RTA-Former: Reverse Transformer Attention for Polyp Segmentation, [[Paper]](https://arxiv.org/pdf/2401.11671.pdf)
- (arXiv 2024.01) CAFCT: Contextual and Attentional Feature Fusions of Convolutional Neural Networks and Transformer for Liver Tumor Segmentation, [[Paper]](https://arxiv.org/pdf/2401.16886.pdf)
- (arXiv 2024.02) Disentangled Multimodal Brain MR Image Translation via Transformer-based Modality Infuser, [[Paper]](https://arxiv.org/pdf/2402.00375.pdf)
- (arXiv 2024.02) Vision Transformer-based Multimodal Feature Fusion Network for Lymphoma Segmentation on PET/CT Images, [[Paper]](https://arxiv.org/pdf/2402.02349.pdf)
- (arXiv 2024.02) ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2402.02029.pdf), [[Code]](https://github.com/HUANGLIZI/ScribFormer)
- (arXiv 2024.02) Hypergraph-Transformer (HGT) for Interactive Event Prediction in Laparoscopic and Robotic Surgery, [[Paper]](https://arxiv.org/pdf/2402.01974.pdf)
- (arXiv 2024.02) ConUNETR: A Conditional Transformer Network for 3D Micro-CT Embryonic Cartilage Segmentation, [[Paper]](https://arxiv.org/pdf/2402.03695.pdf)
- (arXiv 2024.02) Detection Transformer for Teeth Detection, Segmentation, and Numbering in Oral Rare Diseases: Focus on Data Augmentation and Inpainting Techniques, [[Paper]](https://arxiv.org/pdf/2402.04408.pdf)
- (arXiv 2024.02) Triplet-constraint Transformer with Multi-scale Refinement for Dose Prediction in Radiotherapy, [[Paper]](https://arxiv.org/pdf/2402.04556.pdf)
- (arXiv 2024.02) Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware Transformer (GOAT) for Whole Slide Histopathology Image Analysis, [[Paper]](https://arxiv.org/pdf/2402.05373.pdf)
- (arXiv 2024.02) NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung Nodule Invasiveness Prediction, [[Paper]](https://arxiv.org/pdf/2402.10066.pdf)
- (arXiv 2024.02) Deciphering Heartbeat Signatures: A Vision Transformer Approach to Explainable Atrial Fibrillation Detection from ECG Signals, [[Paper]](https://arxiv.org/pdf/2402.09474.pdf)
- (arXiv 2024.02) Epilepsy Seizure Detection and Prediction using an Approximate Spiking Convolutional Transformer, [[Paper]](https://arxiv.org/pdf/2402.09424.pdf)
- (arXiv 2024.02) Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation, [[Paper]](https://arxiv.org/pdf/2402.10887.pdf), [[Code]](https://github.com/ziyangwang007/Mamba-UNet)
- (arXiv 2024.02) FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model, [[Paper]](https://arxiv.org/pdf/2402.11775.pdf), [[Code]](https://github.com/MICLab-Unicamp/FOD-Swin-Net)
- (arXiv 2024.02) RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer, [[Paper]](https://arxiv.org/pdf/2402.12788.pdf), [[Code]](https://github.com/zizheng-guo/RhythmFormer)
- (arXiv 2024.02) Cell Graph Transformer for Nuclei Classification, [[Paper]](https://arxiv.org/pdf/2402.12946.pdf), [[Code]](https://github.com/lhaof/CGT)
- (arXiv 2024.02) wmh_seg: Transformer based U-Net for Robust and Automatic White Matter Hyperintensity Segmentation across 1.5T, 3T and 7T, [[Paper]](https://arxiv.org/pdf/2402.12701.pdf), [[Code]](https://github.com/lhaof/CGT)
- (arXiv 2024.02) SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion Classification Using 3D Multi-Phase Imaging, [[Paper]](https://arxiv.org/pdf/2402.17246.pdf)
- (arXiv 2024.02) UN-SAM: Universal Prompt-Free Segmentation for Generalized Nuclei Images, [[Paper]](https://arxiv.org/pdf/2402.16663.pdf), [[Code]](https://github.com/CUHK-AIM-Group/UN-SAM)
- (arXiv 2024.02) Investigating the Robustness of Vision Transformers against Label Noise in Medical Image Classification, [[Paper]](https://arxiv.org/pdf/2402.16734.pdf)
- (arXiv 2024.02) MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer, [[Paper]](https://arxiv.org/pdf/2402.16298.pdf), [[Code]](https://github.com/prithuls/MV-Swin-T)
- (arXiv 2024.03) Redefining cystoscopy with ai: bladder cancer diagnosis using an efficient hybrid cnn-transformer model, [[Paper]](https://arxiv.org/pdf/2403.03879.pdf)
- (arXiv 2024.03) Shifting Focus: From Global Semantics to Local Prominent Features in Swin-Transformer for Knee Osteoarthritis Severity Assessment, [[Paper]](https://arxiv.org/pdf/2403.09947.pdf), [[Code]](https://github.com/mtliba/KOA_NLCS2024)
- (arXiv 2024.03) Improved EATFormer: A Vision Transformer for Medical Image Classification, [[Paper]](https://arxiv.org/pdf/2403.13167.pdf)

### Mesh
- (arXiv 2022.07) Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers, [[Paper]](https://arxiv.org/pdf/2207.13820.pdf), [[Code]](https://github.com/postech-ami/FastMETRO)
- (arXiv 2022.11) TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer, [[Paper]](https://arxiv.org/pdf/2211.10705.pdf)
- (arXiv 2023.03) GATOR: Graph-Aware Transformer with Motion-Disentangled Regression for Human Mesh Recovery from a 2D Pose, [[Paper]](https://arxiv.org/pdf/2303.05652.pdf)
- (arXiv 2023.03) DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video, [[Paper]](https://arxiv.org/pdf/2303.13397.pdf)
- (arXiv 2023.03) POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery, [[Paper]](https://arxiv.org/pdf/2303.13357.pdf), [[Project]](https://zczcwh.github.io/potter_page)
- (arXiv 2023.03) One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer, [[Paper]](https://arxiv.org/pdf/2303.16160.pdf), [[Project]](https://osx-ubody.github.io/)
- (arXiv 2023.07) MeT: A Graph Transformer for ation of 3D Meshes, [[Paper]](https://arxiv.org/pdf/2307.01115.pdf), [[Project]](https://osx-ubody.github.io/)
- (arXiv 2023.07) 3Deformer: A Common Framework for Image-Guided Mesh Deformation, [[Paper]](https://arxiv.org/pdf/2307.09892.pdf), [[Project]](https://osx-ubody.github.io/)
- (arXiv 2023.07) JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human Mesh Recovery, [[Paper]](https://arxiv.org/pdf/2307.16377.pdf), [[Code]](https://github.com/xljh0520/JOTR)
- (arXiv 2023.08) Coordinate Transformer: Achieving Single-stage Multi-person Mesh Recovery from Videos, [[Paper]](https://arxiv.org/pdf/2308.10334.pdf), [[Code]](https://github.com/Li-Hao-yuan/CoordFormer)
- (arXiv 2023.11) MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers, [[Paper]](https://arxiv.org/pdf/2311.15475.pdf), [[Project]](https://nihalsid.github.io/mesh-gpt/)
- (arXiv 2024.02) Multi-Human Mesh Recovery with Transformers, [[Paper]](https://arxiv.org/pdf/2402.16806.pdf)
- (arXiv 2024.03) Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery, [[Paper]](https://arxiv.org/pdf/2403.09063.pdf)
- (arXiv 2024.03) T-Pixel2Mesh: Combining Global and Local Transformer for 3D Mesh Generation from a Single Image, [[Paper]](https://arxiv.org/pdf/2403.13663.pdf)
- (arXiv 2024.03) PostoMETRO: Pose Token Enhanced Mesh Transformer for Robust 3D Human Mesh Recovery, [[Paper]](https://arxiv.org/pdf/2403.12473.pdf)

### Metric learning
- (arXiv 2022.03) Hyperbolic Vision Transformers: Combining Improvements in Metric Learning, [[Paper]](https://arxiv.org/pdf/2203.10833.pdf),[[Code]](https://github.com/htdt/hyp_metric)

### Motion
- (arXiv 2021.03) Single-Shot Motion Completion with Transformer, [[Paper]](https://arxiv.org/pdf/2103.00776.pdf), [[Code]](https://github.com/FuxiCV/SSMCT)
- (arXiv 2021.03) DanceNet3D: Music Based Dance Generation with Parametric Motion Transformer, [[Paper]](https://arxiv.org/pdf/2103.10206.pdf)
- (arXiv 2021.03) Multimodal Motion Prediction with Stacked Transformers, [[Paper]](https://arxiv.org/pdf/2103.11624.pdf), [[Code]](https://github.com/decisionforce/mmTransformer)
- (arXiv 2021.04) Action-Conditioned 3D Human Motion Synthesis with Transformer VAE, [[Paper]](https://arxiv.org/abs/2104.05670)
- (arXiv 2021.10) AniFormer: Data-driven 3D Animation with Transformer, [[Paper]](https://arxiv.org/pdf/2110.10533.pdf), [[Code]](https://github.com/mikecheninoulu/AniFormer)
- (arXiv 2021.11) Multi-Person 3D Motion Prediction with Multi-Range Transformers, [[Paper]](https://arxiv.org/pdf/2111.12073.pdf), [[Code]](https://jiashunwang.github.io/MRT/)
- (arXiv 2022.03) ActFormer: A GAN Transformer Framework towards General Action-Conditioned 3D Human Motion Generation, [[Paper]](https://arxiv.org/pdf/2203.07706.pdf)
- (arXiv 2022.03) Transformer Inertial Poser: Attention-based Real-time Human Motion Reconstruction from Sparse IMUs, [[Paper]](https://arxiv.org/pdf/2203.15720.pdf)
- (arXiv 2022.03) Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation, [[Paper]](https://arxiv.org/pdf/2203.16202.pdf)
- (arXiv 2022.04) HiT-DVAE: Human Motion Generation via Hierarchical Transformer Dynamical VAE, [[Paper]](https://arxiv.org/pdf/2204.01565.pdf)
- (arXiv 2022.07) TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction, [[Paper]](https://arxiv.org/pdf/2207.00170.pdf)
- (arXiv 2022.08) SoMoFormer: Social-Aware Motion Transformer for Multi-Person Motion Prediction, [[Paper]](https://arxiv.org/pdf/2208.09224.pdf)
- (arXiv 2022.09) Motion Transformer with Global Intention Localization and Local Movement Refinement, [[Paper]](https://arxiv.org/pdf/2209.13508.pdf), [[Code]](https://github.com/sshaoshuai/MTR)
- (arXiv 2022.09) NEURAL MARIONETTE: A Transformer-based Multi-action Human Motion Synthesis System, [[Paper]](https://arxiv.org/pdf/2209.13204.pdf), [[Project]](https://wjohnnyw.github.io/blog/tag2motion/)
- (arXiv 2022.09) Motion Transformer for Unsupervised Image Animation, [[Paper]](https://arxiv.org/pdf/2209.14024.pdf), [[Code]](https://github.com/JialeTao/MoTrans)
- (arXiv 2022.11) Blur Interpolation Transformer for Real-World Motion from Blur, [[Paper]](https://arxiv.org/pdf/2211.11423.pdf)
- (arXiv 2022.12) Transformer-Based Learned Optimization, [[Paper]](https://arxiv.org/pdf/2212.01055.pdf)
- (arXiv 2023.01) Diagnose Like a Pathologist: Transformer-Enabled Hierarchical Attention-Guided Multiple Instance Learning for Whole Slide Image Classification, [[Paper]](https://arxiv.org/pdf/2301.08125.pdf)
- (arXiv 2023.02) Robust Human Motion Forecasting using Transformer-based Model, [[Paper]](https://arxiv.org/pdf/2302.08274.pdf)
- (arXiv 2023.02) STB-VMM: Swin Transformer Based Video Motion Magnification, [[Paper]](https://arxiv.org/pdf/2302.10001.pdf)
- (arXiv 2023.02) Human MotionFormer: Transferring Human Motions with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.11306.pdf), [[Project]](https://github.com/KumapowerLIU/Human-MotionFormer)
- (arXiv 2023.02) Multi-Scale Control Signal-Aware Transformer for Motion Synthesis without Phase, [[Paper]](https://arxiv.org/pdf/2303.01685.pdf)
- (arXiv 2023.03) SPOTR: Spatio-temporal Pose Transformers for Human Motion Prediction, [[Paper]](https://arxiv.org/pdf/2303.06277.pdf)
- (arXiv 2023.04) BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation, [[Paper]](https://arxiv.org/pdf/2304.02225.pdf), [[Code]](https://github.com/JunHeum/BiFormer)
- (arXiv 2023.05) XFormer: Fast and Accurate Monocular 3D Body Capture, [[Paper]](https://arxiv.org/pdf/2305.11101.pdf)
- (arXiv 2023.05) Imitating Task and Motion Planning with Visuomotor Transformers, [[Paper]](https://arxiv.org/pdf/2305.16309.pdf), [[Code]](https://mihdalal.github.io/optimus/)
- (arXiv 2023.06) PGformer: Proxy-Bridged Game Transformer for Multi-Person Extremely Interactive Motion Prediction, [[Paper]](https://arxiv.org/pdf/2306.03374.pdf)
- (arXiv 2023.06) ModeT: Learning Deformable Image Registration via Motion Decomposition Transformer, [[Paper]](https://arxiv.org/pdf/2306.05688.pdf), [[Code]](https://github.com/ZAX130/SmileCode)
- (arXiv 2023.07) TransFusion: A Practical and Effective Transformer-based Diffusion Model for 3D Human Motion Prediction, [[Paper]](https://arxiv.org/pdf/2307.16106.pdf)
- (arXiv 2023.08) Joint-Relation Transformer for Multi-Person Motion Prediction, [[Paper]](https://arxiv.org/pdf/2308.04808.pdf), [[Code]](https://github.com/MediaBrain-SJTU/JRTransformer)
- (arXiv 2023.08) A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis, [[Paper]](https://arxiv.org/pdf/2308.07301.pdf), [[Code]](https://github.com/Sadegh28/ATR)
- (arXiv 2023.10) Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding, [[Paper]](https://arxiv.org/pdf/2310.12970.pdf), [[Code]](https://github.com/zhejz/HPTR)
- (arXiv 2023.11) Egocentric Whole-Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement, [[Paper]](https://arxiv.org/pdf/2311.16495.pdf)
- (arXiv 2023.12) MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR, [[Paper]](https://arxiv.org/pdf/2312.02409.pdf), [[Code]](https://waymo.com/open/challenges/2023/motion-prediction/)
- (arXiv 2023.12) EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering within Transformer, [[Paper]](https://arxiv.org/pdf/2312.04152.pdf), [[Code]](https://github.com/VUT-HFUT/EulerMormer)
- (arXiv 2023.12) Sign Language Production with Latent Motion Transformer, [[Paper]](https://arxiv.org/pdf/2312.12917.pdf)
- (arXiv 2024.01) AdvMT: Adversarial Motion Transformer for Long-term Human Motion Prediction, [[Paper]](https://arxiv.org/pdf/2401.05018.pdf)

### Multi-label
- (arXiv 2021.06) MlTr: Multi-label Classification with Transformer, [[Paper]](https://arxiv.org/pdf/2106.06195.pdf), [[Code]](https://github.com/starmemda/MlTr/)
- (arXiv 2021.07) Query2Label: A Simple Transformer Way to Multi-Label Classification, [[Paper]](https://arxiv.org/pdf/2107.10834.pdf), [[Code]](https://github.com/SlongLiu/query2labels)
- (arXiv 2021.10) Transformer-based Dual Relation Graph for Multi-label Image Recognition, [[Paper]](https://arxiv.org/pdf/2110.04722.pdf), [[Code]](https://github.com/iCVTEAM/TDRG)
- (arXiv 2020.11) General Multi-label Image Classification with Transformers, [[Paper]](https://arxiv.org/pdf/2011.14027)
- (arXiv 2022.03) Graph Attention Transformer Network for Multi-Label Image Classification, [[Paper]](https://arxiv.org/pdf/2203.04049.pdf)
- (arXiv 2022.03) Incomplete Multi-View Multi-Label Learning via Label-Guided Masked Viewand Category-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2303.07180.pdf)
- (arXiv 2023.09) Multi-Label Feature Selection Using Adaptive and Transformed Relevance, [[Paper]](https://arxiv.org/pdf/2309.14768.pdf)

### Multi-task/modal
- (arXiv 2021.02) Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer, [[Paper]](https://arxiv.org/abs/2102.10772), [[Code]](https://mmf.sh/)
- (arXiv 2021.04) MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding, [[Paper]](https://arxiv.org/pdf/2104.12763.pdf), [[Code]](https://github.com/ashkamath/mdetr)
- (arXiv 2021.04) Multi-Modal Fusion Transformer for End-to-End Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2104.09224.pdf)
- (arXiv 2021.04) VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text, [[Paper]](https://arxiv.org/pdf/2104.11178.pdf)
- (arXiv 2021.04) Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning, [[Paper]](https://arxiv.org/abs/2104.03135)
- (arXiv 2021.06) Scene Transformer: A Unified Multi-task Model for Behavior Prediction and Planning, [[Paper]](https://arxiv.org/pdf/2106.08417.pdf)
- (arXiv 2021.06) Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation, [[Paper]](https://arxiv.org/pdf/2106.11401.pdf)
- (arXiv 2021.06) A Transformer-based Cross-modal Fusion Model with Adversarial Training, [[Paper]](https://arxiv.org/pdf/2106.13033.pdf)
- (arXiv 2021.07) Attention Bottlenecks for Multimodal Fusion, [[Paper]](https://arxiv.org/pdf/2107.00135.pdf)
- (arXiv 2021.07) Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots, [[Paper]](https://arxiv.org/pdf/2107.00811.pdf)
- (arXiv 2021.07) Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions, [[Paper]](https://arxiv.org/pdf/2107.00789.pdf)
- (arXiv 2021.07) Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers, [[Paper]](https://arxiv.org/pdf/2107.03996.pdf), [[Code]](https://RchalYang.github.io/LocoTransformer)
- (arXiv 2021.08) StrucTexT: Structured Text Understanding with Multi-Modal Transformers, [[Paper]](https://arxiv.org/pdf/2108.02923.pdf)
- (arXiv 2021.08) Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations, [[Paper]](https://arxiv.org/pdf/2108.05887.pdf)
- (arXiv 2021.09) TxT: Crossmodal End-to-End Learning with Transformers, [[Paper]](https://arxiv.org/pdf/2109.04422.pdf)
- (arXiv 2021.09) Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2109.04448.pdf)
- (arXiv 2021.09) Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering, [[Paper]](https://arxiv.org/pdf/2109.04735.pdf)
- (arXiv 2021.09) On Pursuit of Designing Multi-modal Transformer for Video Grounding, [[Paper]](https://arxiv.org/pdf/2109.06085.pdf), [[Code]](https://sites.google.com/view/mengcao/publication/gtr)
- (arXiv 2021.09) Dyadformer: A Multi-modal Transformer for Long-Range Modeling of Dyadic Interactions, [[Paper]](https://arxiv.org/pdf/2109.09487.pdf)
- (arXiv 2021.09) KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2109.10504.pdf)
- (arXiv 2021.10) Unifying Multimodal Transformer for Bi-directional Image and Text Generation, [[Paper]](https://arxiv.org/pdf/2110.09753.pdf), [[Code]](https://github.com/researchmm/generate-it)
- (arXiv 2021.10) VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing, [[Paper]](https://arxiv.org/pdf/2110.11338.pdf)
- (arXiv 2021.10) Detecting Dementia from Speech and Transcripts using Transformers, [[Paper]](https://arxiv.org/pdf/2110.14769.pdf)
- (arXiv 2021.11) MEmoBERT: Pre-training Model with Prompt-based Learning for Multimodal Emotion Recognition, [[Paper]](https://arxiv.org/pdf/2111.00865.pdf)
- (arXiv 2021.11) VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts, [[Paper]](https://arxiv.org/pdf/2111.02358.pdf), [[Code]](https://github.com/microsoft/unilm/tree/master/vlmo)
- (arXiv 2021.11) An Empirical Study of Training End-to-End Vision-and-Language Transformers, [[Paper]](https://arxiv.org/pdf/2111.02387.pdf), [[Code]](https://github.com/zdou0830/METER)
- (arXiv 2021.11) CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval, [[Paper]](https://arxiv.org/pdf/2111.05610.pdf)
- (arXiv 2021.11) Graph Relation Transformer: Incorporating pairwise object features into the Transformer architecture, [[Paper]](https://arxiv.org/pdf/2111.06075.pdf), [[Code1]](https://github.com/michaelzyang/graph-relation-m4c), [[Code2]](https://github.com/derikclive/transformers)
- (arXiv 2021.11) UFO: A UniFied TransfOrmer for Vision-Language Representation Learning, [[Paper]](https://arxiv.org/pdf/2111.10023.pdf)
- (arXiv 2021.11) Multi-modal Transformers Excel at Class-agnostic Object Detection, [[Paper]](https://arxiv.org/pdf/2111.11430.pdf), [[Code]](https://git.io/J1HPY)
- (arXiv 2021.11) Sparse Fusion for Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2111.11992.pdf)
- (arXiv 2021.11) VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling, [[Paper]](https://arxiv.org/pdf/2111.12681.pdf), [[Code]](https://github.com/tsujuifu/pytorch_violet)
- (arXiv 2021.11) Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing, [[Paper]](https://arxiv.org/pdf/2111.12608.pdf), [[Code]](https://github.com/OPEN-AIR-SUN/Cerberus)
- (arXiv 2021.11) PolyViT: Co-training Vision Transformers on Images, Videos and Audio, [[Paper]](https://arxiv.org/pdf/2111.12993.pdf)
- (arXiv 2021.11) End-to-End Referring Video Object Segmentation with Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2111.14821.pdf), [[Code]](https://github.com/mttr2021/MTTR)
- (arXiv 2021.12) TransMEF: A Transformer-Based Multi-Exposure Image Fusion Framework using Self-Supervised Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2112.01030.pdf), [[Code]](https://github.com/miccaiif/TransMEF)
- (arXiv 2021.12) LMR-CBT: Learning Modality-fused Representations with CB-Transformer for Multimodal Emotion Recognition from Unaligned Multimodal Sequences, [[Paper]](https://arxiv.org/pdf/2112.01697.pdf)
- (arXiv 2021.12) Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2112.05587.pdf)
- (arXiv 2021.12) VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling, [[Paper]](https://arxiv.org/pdf/2112.05692.pdf)
- (arXiv 2021.12) VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks, [[Paper]](https://arxiv.org/pdf/2112.06825.pdf),[[Code]](https://github.com/ylsung/VL_adapter)
- (arXiv 2021.12) Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text, [[Paper]](https://arxiv.org/pdf/2112.07074.pdf)
- (arXiv 2021.12) Distilled Dual-Encoder Model for Vision-Language Understanding, [[Paper]](https://arxiv.org/pdf/2112.08723.pdf),[[Code]](https://github.com/kugwzk/Distilled-DualEncoder)
- (arXiv 2021.12) Multimodal Personality Recognition using Cross-Attention Transformer and Behaviour Encoding, [[Paper]](https://arxiv.org/pdf/2112.12180.pdf)
- (arXiv 2021.12) SLIP: Self-supervision meets Language-Image Pre-training, [[Paper]](https://arxiv.org/pdf/2112.12750.pdf),[[Code]](https://github.com/facebookresearch/SLIP)
- (arXiv 2021.12) Synchronized Audio-Visual Frames with Fractional Positional Encoding for Transformers in Video-to-Text Translation, [[Paper]](https://arxiv.org/pdf/2112.14088.pdf),[[Code]](https://github.com/fpe-vtt/ftt-vpe)
- (arXiv 2022.01) Robust Self-Supervised Audio-Visual Speech Recognition, [[Paper]](https://arxiv.org/pdf/2201.01763.pdf),[[Code]](https://github.com/facebookresearch/av_hubert)
- (arXiv 2022.01) Self-Training Vision Language BERTs with a Unified Conditional Model, [[Paper]](https://arxiv.org/pdf/2201.02010.pdf)
- (arXiv 2022.01) Uniformer: Unified Transformer for Efficient Spatiotemporal Representation Learning, [[Paper]](https://arxiv.org/pdf/2201.04676.pdf),[[Code]](https://github.com/Sense-X/UniFormer)
- (arXiv 2022.01) BridgeFormer: Bridging Video-text Retrieval with Multiple Choice Questions, [[Paper]](https://arxiv.org/pdf/2201.04850.pdf),[[Code]](https://geyuying.github.io/MCQ.html)
- (arXiv 2022.01) OMNIVORE: A Single Model for Many Visual Modalities, [[Paper]](https://arxiv.org/pdf/2201.08377.pdf),[[Code]](https://facebookresearch.github.io/omnivore/)
- (arXiv 2022.01) A Pre-trained Audio-Visual Transformer for Emotion Recognition, [[Paper]](https://arxiv.org/pdf/2201.09165.pdf)
- (arXiv 2022.01) Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition, [[Paper]](https://arxiv.org/pdf/2201.10439.pdf)
- (arXiv 2022.02) Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer, [[Paper]](https://arxiv.org/pdf/2202.05508.pdf)
- (arXiv 2022.03) DXM-TransFuse U-net: Dual Cross-Modal Transformer Fusion U-net for Automated Nerve Identification, [[Paper]](https://arxiv.org/pdf/2202.13304.pdf)
- (arXiv 2022.03) LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives, [[Paper]](https://arxiv.org/pdf/2203.01445.pdf)
- (arXiv 2022.03) VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer, [[Paper]](https://arxiv.org/pdf/2203.04099.pdf),[[Project]](https://ipcv.github.io/VoViT/)
- (arXiv 2022.03) MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization, [[Paper]](https://arxiv.org/pdf/2203.07086.pdf),[[Project]](https://ipcv.github.io/VoViT/)
- (arXiv 2022.03) Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2203.06386.pdf)
- (arXiv 2022.03) Inverted Pyramid Multi-task Transformer for Dense Scene Understanding, [[Paper]](https://arxiv.org/pdf/2203.07997.pdf)
- (arXiv 2022.03) UNIMO-2: End-to-End Unified Vision-Language Grounded Learning, [[Paper]](https://arxiv.org/pdf/2203.09067.pdf),[[Project]](https://unimo-ptm.github.io/)
- (arXiv 2022.03) Multi-Modal Learning for AU Detection Based on Multi-Head Fused Transformers, [[Paper]](https://arxiv.org/pdf/2203.11441.pdf)
- (arXiv 2022.03) UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection, [[Paper]](https://arxiv.org/pdf/2203.12745.pdf),[[Project]](https://github.com/TencentARC/UMT)
- (arXiv 2022.03) Multi-modal Multi-label Facial Action Unit Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2203.13301.pdf),[[Project]](https://github.com/TencentARC/UMT)
- (arXiv 2022.03) Multimodal Fusion Transformer for Remote Sensing Image Classification, [[Paper]](https://arxiv.org/pdf/2203.16952.pdf)
- (arXiv 2022.03) VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers, [[Paper]](https://arxiv.org/pdf/2203.17247.pdf)
- (arXiv 2022.04) MultiMAE: Multi-modal Multi-task Masked Autoencoders, [[Paper]](https://arxiv.org/pdf/2204.01678.pdf),[[Project]](https://multimae.epfl.ch/)
- (arXiv 2022.04) Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation, [[Paper]](https://arxiv.org/pdf/2204.03500.pdf)
- (arXiv 2022.04) MHMS: Multimodal Hierarchical Multimedia Summarization, [[Paper]](https://arxiv.org/pdf/2204.03734.pdf)
- (arXiv 2022.04) Multimodal Transformer for Nursing Activity Recognition, [[Paper]](https://arxiv.org/pdf/2204.04564.pdf)
- (arXiv 2022.04) Are Multimodal Transformers Robust to Missing Modality?, [[Paper]](https://arxiv.org/pdf/2204.05454.pdf)
- (arXiv 2022.04) X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks, [[Paper]](https://arxiv.org/pdf/2204.05626.pdf)
- (arXiv 2022.04) Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks, [[Paper]](https://arxiv.org/pdf/2204.07780.pdf)
- (arXiv 2022.04) Multimodal Token Fusion for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.08721.pdf)
- (arXiv 2022.04) Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval, [[Paper]](https://arxiv.org/pdf/2204.09730.pdf), [[Code]](https://github.com/mshukor/TFood)
- (arXiv 2022.04) ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer, [[Paper]](https://arxiv.org/pdf/2204.10777.pdf)
- (arXiv 2022.05) Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training, [[Paper]](https://arxiv.org/pdf/2205.04948.pdf)
- (arXiv 2022.05) MulT: An End-to-End Multitask Learning Transformer, [[Paper]](https://arxiv.org/pdf/2205.08303.pdf)), [[Project]](https://ivrl.github.io/MulT/)
- (arXiv 2022.05) Training Vision-Language Transformers from Captions Alone, [[Paper]](https://arxiv.org/pdf/2205.09256.pdf), [[Code]](https://github.com/guilk/VLC)
- (arXiv 2022.05) GIT: A Generative Image-to-text Transformer for Vision and Language, [[Paper]](https://arxiv.org/pdf/2205.14100.pdf)
- (arXiv 2022.05) Multi-Task Learning with Multi-query Transformer for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2205.14354.pdf)
- (arXiv 2022.06) VL-BEIT: Generative Vision-Language Pretraining, [[Paper]](https://arxiv.org/pdf/2206.01127.pdf), [[Code]](https://github.com/microsoft/unilm)
- (arXiv 2022.06) AntPivot: Livestream Highlight Detection via Hierarchical Attention Mechanism, [[Paper]](https://arxiv.org/pdf/2206.04888.pdf)
- (arXiv 2022.06) A Unified Sequence Interface for Vision Tasks, [[Paper]](https://arxiv.org/pdf/2206.07669.pdf)
- (arXiv 2022.06) Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos, [[Paper]](https://arxiv.org/pdf/2206.07981.pdf)
- (arXiv 2022.06) Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks, [[Paper]](https://arxiv.org/pdf/2206.08916.pdf)
- (arXiv 2022.06) M&M Mix: A Multimodal Multiview Transformer Ensemble, [[Paper]](https://arxiv.org/pdf/2206.09852.pdf)
- (arXiv 2022.06) RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval, [[Paper]](https://arxiv.org/pdf/2206.12845.pdf)
- (arXiv 2022.07) You Only Need One Detector: Unified Object Detector for Different Modalities based on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.01071.pdf), [[Code]](https://github.com/liketheflower/YONOD.git)
- (arXiv 2022.07) Open-Vocabulary Multi-Label Classification via Multi-modal Knowledge Transfer, [[Paper]](https://arxiv.org/pdf/2207.01887.pdf), [[Code]](https://github.com/seanhe97/MKT)
- (arXiv 2022.07) Audio鈭扸isual Segmentation, [[Paper]](https://arxiv.org/pdf/2207.05042.pdf), [[Code]](https://github.com/OpenNLPLab/AVSBench)
- (arXiv 2022.07) FashionViL: Fashion-Focused Vision-and-Language Representation Learning, [[Paper]](https://arxiv.org/pdf/2207.08150.pdf), [[Code]](https://github.com/BrandonHanx/mmf)
- (arXiv 2022.07) Multimodal Transformer for Automatic 3D Annotation and Object Detection, [[Paper]](https://arxiv.org/pdf/2207.09805.pdf), [[Code]](https://github.com/Cliu2/MTrans)
- (arXiv 2022.07) UFO: Unified Feature Optimization, [[Paper]](https://arxiv.org/pdf/2207.10341.pdf), [[Code]](https://github.com/PaddlePaddle/VIMER/tree/main/UFO)
- (arXiv 2022.07) An Ensemble Approach for Multiple Emotion Descriptors Estimation Using Multi-task Learning, [[Paper]](https://arxiv.org/pdf/2207.10878.pdf)
- (arXiv 2022.07) STrajNet: Occupancy Flow Prediction via Multi-modal Swin Transformer, [[Paper]](https://arxiv.org/pdf/2208.00394.pdf)
- (arXiv 2022.08) Multi-Task Transformer with uncertainty modelling for Face Based Affective Computing, [[Paper]](https://arxiv.org/pdf/2208.03506.pdf)
- (arXiv 2022.08) Multi-modal Transformer Path Prediction for Autonomous Vehicle, [[Paper]](https://arxiv.org/pdf/2208.07256.pdf)
- (arXiv 2022.08) Efficient Multimodal Transformer with Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis, [[Paper]](https://arxiv.org/pdf/2208.07589.pdf)
- (arXiv 2022.08) VAuLT: Augmenting the Vision-and-Language Transformer with the Propagation of Deep Language Representations, [[Paper]](https://arxiv.org/pdf/2208.09021.pdf), [[Code]](https://github.com/gchochla/VAuLT)
- (arXiv 2022.08) Flat Multi-modal Interaction Transformer for Named Entity Recognition, [[Paper]](https://arxiv.org/pdf/2208.11039.pdf), [[Code]](https://github.com/gchochla/VAuLT)
- (arXiv 2022.08) TFusion: Transformer based N-to-One Multimodal Fusion Block, [[Paper]](https://arxiv.org/pdf/2208.12776.pdf)
- (arXiv 2022.09) Multi-task Swin Transformer for Motion Artifacts Classification and Cardiac Magnetic Resonance Image Segmentation, [[Paper]](https://arxiv.org/pdf/2209.02470.pdf)
- (arXiv 2022.09) TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction, [[Paper]](https://arxiv.org/pdf/2209.05036.pdf), [[Code]](https://t.ly/V-_W)
- (arXiv 2022.09) Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer, [[Paper]](https://arxiv.org/pdf/2209.07026.pdf), [[Code]](https://github.com/Reimilia/Simple3D-Former)
- (arXiv 2022.09) UniColor: A Unified Framework for Multi-Modal Colorization with Transformer, [[Paper]](https://arxiv.org/pdf/2209.11223.pdf), [[Code]](https://luckyhzt.github.io/unicolor)
- (arXiv 2022.09) TVLT: Textless Vision-Language Transformer, [[Paper]](https://arxiv.org/pdf/2209.14156.pdf), [[Code]](https://github.com/zinengtang/TVLT)
- (arXiv 2022.10) A Strong Transfer Baseline for RGB-D Fusion in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.00843.pdf)
- (arXiv 2022.10) Cascaded Multi-Modal Mixing Transformers for Alzheimer's Disease Classification with Incomplete Data, [[Paper]](https://arxiv.org/pdf/2210.00255.pdf)
- (arXiv 2022.10) VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment, [[Paper]](https://arxiv.org/pdf/2210.04135.pdf)
- (arXiv 2022.10) Transformer-based Localization from Embodied Dialog with Large-scale Pre-training, [[Paper]](https://arxiv.org/pdf/2210.04864.pdf)
- (arXiv 2022.10) AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization, [[Paper]](https://arxiv.org/pdf/2210.05060.pdf)
- (arXiv 2022.10) Understanding Embodied Reference with Touch-Line Transformer, [[Paper]](https://arxiv.org/pdf/2210.05668.pdf)
- (arXiv 2022.10) Foundation Transformers, [[Paper]](https://arxiv.org/pdf/2210.06423.pdf)
- (arXiv 2022.10) PedFormer: Pedestrian Behavior Prediction via Cross-Modal Attention Modulation and Gated Multitask Learning, [[Paper]](https://arxiv.org/pdf/2210.07886.pdf)
- (arXiv 2022.10) Multimodal Image Fusion based on Hybrid CNN-Transformer and Non-local Cross-modal Attention, [[Paper]](https://arxiv.org/pdf/2210.09847.pdf), [[Code]](https://github.com/pandayuanyu/HCFusion)
- (arXiv 2022.10) Multi-Source Transformer Architectures for Audiovisual Scene Classification, [[Paper]](https://arxiv.org/pdf/2210.10212.pdf)
- (arXiv 2022.10) Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies, [[Paper]](https://arxiv.org/pdf/2210.12079.pdf)
- (arXiv 2022.10) M3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design, [[Paper]](https://arxiv.org/pdf/2210.14793.pdf), [[Code]](https://github.com/VITA-Group/M3ViT)
- (arXiv 2022.10) TAMFormer: Multi-Modal Transformer with Learned Attention Mask for Early Intent Prediction, [[Paper]](https://arxiv.org/pdf/2210.14714.pdf), [[Code]](https://github.com/VITA-Group/M3ViT)
- (arXiv 2022.10) Multimodal Transformer Distillation for Audio-Visual Synchronization, [[Paper]](https://arxiv.org/pdf/2210.15563.pdf)
- (arXiv 2022.10) Masked Vision-Language Transformer in Fashion, [[Paper]](https://arxiv.org/pdf/2210.15110.pdf), [[Code]](https://github.com/GewelsJI/MVLT)
- (arXiv 2022.10) Multimodal Transformer for Parallel Concatenated Variational Autoencoders, [[Paper]](https://arxiv.org/pdf/2210.16174.pdf)
- (arXiv 2022.10) RCDPT: Radar-Camera fusion Dense Prediction Transformer, [[Paper]](https://arxiv.org/pdf/2211.02432.pdf)
- (arXiv 2022.11) Efficient Joint Detection and Multiple Object Tracking with Spatially Aware Transformer, [[Paper]](https://arxiv.org/pdf/2211.05654.pdf)
- (arXiv 2022.11) OneFormer: One Transformer to Rule Universal Image Segmentation, [[Paper]](https://arxiv.org/pdf/2211.06220.pdf), [[Code]](https://github.com/SHI-Labs/OneFormer)
- (arXiv 2022.11) TransCC: Transformer-based Multiple Illuminant Color Constancy Using Multitask Learning, [[Paper]](https://arxiv.org/pdf/2211.08772.pdf)
- (arXiv 2022.11) Unifying Vision-Language Representation Space with Single-tower Transformer, [[Paper]](https://arxiv.org/pdf/2211.11153.pdf)
- (arXiv 2022.11) Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion, [[Paper]](https://arxiv.org/pdf/2205.02357.pdf), [[Code]](https://github.com/zjunlp/MKGformer)
- (arXiv 2022.12) Multimodal Vision Transformers with Forced Attention for Behavior Analysis, [[Paper]](https://arxiv.org/pdf/2212.03968.pdf)
- (arXiv 2022.12) Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers, [[Paper]](https://arxiv.org/pdf/2212.04970.pdf), [[Code]](https://hangz-nju-cuhk.github.io/projects/AV-CAT)
- (arXiv 2022.12) Hierarchical multimodal transformers for Multi-Page DocVQA, [[Paper]](https://arxiv.org/pdf/2212.05935.pdf)
- (arXiv 2022.12) Vision Transformers are Parameter-Efficient Audio-Visual Learners, [[Paper]](https://arxiv.org/pdf/2212.07983.pdf), [[Code]](https://genjib.github.io/project_page/LAVISH/)
- (arXiv 2022.12) Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program, [[Paper]](https://arxiv.org/pdf/2212.12952.pdf)
- (arXiv 2023.01) Cross Modal Transformer via Coordinates Encoding for 3D Object Dectection, [[Paper]](https://arxiv.org/pdf/2301.01283.pdf), [[Code]](https://github.com/junjie18/CMT)
- (arXiv 2023.01) DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction, [[Paper]](https://arxiv.org/pdf/2301.03461.pdf), [[Code]](https://github.com/yangyangxu0/DeMT)
- (arXiv 2023.01) Multi-scale multi-modal micro-expression recognition algorithm based on transformer, [[Paper]](https://arxiv.org/pdf/2301.02969.pdf)
- (arXiv 2023.01) Logically at Factify 2023: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture, [[Paper]](https://arxiv.org/pdf/2301.03127.pdf)
- (arXiv 2023.01) ViTs for SITS: Vision Transformers for Satellite Image ries, [[Paper]](https://arxiv.org/pdf/2301.04944.pdf)
- (arXiv 2023.01) Zorro: the masked multimodal transformer, [[Paper]](https://arxiv.org/pdf/2301.09595.pdf)
- (arXiv 2023.01) Multimodal Event Transformer for Image-guided Story Ending Generation, [[Paper]](https://arxiv.org/pdf/2301.11357.pdf)
- (arXiv 2023.01) UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers, [[Paper]](https://arxiv.org/pdf/2301.13741.pdf)
- (arXiv 2023.02) Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing, [[Paper]](https://arxiv.org/pdf/2302.05744.pdf)
- (arXiv 2023.02) ViM: Vision Middleware for Unified Downstream Transferring, [[Paper]](https://arxiv.org/pdf/2303.06911.pdf)
- (arXiv 2023.03) One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale, [[Paper]](https://arxiv.org/pdf/2303.06555.pdf), [[Code]](https://github.com/thu-ml/unidiffuser)
- (arXiv 2023.03) MAGVLT: Masked Generative Vision-and-Language Transformer, [[Paper]](https://arxiv.org/pdf/2303.12208.pdf)
- (arXiv 2023.03) LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR Perception, [[Paper]](https://arxiv.org/pdf/2303.12194.pdf)
- (arXiv 2023.03) MMFormer: Multimodal Transformer Using Multiscale Self-Attention for Remote Sensing Image Classification, [[Paper]](https://arxiv.org/pdf/2303.13101.pdf)
- (arXiv 2023.04) Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification, [[Paper]](https://arxiv.org/pdf/2304.02836.pdf)
- (arXiv 2023.04) PARFormer: Transformer-based Multi-Task Network for Pedestrian Attribute Recognition, [[Paper]](https://arxiv.org/pdf/2304.07230.pdf), [[Code]](https://github.com/xwf199/PARFormer)
- (arXiv 2023.04) AutoTaskFormer: Searching Vision Transformers for Multi-task Learning, [[Paper]](https://arxiv.org/pdf/2304.08756.pdf)
- (arXiv 2023.05) MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer, [[Paper]](https://arxiv.org/pdf/2305.00355.pdf), [[Code]](https://github.com/YoucanBaby/MH-DETR)
- (arXiv 2023.05) MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis, [[Paper]](https://arxiv.org/pdf/2305.05992.pdf), [[Project]](https://jabir-zheng.github.io/MMoT)
- (arXiv 2023.05) JOINEDTrans: Prior Guided Multi-task Transformer for Joint Optic Disc/Cup Segmentation and Fovea Detection, [[Paper]](https://arxiv.org/pdf/2305.11504.pdf), [[Project]](https://github.com/HuaqingHe/JOINEDTrans)
- (arXiv 2023.05) Brain encoding models based on multimodal transformers can transfer across language and vision, [[Paper]](https://arxiv.org/pdf/2305.12248.pdf)
- (arXiv 2023.05) CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers, [[Paper]](https://arxiv.org/pdf/2305.17455.pdf), [[Code]](https://github.com/sdc17/CrossGET)
- (arXiv 2023.05) Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts, [[Paper]](https://arxiv.org/pdf/2305.18691.pdf), [[Code]](https://github.com/sharc-lab/Edge-MoE)
- (arXiv 2023.06) Transformer-based Multi-Modal Learning for Multi Label Remote Sensing Image Classification, [[Paper]](https://arxiv.org/pdf/2306.01523.pdf), [[Code]](https://git.tu-berlin.de/rsim/sct-fusion)
- (arXiv 2023.06) Energy-Based Models for Cross-Modal Localization using Convolutional Transformers, [[Paper]](https://arxiv.org/pdf/2306.04021.pdf)
- (arXiv 2023.06) Efficient Multi-Task Scene Analysis with RGB-D Transformers, [[Paper]](https://arxiv.org/pdf/2306.05242.pdf), [[Code]](https://github.com/TUI-NICR/EMSAFormer)
- (arXiv 2023.06) ContentCTR: Frame-level Live Streaming Click-Through Rate Prediction with Multimodal Transformer, [[Paper]](https://arxiv.org/pdf/2306.14392.pdf)
- (arXiv 2023.07) End-To-End Prediction of Knee Osteoarthritis Progression With Multi-Modal Transformers, [[Paper]](https://arxiv.org/pdf/2307.00873.pdf)
- (arXiv 2023.07) Interactive Image Segmentation with Cross-Modality Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.02280.pdf), [[Code]](https://github.com/lik1996/iCMFormer)
- (arXiv 2023.07) TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation, [[Paper]](https://arxiv.org/pdf/2307.08051.pdf), [[Code]](https://github.com/zhenqi-he/transnuseg)
- (arXiv 2023.07) Meta-Transformer: A Unified Framework for Multimodal Learning, [[Paper]](https://arxiv.org/pdf/2307.10802.pdf), [[Project]](https://kxgong.github.io/meta_transformer/)
- (arXiv 2023.07) ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer, [[Paper]](https://arxiv.org/pdf/2307.12349.pdf), [[Code]](https://github.com/lartpang/ComPtr)
- (arXiv 2023.07) Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation, [[Paper]](https://arxiv.org/pdf/2307.13236.pdf)
- (arXiv 2023.07) Prompt Guided Transformer for Multi-Task Dense Prediction, [[Paper]](https://arxiv.org/pdf/2307.15362.pdf)
- (arXiv 2023.07) Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics, [[Paper]](https://arxiv.org/pdf/2307.16005)
- (arXiv 2023.08) FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2308.01006.pdf)
- (arXiv 2023.08) Multimodal Neurons in Pretrained Text-Only Transformers, [[Paper]](https://arxiv.org/pdf/2308.01544.pdf), [[Project]](https://mmns.csail.mit.edu/)
- (arXiv 2023.08) A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models, [[Paper]](https://arxiv.org/pdf/2308.01328.pdf)
- (arXiv 2023.08) 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment, [[Paper]](https://arxiv.org/pdf/2308.04352.pdf), [[Project]](https://3d-vista.github.io/)
- (arXiv 2023.08) Vision Transformer Adapters for Generalizable Multitask Learning, [[Paper]](https://arxiv.org/pdf/2308.12372.pdf), [[Code]](https://ivrl.github.io/VTAGML)
- (arXiv 2023.08) UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization, [[Paper]](https://arxiv.org/pdf/2308.14395.pdf), [[Code]](https://github.com/ymhzyj/UMMAFormer/)
- (arXiv 2023.09) Exchanging-based Multimodal Fusion with Transformer, [[Paper]](https://arxiv.org/pdf/2309.02190.pdf), [[Code]](https://github.com/RecklessRonan/MuSE)
- (arXiv 2023.09) Multimodal Transformer for Material Segmentation, [[Paper]](https://arxiv.org/pdf/2309.04001.pdf), [[Code]](https://github.com/csiplab/MMSFormer)
- (arXiv 2023.09) Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens, [[Paper]](https://arxiv.org/pdf/2309.08531.pdf)
- (arXiv 2023.09) MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer, [[Paper]](https://arxiv.org/pdf/2309.09067.pdf), [[Code]](https://github.com/fudong03/MMST-ViT)
- (arXiv 2023.09) Unified Frequency-Assisted Transformer Framework for Detecting and Grounding Multi-Modal Manipulation, [[Paper]](https://arxiv.org/pdf/2309.09667.pdf)
- (arXiv 2023.09) Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer, [[Paper]](https://arxiv.org/pdf/2309.14704.pdf)
- (arXiv 2023.10) LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for Autonomous Driving with Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2310.13135.pdf), [[Code]](https://github.com/pagand/e2etransfuser/tree/cvpr-w)
- (arXiv 2023.10) 3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction, [[Paper]](https://arxiv.org/pdf/2310.14859.pdf)
- (arXiv 2023.10) MMTF-DES: A Fusion of Multimodal Transformer Models for Desire, Emotion, and Sentiment Analysis of Social Media Data, [[Paper]](https://arxiv.org/pdf/2310.14143.pdf)
- (arXiv 2023.11) Learning A Multi-Task Transformer Via Unified And Customized Instruction Tuning For Chest Radiograph Interpretation, [[Paper]](https://arxiv.org/pdf/2311.01092.pdf)
- (arXiv 2023.11) Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization, [[Paper]](https://arxiv.org/pdf/2311.03785.pdf)
- (arXiv 2023.11) PolyMaX: General Dense Prediction with Mask Transformer, [[Paper]](https://arxiv.org/abs/2311.05770)
- (arXiv 2023.11) Vision-Language Integration in Multimodal Video Transformers (Partially) Aligns with the Brain, [[Paper]](https://arxiv.org/pdf/2311.07766.pdf)
- (arXiv 2023.11) Language Grounded QFormer for Efficient Vision Language Understanding, [[Paper]](https://arxiv.org/pdf/2311.07449.pdf)
- (arXiv 2023.11) DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models,  [[Paper]](https://arxiv.org/pdf/2311.08623.pdf)
- (arXiv 2023.11) VIT-LENS-2: Gateway to Omni-modal Intelligence,  [[Paper]](https://arxiv.org/pdf/2311.16081.pdf), [[Code]](https://github.com/TencentARC/ViT-Lens锛?
- (arXiv 2023.11) You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception,  [[Paper]](https://arxiv.org/pdf/2312.05525.pdf)
- (arXiv 2023.12) VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation,  [[Paper]](https://arxiv.org/pdf/2312.09251.pdf)
- (arXiv 2024.01) Multimodal Informative ViT: Information Aggregation and Distribution for Hyperspectral and LiDAR Classification,  [[Paper]](https://arxiv.org/pdf/2401.03179.pdf), [[Code]](https://github.com/icey-zhang/MIViT)
- (arXiv 2024.01) SeTformer is What You Need for Vision and Language,  [[Paper]](https://arxiv.org/pdf/2401.03540.pdf)
- (arXiv 2024.01) Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities,  [[Paper]](https://arxiv.org/pdf/2401.14405.pdf), [[Code]](https://github.com/AILab-CVC/M2PT)
- (arXiv 2024.01) Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition,  [[Paper]](https://arxiv.org/pdf/2401.17604.pdf)
- (arXiv 2024.02) Question Aware Vision Transformer for Multimodal Reasoning,  [[Paper]](https://arxiv.org/pdf/2402.05472.pdf)
- (arXiv 2024.02) A Touch, Vision, and Language Dataset for Multimodal Alignment,  [[Paper]](https://arxiv.org/pdf/2402.13232.pdf), [[Code]](https://tactile-vlm.github.io/)
- (arXiv 2024.02) Multimodal Transformer With a Low-Computational-Cost Guarantee,  [[Paper]](https://arxiv.org/pdf/2402.15096.pdf)
- (arXiv 2024.03) Task Indicating Transformer for Task-conditional Dense Predictions,  [[Paper]](https://arxiv.org/pdf/2403.00327.pdf)
- (arXiv 2024.03) Multimodal Transformer for Comics Text-Cloze, [[Paper]](https://arxiv.org/pdf/2403.03719.pdf)
- (arXiv 2024.03) MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer, [[Paper]](https://arxiv.org/pdf/2403.02991.pdf), [[Code]](https://github.com/double125/MADTP)
- (arXiv 2024.03) GiT: Towards Generalist Vision Transformer through Universal Language Interface, [[Paper]](https://arxiv.org/pdf/2403.09394.pdf), [[Code]](https://github.com/Haiyang-W/GiT)
- (arXiv 2024.03) Uni-SMART: Universal Science Multimodal Analysis and Research Transformer, [[Paper]](https://arxiv.org/pdf/2403.10301.pdf)
- (arXiv 2024.03) Affective Behaviour Analysis via Integrating Multi-Modal Knowledge, [[Paper]](https://arxiv.org/pdf/2403.10825.pdf)
- (arXiv 2024.03) M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2403.12552.pdf), [[Code]](https://anonymous.4open.science/r/M2DA-4772/)

### Multi-view Stereo
- (arXiv 2021.11) TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers, [[Paper]](https://arxiv.org/pdf/2111.14600.pdf), [[Code]](https://github.com/MegviiRobot/TransMVSNet)
- (arXiv 2021.12) Multi-View Stereo with Transformer, [[Paper]](https://arxiv.org/pdf/2112.00336.pdf)
- (arXiv 2022.04) MVSTER: Epipolar Transformer for Efficient Multi-View Stereo, [[Paper]](https://arxiv.org/pdf/2204.07346.pdf), [[Code]](https://github.com/JeffWang987)
- (arXiv 2022.05) WT-MVSNet: Window-based Transformers for Multi-view Stereo, [[Paper]](https://arxiv.org/pdf/2205.14319.pdf), [[Code]](https://github.com/JeffWang987)
- (arXiv 2022.08) MVSFormer: Learning Robust Image Representations via Transformers and Temperature-based Depth for Multi-View Stereo, [[Paper]](https://arxiv.org/pdf/2208.02541.pdf)
- (arXiv 2022.08) A Light Touch Approach to Teaching Transformers Multi-view Geometry, [[Paper]](https://arxiv.org/pdf/2211.15107.pdf)
- (arXiv 2023.03) Implicit Ray-Transformers for Multi-view Remote Sensing Image Segmentation, [[Paper]](https://arxiv.org/pdf/2303.08401.pdf)
- (arXiv 2023.05) CostFormer:Cost Transformer for Cost Aggregation in Multi-view Stereo, [[Paper]](https://arxiv.org/pdf/2305.10320.pdf)
- (arXiv 2023.10) GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers, [[Paper]](https://arxiv.org/pdf/2310.10375.pdf)
- (arXiv 2023.12) CT-MVSNet: Efficient Multi-View Stereo with Cross-scale Transformer, [[Paper]](https://arxiv.org/pdf/2312.08594.pdf), [[Code]](https://github.com/wscstrive/CT-MVSNet)
- (arXiv 2023.12) Global Occlusion-Aware Transformer for Robust Stereo Matching, [[Paper]](https://arxiv.org/pdf/2312.14650.pdf), [[Code]](https://github.com/Magicboomliu/GOAT)

### NAS
- (CVPR'21) HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers, [[Paper]](https://arxiv.org/pdf/2106.06560.pdf), [[Code]](https://github.com/dingmyu/HR-NAS)
- (arXiv.2021.02) Towards Accurate and Compact Architectures via Neural Architecture Transformer, [[Paper]](https://arxiv.org/pdf/2102.10301.pdf)
- (arXiv.2021.03) BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search, [[Paper]](https://arxiv.org/abs/2103.12424), [[Code]](https://github.com/changlin31/BossNAS)
- (arXiv.2021.06) Vision Transformer Architecture Search, [[Paper]](https://arxiv.org/pdf/2106.13700.pdf), [[Code]](https://github.com/xiusu/ViTAS)
- (arXiv.2021.07) AutoFormer: Searching Transformers for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2107.00651.pdf), [[Code]](https://github.com/microsoft/AutoML)
- (arXiv.2021.07) GLiT: Neural Architecture Search for Global and Local Image Transformer, [[Paper]](https://arxiv.org/pdf/2107.02960.pdf)
- (arXiv.2021.09) Searching for Efficient Multi-Stage Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.00642.pdf)
- (arXiv.2021.10) UniNet: Unified Architecture Search with Convolution, Transformer, and MLP, [[Paper]](https://arxiv.org/pdf/2110.04035.pdf)
- (arXiv.2021.11) Searching the Search Space of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.14725.pdf), [[Code]](https://github.com/microsoft/Cream)
- (arXiv.2022.01) Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space, [[Paper]](https://arxiv.org/pdf/2201.00814.pdf)
- (arXiv.2022.03) Vision Transformer with Convolutions Architecture Search, [[Paper]](https://arxiv.org/pdf/2203.10435.pdf)
- (arXiv.2022.03) Training-free Transformer Architecture Search, [[Paper]](https://arxiv.org/pdf/2203.12217.pdf)
- (arXiv.2022.06) Neural Prompt Search, [[Paper]](https://arxiv.org/pdf/2206.04673.pdf)
- (arXiv.2022.07) UniNet: Unified Architecture Search with Convolution, Transformer, and MLP, [[Paper]](https://arxiv.org/pdf/2207.05420.pdf), [[Code]](https://github.com/Sense-X/UniNet)
- (arXiv.2022.09) NasHD: Efficient ViT Architecture Performance Ranking using Hyperdimensional Computing, [[Paper]](https://arxiv.org/pdf/2209.11356.pdf)
- (arXiv.2022.11) NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction, [[Paper]](https://arxiv.org/pdf/2211.08024.pdf)
- (arXiv 2023.03) HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices, [[Paper]](https://arxiv.org/pdf/2303.04440.pdf), [[Code]](https://anonymous.4open.science/r/HyT-NAS-Search-Algorithm-A864/README.md)
- (arXiv 2023.07) AutoST: Training-free Neural Architecture Search for Spiking Transformers, [[Paper]](https://arxiv.org/pdf/2307.00293.pdf)
- (arXiv 2023.08) TurboViT: Generating Fast Vision Transformers via Generative Architecture Search, [[Paper]](https://arxiv.org/pdf/2308.11421.pdf)
- (arXiv 2023.11) FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2311.03912.pdf), [[Code]](https://github.com/shadowpa0327/FLORA)
- (arXiv 2023.11) TVT: Training-Free Vision Transformer Search on Tiny Datasets, [[Paper]](https://arxiv.org/pdf/2311.14337.pdf)
- (arXiv 2023.12) Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery, [[Paper]](https://arxiv.org/pdf/2312.09059.pdf)

### Navigation
- (ICLR'21) VTNet: Visual Transformer Network for Object Goal Navigation, [[Paper]](https://arxiv.org/pdf/2105.09447.pdf)
- (arXiv 2021.03) MaAST: Map Attention with Semantic Transformers for Efficient Visual Navigation, [[Paper]](https://arxiv.org/pdf/2103.11374.pdf)
- (arXiv 2021.04) Know What and Know Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation, [[Paper]](https://arxiv.org/pdf/2104.04167.pdf)
- (arXiv 2021.05) Episodic Transformer for Vision-and-Language Navigation, [[Paper]](https://arxiv.org/pdf/2105.06453.pdf)
- (arXiv 2021.07) Trans4Trans: Efficient Transformer for Transparent Object Segmentation to Help Visually Impaired People Navigate in the Real World, [[Paper]](https://arxiv.org/pdf/2107.03172.pdf)
- (arXiv 2021.10) SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation, [[Paper]](https://arxiv.org/pdf/2110.14143.pdf)
- (arXiv 2021.10) History Aware Multimodal Transformer for Vision-and-Language Navigation, [[Paper]](https://arxiv.org/pdf/2110.13309.pdf), [[Code]](https://cshizhe.github.io/projects/vln_hamt.html)
- (arXiv 2021.11) Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation, [[Paper]](https://arxiv.org/pdf/2111.05759.pdf)
- (arXiv 2022.02) Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation, [[Paper]](https://arxiv.org/pdf/2202.11742.pdf), [[Project]](https://cshizhe.github.io/projects/vln_duet.html)
- (arXiv 2022.03) Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.03682.pdf), [[Project]](https://sachamorin.github.io/dino/)
- (arXiv 2022.03) Object Memory Transformer for Object Goal Navigation, [[Paper]](https://arxiv.org/pdf/2203.14708.pdf)
- (arXiv 2022.07) Target-Driven Structured Transformer Planner for Vision-Language Navigation, [[Paper]](https://arxiv.org/pdf/2207.11201.pdf), [[Code]](https://github.com/YushengZhao/TD-STP)
- (arXiv 2023.05) ASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation,  [[Paper]](https://arxiv.org/pdf/2305.11918.pdf)
- (arXiv 2023.06) ViNT: A Foundation Model for Visual Navigation, [[Paper]](https://arxiv.org/pdf/2306.14846.pdf), [[Code]](https://visualnav-transformer.github.io/)
- (arXiv 2023.07) GridMM: Grid Memory Map for Vision-and-Language Navigation, [[Paper]](https://arxiv.org/pdf/2307.12907.pdf), [[Code]](https://github.com/MrZihan/GridMM)
- (arXiv 2023.08) Bird鈥檚-Eye-View Scene Graph for Vision-Language Navigation, [[Paper]](https://arxiv.org/pdf/2308.04758.pdf), [[Code]](https://github.com/DefaultRui/BEV-Scene-Graph)
- (arXiv 2023.08) Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation, [[Paper]](https://arxiv.org/pdf/2308.11561.pdf), [[Code]](https://github.com/yifeisu/avdn-challenge)
- (arXiv 2023.11) Navigating Scaling Laws: Accelerating Vision Transformer's Training via Adaptive Strategies, [[Paper]](https://arxiv.org/pdf/2311.03314.pdf)

### Neural Rendering
- (arXiv 2022.03) ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers, [[Paper]](https://arxiv.org/pdf/2203.10157.pdf), [[Code]](https://github.com/jkulhanek/viewformer)
- (arXiv 2022.06) Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer, [[Paper]](https://arxiv.org/pdf/2206.05375.pdf)
- (arXiv 2022.06) IRISformer: Dense Vision Transformers for Single-Image Inverse Rendering inIndoor Scenes, [[Paper]](https://arxiv.org/pdf/2206.08423.pdf), [[Code]](https://github.com/ViLab-UCSD/IRISformer)
- (arXiv 2022.07) Vision Transformer for NeRF-Based View Synthesis from a Single Input Image, [[Paper]](https://arxiv.org/pdf/2207.05736.pdf), [[Project]](https://cseweb.ucsd.edu/~viscomp/projects/VisionNeRF/)
- (arXiv 2022.09) NeRF-Loc: Transformer-Based Object Localization Within Neural Radiance Fields, [[Paper]](https://arxiv.org/pdf/2209.12068.pdf)
- (arXiv 2023.03) Single-view Neural Radiance Fields with Depth Teacher, [[Paper]](https://arxiv.org/pdf/2303.09952.pdf)
- (arXiv 2024.01) CTNeRF: Cross-Time Transformer for Dynamic Neural Radiance Field from Monocular Video, [[Paper]](https://arxiv.org/pdf/2401.04861.pdf)

### OCR
- (arXiv 2021.04) Handwriting Transformers, [[Paper]](https://arxiv.org/abs/2104.03964)
- (arXiv 2021.05) I2C2W: Image-to-Character-to-Word Transformers for Accurate Scene Text Recognition, [[Paper]](https://arxiv.org/pdf/2105.08383.pdf)
- (arXiv 2021.05) Vision Transformer for Fast and Efficient Scene Text Recognition, [[Paper]](https://arxiv.org/pdf/2105.08582.pdf)
- (arXiv 2021.06) DocFormer: End-to-End Transformer for Document Understanding, [[Paper]](https://arxiv.org/pdf/2106.11539.pdf)
- (arXiv 2021.08) A Transformer-based Math Language Model for Handwritten Math Expression Recognition, [[Paper]](https://arxiv.org/pdf/2108.05002.pdf)
- (arXiv 2021.09) TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models, [[Paper]](https://arxiv.org/pdf/2109.10282.pdf), [[Code]](https://aka.ms/TrOCR)
- (arXiv 2021.10) Robustness Evaluation of Transformer-based Form Field Extractors via Form Attacks, [[Paper]](https://arxiv.org/pdf/2110.04413.pdf), [[Code]](https://aka.ms/TrOCR)
- (arXiv 2021.10) DocTr: Document Image Transformer for Geometric Unwarping and Illumination Correction, [[Paper]](https://arxiv.org/pdf/2110.12942.pdf)
- (arXiv 2021.12) Visual-Semantic Transformer for Scene Text Recognition, [[Paper]](https://arxiv.org/pdf/2112.00948.pdf)
- (arXiv 2021.12) Transformer-Based Approach for Joint Handwriting and Named Entity Recognition in Historical documents, [[Paper]](https://arxiv.org/pdf/2112.04189.pdf)
- (arXiv 2021.12) SPTS: Single-Point Text Spotting, [[Paper]](https://arxiv.org/pdf/2112.07917.pdf)
- (arXiv 2022.02) Arbitrary Shape Text Detection using Transformers, [[Paper]](https://arxiv.org/pdf/2202.11221.pdf)
- (arXiv 2022.03) DiT: Self-supervised Pre-training for Document Image Transformer, [[Paper]](https://arxiv.org/pdf/2203.02378.pdf), [[Code]](https://aka.ms/msdit)
- (arXiv 2022.03) TrueType Transformer: Character and Font Style Recognition in Outline Format, [[Paper]](https://arxiv.org/pdf/2203.05338.pdf)
- (arXiv 2022.03) SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition, [[Paper]](https://arxiv.org/pdf/2203.10209.pdf), [[Code]](https://github.com/mxin262/SwinTextSpotter)
- (arXiv 2022.03) Transformer-based HTR for Historical Documents, [[Paper]](https://arxiv.org/pdf/2203.11008.pdf)
- (arXiv 2022.04) Text Spotting Transformers, [[Paper]](https://arxiv.org/pdf/2204.01918.pdf), [[Code]](https://github.com/mlpc-ucsd/TESTR)
- (arXiv 2022.05) Arbitrary Shape Text Detection via Boundary Transformer, [[Paper]](https://arxiv.org/pdf/2205.05320.pdf), [[Code]](https://github.com/GXYM/TextBPN-Puls-Plus)
- (arXiv 2022.05) MATrIX - Modality-Aware Transformer for Information eXtraction, [[Paper]](https://arxiv.org/pdf/2205.08094.pdf)
- (arXiv 2022.06) Transformer based Urdu Handwritten Text Optical Character Reader, [[Paper]](https://arxiv.org/pdf/2206.04575.pdf)
- (arXiv 2022.06) SVG Vector Font Generation for Chinese Characters with Transformer, [[Paper]](https://arxiv.org/pdf/2206.10329.pdf)
- (arXiv 2022.07) DPText-DETR: Towards Better Scene Text Detection with Dynamic Points in Transformer, [[Paper]](https://arxiv.org/pdf/2207.04491.pdf), [[Code]](https://github.com/ymy-k/DPText-DETR)
- (arXiv 2022.07) CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition, [[Paper]](https://arxiv.org/pdf/2207.04410.pdf), [[Code]](https://github.com/Green-Wood/CoMER)
- (arXiv 2022.08) Toward Understanding WordArt: Corner-Guided Transformer for Scene Text Recognition, [[Paper]](https://arxiv.org/pdf/2208.00438.pdf), [[Code]](https://github.com/xdxie/WordArt)
- (arXiv 2022.08) DPTNet: A Dual-Path Transformer Architecture for Scene Text Detection, [[Paper]](https://arxiv.org/pdf/2208.09878.pdf)
- (arXiv 2022.08) Offline Handwritten Mathematical Recognition using Adversarial Learning and Transformers, [[Paper]](https://arxiv.org/pdf/2208.09662.pdf)
- (arXiv 2022.08) An End-to-End OCR Framework for Robust Arabic-Handwriting Recognition using a Novel Transformers-based Model and an Innovative 270 Million-Words Multi-Font Corpus of Classical Arabic with Diacritics, [[Paper]](https://arxiv.org/pdf/2208.11484.pdf)
- (arXiv 2022.08) TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers, [[Paper]](https://arxiv.org/pdf/2208.14687.pdf)
- (arXiv 2022.09) ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding, [[Paper]](https://arxiv.org/pdf/2210.07546.pdf)
- (arXiv 2022.11) A Transformer Architecture for Online Gesture Recognition of Mathematical Expressions, [[Paper]](https://arxiv.org/pdf/2211.02643.pdf)
- (arXiv 2022.11) Masked Vision-Language Transformers for Scene Text Recognition, [[Paper]](https://arxiv.org/pdf/2211.04785.pdf), [[Code]](https://github.com/onealwj/MVLT)
- (arXiv 2022.11) Pure Transformer with Integrated Experts for Scene Text Recognition, [[Paper]](https://arxiv.org/pdf/2211.04963.pdf)
- (arXiv 2022.11) DeepSolo: Let Transformer Decoder with Explicit Points Solo for Text Spotting, [[Paper]](https://arxiv.org/pdf/2211.10772.pdf)
- (arXiv 2022.11) Aggregated Text Transformer for Scene Text Detection, [[Paper]](https://arxiv.org/pdf/2211.13984.pdf)
- (arXiv 2023.03) Robust Table Structure Recognition with Dynamic Queries Enhanced Detection Transformer, [[Paper]](https://arxiv.org/pdf/2303.11615.pdf)
- (arXiv 2023.03) MSdocTr-Lite: A Lite Transformer for Full Page Multi-script Handwriting Recognition, [[Paper]](https://arxiv.org/pdf/2303.13931.pdf)
- (arXiv 2023.03) DeepVecFont-v2: Exploiting Transformers to Synthesize Vector Fonts with Higher Quality, [[Paper]](https://arxiv.org/pdf/2303.14585.pdf)
- (arXiv 2023.05) Towards End-to-End Semi-Supervised Table Detection with Deformable Transformer, [[Paper]](https://arxiv.org/pdf/2305.02769.pdf)
- (arXiv 2023.05) Fast-StrucTexT: An Efficient Hourglass Transformer with Modality-guided Dynamic Token Merge for Document Understanding, [[Paper]](https://arxiv.org/pdf/2305.11392.pdf)
- (arXiv 2023.05) Quantifying Character Similarity with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.14672.pdf)
- (arXiv 2023.05) DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Text Spotting, [[Paper]](https://arxiv.org/pdf/2305.19957.pdf), [[Code]](https://github.com/ViTAE-Transformer/DeepSolo)
- (arXiv 2023.06) DocFormerv2: Local Features for Document Understanding, [[Paper]](https://arxiv.org/pdf/2306.01733.pdf)
- (arXiv 2023.06) Transformer-Based UNet with Multi-Headed Cross-Attention Skip Connections to Eliminate Artifacts in Scanned Documents, [[Paper]](https://arxiv.org/pdf/2306.02815.pdf)
- (arXiv 2023.06) TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision, [[Paper]](https://arxiv.org/pdf/2306.03377.pdf)
- (arXiv 2023.06) Exploring Transformers for On-Line Handwritten Signature Verification, [[Paper]](https://arxiv.org/pdf/2307.01663.pdf)
- (arXiv 2023.07) DocTr: Document Transformer for Structured Information Extraction in Documents, [[Paper]](https://arxiv.org/pdf/2307.07929.pdf)
- (arXiv 2023.07) A Transformer-based Approach for Arabic Offline Handwritten Text Recognition, [[Paper]](https://arxiv.org/pdf/2307.15045.pdf)
- (arXiv 2023.08) ChartDETR: A Multi-shape Detection Network for Visual Chart Recognition, [[Paper]](https://arxiv.org/pdf/2308.07743.pdf)
- (arXiv 2023.08) SRFormer: Empowering Regression-Based Text Detection Transformer with Segmentation, [[Paper]](https://arxiv.org/pdf/2308.10531.pdf), [[Code]](https://github.com/retsuh-bqw/SRFormer-Text-Det)
- (arXiv 2023.08) ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer, [[Paper]](https://arxiv.org/pdf/2308.10147.pdf), [[Code]](https://github.com/mxin262/ESTextSpotter)
- (arXiv 2023.08) PBFormer: Capturing Complex Scene Text Shape with Polynomial Band Transformer, [[Paper]](https://arxiv.org/pdf/2308.15004.pdf)
- (arXiv 2023.08) DTrOCR: Decoder-only Transformer for Optical Character Recognition, [[Paper]](https://arxiv.org/pdf/2308.15996.pdf)
- (arXiv 2023.09) Character Queries: A Transformer-based Approach to On-Line Handwritten Character Segmentation, [[Paper]](https://arxiv.org/pdf/2309.03072.pdf), [[Code]](https://github.com/jungomi/character-queries)
- (arXiv 2023.09) ShaDocFormer: A Shadow-attentive Threshold Detector with Cascaded Fusion Refiner for document shadow removal' to the ICASSP 2024 online submission system, [[Paper]](https://arxiv.org/pdf/2309.06670.pdf)
- (arXiv 2023.10) DocStormer: Revitalizing Multi-Degraded Colored Document Images to Pristine PDF, [[Paper]](https://arxiv.org/pdf/2310.17910.pdf)
- (arXiv 2023.11) High-Performance Transformers for Table Structure Recognition Need Early Convolutions, [[Paper]](https://arxiv.org/pdf/2311.05565.pdf), [[Code]](https://github.com/poloclub/tsr-convstem)
- (arXiv 2023.11) Vulnerability Analysis of Transformer-based Optical Character Recognition to Adversarial Attacks, [[Paper]](https://arxiv.org/pdf/2311.17128.pdf)
- (arXiv 2023.12) DocBinFormer: A Two-Level Transformer Network for Effective Document Image Binarization, [[Paper]](https://arxiv.org/pdf/2312.03568.pdf),[[Code]](https://github.com/RisabBiswas/DocBinFormer)
- (arXiv 2024.01) STR-Cert: Robustness Certification for Deep Text Recognition on Deep Learning Pipelines and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.05338.pdf)
- (arXiv 2024.01) SwinTextSpotter v2: Towards Better Synergy for Scene Text Spotting, [[Paper]](https://arxiv.org/pdf/2401.07641.pdf),[[Code]](https://github.com/mxin262/SwinTextSpotterv2)
- (arXiv 2024.01) Dynamic Relation Transformer for Contextual Text Block Detection, [[Paper]](https://arxiv.org/pdf/2401.09232.pdf)
- (arXiv 2024.02) Text Role Classification in Scientific Charts Using Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2402.14579.pdf),[[Code]](https://github.com/hjkimk/text-role-classification)
- (arXiv 2024.02) Self-Supervised Pre-Training for Table Structure Recognition Transformer, [[Paper]](https://arxiv.org/pdf/2402.15578.pdf),[[Code]](https://github.com/poloclub/unitable)
- (arXiv 2024.03) LOCR: Location-Guided Transformer for Optical Character Recognition, [[Paper]](https://arxiv.org/pdf/2403.02127.pdf)

### Octree
- (arXiv 2021.11) Octree Transformer: Autoregressive 3D Shape Generation on Hierarchically Structured Sequences, [[Paper]](https://arxiv.org/pdf/2111.12480.pdf), [[Code]](https://github.com/orrzohar/PROB)
- (arXiv 2023.03) OcTr: Octree-based Transformer for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2303.12621.pdf)
- (arXiv 2023.05) OctFormer: Octree-based Transformers for 3D Point Clouds, [[Paper]](https://arxiv.org/pdf/2305.03045.pdf), [[Code]](https://wang-ps.github.io/octformer)

### Open World
- (arXiv 2022.03) Open Set Recognition using Vision Transformer with an Additional Detection Head, [[Paper]](https://arxiv.org/pdf/2203.08441.pdf)
- (arXiv 2022.06) OOD Augmentation May Be at Odds with Open-Set Recognition, [[Paper]](https://arxiv.org/pdf/2206.04242.pdf)
- (arXiv 2022.07) Scaling Novel Object Detection with Weakly Supervised Detection Transformers, [[Paper]](https://arxiv.org/pdf/2207.05205.pdf)
- (arXiv 2022.09) Pre-training image-language transformers for open-vocabulary tasks, [[Paper]](https://arxiv.org/pdf/2209.04372.pdf)
- (arXiv 2022.10) Transformer-Based Speech Synthesizer Attribution in an Open Set Scenario, [[Paper]](https://arxiv.org/pdf/2209.04372.pdf)
- (arXiv 2022.12) PROB: Probabilistic Objectness for Open World Object Detection, [[Paper]](https://arxiv.org/pdf/2212.01424.pdf), [[Code]](https://github.com/feiyang-cai/osr_vit)
- (arXiv 2022.12) Open World DETR: Transformer based Open World Object Detection, [[Paper]](https://arxiv.org/pdf/2212.02969.pdf)
- (arXiv 2023.01) CAT: LoCalization and IdentificAtion Cascade Detection Transformer for Open-World Object Detection, [[Paper]](https://arxiv.org/pdf/2301.01970.pdf), [[Code]](https://github.com/xiaomabufei/CAT)
- (arXiv 2023.03) Prompt-Guided Transformers for End-to-End Open-Vocabulary Object Detection, [[Paper]](https://arxiv.org/pdf/2303.14386.pdf)
- (arXiv 2023.05) Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.07011.pdf)
- (arXiv 2023.08) SegPrompt: Boosting Open-World Segmentation via Category-level Prompt Learning, [[Paper]](https://arxiv.org/pdf/2308.06531.pdf), [[Code]](https://github.com/aim-uofa/SegPrompt)
- (arXiv 2023.09) Contrastive Feature Masking Open-Vocabulary Vision Transformer, [[Paper]](https://arxiv.org/pdf/2309.00775.pdf)
- (arXiv 2023.09) Diffusion Model is Secretly a Training-free Open Vocabulary er, [[Paper]](https://arxiv.org/pdf/2309.02773.pdf)
- (arXiv 2023.09) Unsupervised Open-Vocabulary Object Localization in Videos, [[Paper]](https://arxiv.org/pdf/2309.09858.pdf), [[Code]](https://github.com/aim-uofa/SegPrompt)
- (arXiv 2023.10) CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2310.02960.pdf), [[Code]](https://github.com/yangcaoai/CoDA_NeurIPS2023)
- (arXiv 2023.11) Enhancing Novel Object Detection via Cooperative Foundational Models, [[Paper]](https://arxiv.org/pdf/2311.12068.pdf), [[Code]](https://github.com/rohit901/cooperative-foundational-models)
- (arXiv 2023.11) Language-conditioned Detection Transformer, [[Paper]](https://arxiv.org/pdf/2311.17902.pdf), [[Code]](https://github.com/janghyuncho/DECOLA)
- (arXiv 2023.12) Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object Detection, [[Paper]](https://arxiv.org/pdf/2312.02103.pdf)
- (arXiv 2023.12) Boosting Segment Anything Model Towards Open-Vocabulary Learning, [[Paper]](https://arxiv.org/pdf/2312.03628.pdf), [[Code]](https://github.com/ucas-vg/Sambor)
- (arXiv 2023.12) Open World Object Detection in the Era of Foundation Models, [[Paper]](https://arxiv.org/pdf/2312.05745.pdf), [[Code]](https://orrzohar.github.io/projects/fomo/)
- (arXiv 2024.02) Semi-supervised Open-World Object Detection, [[Paper]](https://arxiv.org/pdf/2402.16013.pdf), [[Code]](https://github.com/sahalshajim/SS-OWFormer)
- (arXiv 2024.03) CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2403.12455.pdf), [[Code]](https://github.com/zwq456/CLIP-VIS)

### Optical Flow
- (arXiv 2022.03) FlowFormer: A Transformer Architecture for Optical Flow, [[Paper]](https://arxiv.org/pdf/2203.16194.pdf), [[Project]](https://drinkingcoder.github.io/publication/flowformer/)
- (arXiv 2022.03) CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow, [[Paper]](https://arxiv.org/pdf/2203.16896.pdf), [[Code]](https://github.com/askerlee/craft)
- (arXiv 2023.03) FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation, [[Paper]](https://arxiv.org/pdf/2303.01237.pdf)
- (arXiv 2023.04) TransFlow: Transformer as Flow Learner, [[Paper]](https://arxiv.org/pdf/2304.11523.pdf)
- (arXiv 2023.05) SSTM: Spatiotemporal Recurrent Transformers for Multi-frame Optical Flow Estimation, [[Paper]](https://arxiv.org/pdf/2304.14418.pdf)
- (arXiv 2023.06) FlowFormer: A Transformer Architecture and Its Masked Cost Volume Autoencoding for Optical Flow, [[Paper]](https://arxiv.org/pdf/2306.05442.pdf)
- (arXiv 2024.03) CathFlow: Self-Supervised Segmentation of Catheters in Interventional Ultrasound Using Optical Flow and Transformers, [[Paper]](https://arxiv.org/pdf/2403.14465.pdf)

### Panoptic Segmentation
- (arXiv.2020.12) MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers, [[Paper]](https://arxiv.org/pdf/2012.00759.pdf)
- (arXiv 2021.09) Panoptic SegFormer, [[Paper]](https://arxiv.org/pdf/2109.03814.pdf)
- (arXiv 2021.09) PnP-DETR: Towards Efficient Visual Analysis with Transformers, [[Paper]](https://arxiv.org/pdf/2109.07036.pdf), [[Code]](https://github.com/twangnh/pnp-detr)
- (arXiv 2021.10) An End-to-End Trainable Video Panoptic Segmentation Method using Transformers, [[Paper]](https://arxiv.org/pdf/2110.04009.pdf)
- (arXiv 2021.12) Masked-attention Mask Transformer for Universal Image Segmentation, [[Paper]](https://arxiv.org/pdf/2112.01527.pdf), [[Code]](https://bowenc0221.github.io/mask2former/)
- (arXiv 2021.12) PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2112.02582.pdf), [[Code]](https://github.com/HarborYuan/PolyphonicFormer)
- (arXiv 2022.04) Panoptic-PartFormer: Learning a Unified Model for Panoptic Part Segmentation, [[Paper]](https://arxiv.org/pdf/2204.04655.pdf), [[Code]](https://github.com/lxtGH/Panoptic-PartFormer)
- (arXiv 2022.05) CONSENT: Context Sensitive Transformer for Bold Words Classification, [[Paper]](https://arxiv.org/pdf/2205.07683.pdf)
- (arXiv 2022.05) CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2206.08948.pdf)
- (arXiv 2022.07) k-means Mask Transformer, [[Paper]](https://arxiv.org/pdf/2207.04044.pdf), [[Code]](https://github.com/google-research/deeplab2)
- (arXiv 2022.07) Masked-attention Mask Transformer for Universal Image Segmentation, [[Paper]](https://arxiv.org/pdf/2112.01527.pdf), [[Code]](https://bowenc0221.github.io/mask2former)
- (arXiv 2022.07) Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic ation, [[Paper]](https://arxiv.org/pdf/2207.11860.pdf), [[Code]](https://github.com/jamycheung/Trans4PASS)
- (arXiv 2022.10) Time-Space Transformers for Video Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2210.03546.pdf), [[Code]](https://github.com/jamycheung/Trans4PASS)
- (arXiv 2022.10) Uncertainty-aware LiDAR Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2210.04472.pdf), [[Code]](https://github.com/kshitij3112/EvLPSNet)
- (arXiv 2022.10) A Generalist Framework for Panoptic Segmentation of Images and Videos, [[Paper]](https://arxiv.org/pdf/2210.06366.pdf)
- (arXiv 2022.10) Pointly-Supervised Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2210.13950.pdf), [[Code]](https://github.com/BraveGroup/PSPS.git)
- (arXiv 2023.03) Position-Guided Point Cloud Panoptic Segmentation Transformer, [[Paper]](https://arxiv.org/pdf/2303.13509.pdf), [[Code]](https://github.com/SmartBot-PJLab/P3Former)
- (arXiv 2023.07) Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning, [[Paper]](https://arxiv.org/pdf/2307.14786.pdf)
- (arXiv 2023.08) LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment, [[Paper]](https://arxiv.org/pdf/2308.01686.pdf), [[Code]](https://github.com/zhangzw12319/lcps.git)
- (arXiv 2023.08) PanoSwin: a Pano-style Swin Transformer for Panorama Understanding, [[Paper]](https://arxiv.org/pdf/2308.14726.pdf)
- (arXiv 2023.09) MASK4D: Mask Transformer for 4D Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2309.16133.pdf), [[Code]](https://vision.rwth-aachen.de/mask4d)
- (arXiv 2023.10) Hierarchical Mask2Former: Panoptic Segmentation of Crops, Weeds and Leaves, [[Paper]](https://arxiv.org/pdf/2310.06582.pdf), [[Code]](https://github.com/madeleinedarbyshire/HierarchicalMask2Former)
- (arXiv 2023.11) 4D-Former: Multimodal 4D Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2311.01520.pdf), [[Code]](https://waabi.ai/4dformer)
- (arXiv 2023.11) MaXTron: Mask Transformer with Trajectory Attention for Video Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2311.18537.pdf), [[Code]](https://github.com/TACJu/MaXTron)
- (arXiv 2024.01) 3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation, [[Paper]](https://arxiv.org/pdf/2401.02281.pdf)
- (arXiv 2024.01) Scalable 3D Panoptic Segmentation With Superpoint Graph Clustering, [[Paper]](https://arxiv.org/pdf/2401.06704.pdf), [[Code]](https://github.com/drprojects/superpoint_transformer)
- (arXiv 2024.02) Benchmarking the Robustness of Panoptic Segmentation for Automated Driving, [[Paper]](https://arxiv.org/pdf/2402.15469.pdf)
- (arXiv 2024.03) PEM: Prototype-based Efficient MaskFormer for Image Segmentation, [[Paper]](https://arxiv.org/pdf/2402.19422.pdf), [[Code]](https://github.com/NiccoloCavagnero/PEM)

### Point Cloud
- (ICRA'21) NDT-Transformer: Large-Scale 3D Point Cloud Localisation using the Normal Distribution Transform Representation, [[Paper]](https://arxiv.org/abs/2103.12292)
- (arXiv 2020.12) Point Transformer, [[Paper]](https://arxiv.org/pdf/2012.09164)
- (arXiv 2020.12) 3D Object Detection with Pointformer, [[Paper]](https://arxiv.org/pdf/2012.11409)
- (arXiv 2020.12) PCT: Point Cloud Transformer, [[Paper]](https://arxiv.org/pdf/2012.09688)
- (arXiv 2021.03) You Only Group Once: Efficient Point-Cloud Processing with Token Representation and Relation Inference Module, [[Paper]](https://arxiv.org/abs/2103.09975), [[Code]](https://github.com/chenfengxu714/YOGO.git)
- (arXiv 2021.04) Group-Free 3D Object Detection via Transformers, [[Paper]](https://arxiv.org/pdf/2104.00678.pdf), [[Code]](https://github.com/zeliu98/Group-Free-3D) 
- (arXiv 2021.04) M3DETR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2104.00678.pdf)
- (arXiv 2021.04) Dual Transformer for Point Cloud Analysis, [[Paper]](https://arxiv.org/pdf/2104.13044.pdf)
- (arXiv 2021.04) Point Cloud Learning with Transformer, [[Paper]](https://arxiv.org/pdf/2104.13636.pdf)
- (arXiv 2021.08) SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer, [[Paper]](https://arxiv.org/pdf/2108.04444.pdf), [[Code]](https://github.com/AllenXiangX/SnowflakeNet) 
- (arXiv 2021.08) PTT: Point-Track-Transformer Module for 3D Single Object Tracking in Point Clouds, [[Paper]](https://arxiv.org/pdf/2108.06455.pdf), [[Code]](https://github.com/shanjiayao/PTT) 
- (arXiv 2021.08) Point-Voxel Transformer: An Efficient Approach To 3D Deep Learning, [[Paper]](https://arxiv.org/pdf/2108.06076.pdf), [[Code]](https://github.com/2020zhangcheng/PVT) 
- (arXiv 2021.08) PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2108.08839.pdf), [[Code]](https://github.com/yuxumin/PoinTr) 
- (arXiv 2021.08) Improving 3D Object Detection with Channel-wise Transformer, [[Paper]](https://arxiv.org/pdf/2108.10723.pdf), [[Code]](https://github.com/hlsheng1/CT3D) 
- (arXiv 2021.09) PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds, [[Paper]](https://arxiv.org/pdf/2109.05566.pdf), [[Code]](https://github.com/OPEN-AIR-SUN/PQ-Transformer) 
- (arXiv 2021.09) An End-to-End Transformer Model for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2109.08141.pdf)
- (arXiv 2021.10) Spatial-Temporal Transformer for 3D Point Cloud Sequences, [[Paper]](https://arxiv.org/pdf/2110.09783.pdf)
- (arXiv 2021.10) PatchFormer: A Versatile 3D Transformer Based on Patch Attention, [[Paper]](https://arxiv.org/pdf/2111.00207.pdf)
- (arXiv 2021.11) CpT: Convolutional Point Transformer for 3D Point Cloud Processing, [[Paper]](https://arxiv.org/pdf/2111.10866.pdf)
- (arXiv 2021.11) PU-Transformer: Point Cloud Upsampling Transformer, [[Paper]](https://arxiv.org/pdf/2111.12242.pdf)
- (arXiv 2021.11) Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling, [[Paper]](https://arxiv.org/pdf/2111.14819.pdf), [[Code]](https://github.com/lulutang0608/Point-BERT) 
- (arXiv 2021.11) Adaptive Channel Encoding Transformer for Point Cloud Analysis, [[Paper]](https://arxiv.org/pdf/2112.02507.pdf), [[Code]](https://github.com/lulutang0608/Point-BERT) 
- (arXiv 2021.11) Fast Point Transformer, [[Paper]](https://arxiv.org/pdf/2112.04702.pdf)
- (arXiv 2021.12) Embracing Single Stride 3D Object Detector with Sparse Transformer, [[Paper]](https://arxiv.org/pdf/2112.06375.pdf), [[Code]](https://github.com/TuSimple/SST) 
- (arXiv 2021.12) Full Transformer Framework for Robust Point Cloud Registration with Deep Information Interaction, [[Paper]](https://arxiv.org/pdf/2112.09385.pdf), [[Code]](https://github.com/CGuangyan-BIT/DIT) 
- (arXiv 2022.02) Geometric Transformer for Fast and Robust Point Cloud Registration, [[Paper]](https://arxiv.org/pdf/2202.06688.pdf), [[Code]](https://github.com/qinzheng93/GeoTransformer) 
- (arXiv 2022.02) LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling, [[Paper]](https://arxiv.org/pdf/2202.06263.pdf)
- (arXiv 2022.02) PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths, [[Paper]](https://arxiv.org/pdf/2202.09507.pdf)
- (arXiv 2022.02) Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer, [[Paper]](https://arxiv.org/pdf/2202.09367.pdf), [[Code]](https://github.com/AllenXiangX/SnowflakeNet)
- (arXiv 2022.03) Spatiotemporal Transformer Attention Network for 3D Voxel Level Joint Segmentation and Motion Prediction in Point Cloud, [[Paper]](https://arxiv.org/pdf/2203.00138.pdf)
- (arXiv 2022.03) 3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification, [[Paper]](https://arxiv.org/pdf/2203.00828.pdf)
- (arXiv 2022.03) Masked Autoencoders for Point Cloud Self-supervised Learning, [[Paper]](https://arxiv.org/pdf/2203.06604.pdf)
- (arXiv 2022.03) CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance, [[Paper]](https://arxiv.org/pdf/2203.09887.pdf)
- (arXiv 2022.03) Masked Discrimination for Self-Supervised Learning on Point Clouds, [[Paper]](https://arxiv.org/pdf/2203.11183.pdf), [[Code]](https://github.com/haotian-liu/MaskPoint)
- (arXiv 2022.03) Voxel Set Transformer: A Set-to-Set Approach to 3D Object Detection from Point Clouds, [[Paper]](https://arxiv.org/pdf/2203.10314.pdf), [[Code]](https://github.com/skyhehe123/VoxSeT)
- (arXiv 2022.03) V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.10638.pdf)
- (arXiv 2022.03) REGTR: End-to-end Point Cloud Correspondences with Transformers, [[Paper]](https://arxiv.org/pdf/2203.14517.pdf), [[Code]](https://github.com/yewzijian/RegTR)
- (arXiv 2022.03) Stratified Transformer for 3D Point Cloud Segmentation, [[Paper]](https://arxiv.org/pdf/2203.14508.pdf), [[Code]](https://github.com/dvlab-research/Stratified-Transformer)
- (arXiv 2022.04) HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud, [[Paper]](https://arxiv.org/pdf/2204.05481.pdf)
- (arXiv 2022.04) Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds, [[Paper]](https://arxiv.org/pdf/2204.10688.pdf), [[Code]](https://spacap3d.github.io/)
- (arXiv 2022.04) Panoptic-PHNet: Towards Real-Time and High-Precision LiDAR Panoptic Segmentation via Clustering Pseudo Heatmap, [[Paper]](https://arxiv.org/pdf/2205.07002.pdf)
- (arXiv 2022.04) VNT-Net: Rotational Invariant Vector Neuron Transformers, [[Paper]](https://arxiv.org/pdf/2205.09690.pdf)
- (arXiv 2022.05) Towards Model Generalization for Monocular 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2205.11664.pdf)
- (arXiv 2022.05) CompleteDT: Point Cloud Completion with Dense Augment Inference Transformers, [[Paper]](https://arxiv.org/pdf/2205.14999.pdf)
- (arXiv 2022.05) TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2205.15997.pdf), [[Code]](https://github.com/autonomousvision/transfuser)
- (arXiv 2022.06) SpikiLi: A Spiking Simulation of LiDAR based Real-time Object Detection for Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2206.02876.pdf)
- (arXiv 2022.06) VN-Transformer: Rotation-Equivariant Attention for Vector Neurons, [[Paper]](https://arxiv.org/pdf/2206.04176.pdf)
- (arXiv 2022.06) PST: Plant Segmentation Transformer Enhanced Phenotyping of MLS Oilseed Rape Point Cloud, [[Paper]](https://arxiv.org/pdf/2206.13082.pdf)
- (arXiv 2022.07) SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer, [[Paper]](https://arxiv.org/pdf/2207.10315.pdf), [[Code]](https://github.com/hrzhou2/seedformer)
- (arXiv 2022.07) Geodesic-Former: a Geodesic-Guided Few-shot 3D Point Cloud Instance Segmenter, [[Paper]](https://arxiv.org/pdf/2207.10859.pdf), [[Code]](https://github.com/VinAIResearch/GeoFormer)
- (arXiv 2022.07) Graph Neural Network and Spatiotemporal Transformer Attention for 3D Video Object Detection from Point Clouds, [[Paper]](https://arxiv.org/pdf/2207.12659.pdf)
- (arXiv 2022.08) Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding, [[Paper]](https://arxiv.org/pdf/2208.00281.pdf)
- (arXiv 2022.08) Exploring Point-BEV Fusion for 3D Point Cloud Object Tracking with Transformer, [[Paper]](https://arxiv.org/pdf/2208.05216.pdf), [[Code]](https://github.com/Jasonkks/PTTR)
- (arXiv 2022.08) PointTree: Transformation-Robust Point Cloud Encoder with Relaxed K-D Trees, [[Paper]](https://arxiv.org/pdf/2208.05962.pdf), [[Code]](https://github.com/immortalCO/PointTree)
- (arXiv 2022.08) Pix4Point: Image Pretrained Transformers for 3D Point Cloud Understanding, [[Paper]](https://arxiv.org/pdf/2208.12259.pdf), [[Code]](https://github.com/guochengqian/Pix4Point)
- (arXiv 2022.09) 3DPCT: 3D Point Cloud Transformer with Dual Self-attention, [[Paper]](https://arxiv.org/pdf/2209.11255.pdf)
- (arXiv 2022.10) Transformers for Object Detection in Large Point Clouds, [[Paper]](https://arxiv.org/pdf/2209.15258.pdf)
- (arXiv 2022.10) Bridged Transformer for Vision and Point Cloud 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2210.01391.pdf)
- (arXiv 2022.10) Introducing Vision Transformer for Alzheimer's Disease classification task with 3D input, [[Paper]](https://arxiv.org/pdf/2210.01177.pdf)
- (arXiv 2022.10) Point Cloud Recognition with Position-to-Structure Attention Transformers, [[Paper]](https://arxiv.org/pdf/2210.02030.pdf)
- (arXiv 2022.10) Point Transformer V2: Grouped Vector Attention and Partition-based Pooling, [[Paper]](https://arxiv.org/pdf/2210.05666.pdf), [[Code]](https://github.com/Gofinge/PointTransformerV2)
- (arXiv 2022.10) SWFormer: Sparse Window Transformer for 3D Object Detection in Point Clouds, [[Paper]](https://arxiv.org/pdf/2210.07372.pdf)
- (arXiv 2022.10) LCPFormer: Towards Effective 3D Point Cloud Analysis via Local Context Propagation in Transformers, [[Paper]](https://arxiv.org/pdf/2210.12755.pdf)
- (arXiv 2022.10) PSFormer: Point Transformer for 3D Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2210.15933.pdf)
- (arXiv 2022.11) Hyperbolic Cosine Transformer for LiDAR 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2211.05580.pdf)
- (arXiv 2022.11) Completing point cloud from few points by Wasserstein GAN and Transformers, [[Paper]](https://arxiv.org/pdf/2211.12746.pdf), [[Code]](https://github.com/WxfQjh/Stability-point-recovery.git)
- (arXiv 2022.11) PVT3D: Point Voxel Transformers for Place Recognition from Sparse Lidar Scans, [[Paper]](https://arxiv.org/pdf/2211.12542.pdf)
- (arXiv 2022.11) 3D Point Positional Encoding for Multi-Camera 3D Object Detection Transformers, [[Paper]](https://arxiv.org/pdf/2211.14710.pdf)
- (arXiv 2023.01) AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers, [[Paper]](https://arxiv.org/pdf/2211.14710.pdf), [[Code]](https://github.com/yuxumin/PoinTr)
- (arXiv 2023.01) Text to Point Cloud Localization with Relation-Enhanced Transformer, [[Paper]](https://arxiv.org/pdf/2301.05372.pdf)
- (arXiv 2023.01) SAT: Size-Aware Transformer for 3D Point Cloud ation, [[Paper]](https://arxiv.org/pdf/2301.06869.pdf)
- (arXiv 2023.01) DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets, [[Paper]](https://arxiv.org/pdf/2301.06051.pdf), [[Code]](https://github.com/Haiyang-W/DSVT)
- (arXiv 2023.01) PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2301.07301.pdf)
- (arXiv 2023.01) FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer, [[Paper]](https://arxiv.org/pdf/2301.08739.pdf)
- (arXiv 2023.01) Slice Transformer and Self-supervised Learning for 6DoF Localization in 3D Point Cloud Maps, [[Paper]](https://arxiv.org/pdf/2301.08957.pdf)
- (arXiv 2023.02) TR3D: Towards Real-Time Indoor 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2302.02858.pdf), [[Code]](https://github.com/SamsungLabs/tr3d)
- (arXiv 2023.02) ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer, [[Paper]](https://arxiv.org/pdf/2302.14435.pdf), [[Code]](https://github.com/I2-Multimedia-Lab/ProxyFormer)
- (arXiv 2023.03) Applying Plain Transformers to Real-World Point Clouds, [[Paper]](https://arxiv.org/pdf/2303.00086.pdf)
- (arXiv 2023.03) BPT: Binary Point Cloud Transformer for Place Recognition, [[Paper]](https://arxiv.org/pdf/2303.01166.pdf)
- (arXiv 2023.03) Improving the quality of dental crown using a Transformer-based method, [[Paper]](https://arxiv.org/pdf/2303.02426.pdf), [[Code]](https://github.com/Golriz-code/shellGeneration)
- (arXiv 2023.03) Point Cloud Classification Using Content-based Transformer via Clustering in Feature Space, [[Paper]](https://arxiv.org/pdf/2303.04599.pdf), [[Code]](https://github.com/yahuiliu99/PointConT)
- (arXiv 2023.03) Efficient Transformer-based 3D Object Detection with Dynamic Token Halting, [[Paper]](https://arxiv.org/pdf/2303.05078.pdf)
- (arXiv 2023.03) Rotation-Invariant Transformer for Point Cloud Matching, [[Paper]](https://arxiv.org/pdf/2303.08231.pdf)
- (arXiv 2023.03) Quality evaluation of point clouds: a novel no-reference approach using transformer-based architecture, [[Paper]](https://arxiv.org/pdf/2303.08634.pdf)
- (arXiv 2023.03) Spherical Transformer for LiDAR-based 3D Recognition, [[Paper]](https://arxiv.org/pdf/2303.12766.pdf), [[Code]](https://github.com/dvlab-research/SphereFormer.git)
- (arXiv 2023.03) Context-Aware Transformer for 3D Point Cloud Automatic Annotation, [[Paper]](https://arxiv.org/pdf/2303.14893.pdf)
- (arXiv 2023.03) ViPFormer: Efficient Vision-and-Pointcloud Transformer for Unsupervised Pointcloud Understanding, [[Paper]](https://arxiv.org/pdf/2303.14893.pdf), [[Code]](https://github.com/auniquesun/ViPFormer)
- (arXiv 2023.03) StarNet: Style-Aware 3D Point Cloud Generation, [[Paper]](https://arxiv.org/pdf/2303.15805.pdf)
- (arXiv 2023.03) Self-positioning Point-based Transformer for Point Cloud Understanding, [[Paper]](https://arxiv.org/pdf/2303.16450.pdf), [[Code]](https://github.com/mlvlab/SPoTr)
- (arXiv 2023.04) APPT : Asymmetric Parallel Point Transformer for 3D Point Cloud Understanding, [[Paper]](https://arxiv.org/pdf/2303.17815.pdf)
- (arXiv 2023.04) PointCAT: Cross-Attention Transformer for point cloud, [[Paper]](https://arxiv.org/pdf/2304.03012.pdf)
- (arXiv 2023.04) Multi-scale Geometry-aware Transformer for 3D Point Cloud Classification, [[Paper]](https://arxiv.org/pdf/2304.05694.pdf)
- (arXiv 2023.04) Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding, [[Paper]](https://arxiv.org/pdf/2304.06906.pdf), [[Code]](https://github.com/microsoft/Swin3D)
- (arXiv 2023.04) PCPNet: An Efficient and Semantic-Enhanced Transformer Network for Point Cloud Prediction, [[Paper]](https://arxiv.org/pdf/2304.07773.pdf), [[Code]](https://github.com/Blurryface0814/PCPNet)
- (arXiv 2023.04) Exploiting Inductive Bias in Transformer for Point Cloud Classification and Segmentation, [[Paper]](https://arxiv.org/pdf/2304.14124.pdf), [[Code]](https://github.com/jiamang/IBT)
- (arXiv 2023.05) PU-EdgeFormer: Edge Transformer for Dense Prediction in Point Cloud Upsampling, [[Paper]](https://arxiv.org/pdf/2305.01148.pdf), [[Code]](https://github.com/dohoon2045/PU-EdgeFormer)
- (arXiv 2023.05) Point Transformer For Coronary Artery Labeling, [[Paper]](https://arxiv.org/pdf/2305.02533.pdf)
- (arXiv 2023.06) Collect-and-Distribute Transformer for 3D Point Cloud Analysis, [[Paper]](https://arxiv.org/pdf/2306.01257.pdf), [[Code]](https://github.com/haibo-qiu/CDFormer)
- (arXiv 2023.06) Efficient 3D ation with Superpoint Transformer, [[Paper]](https://arxiv.org/pdf/2306.08045.pdf), [[Code]](http://github.com/drprojects/superpoint_transformer)
- (arXiv 2023.06) A deep dive into explainable self-supervised transformers for point clouds, [[Paper]](https://arxiv.org/pdf/2306.10798.pdf), [[Code]](http://github.com/drprojects/superpoint_transformer)
- (arXiv 2023.07) SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator, [[Paper]](https://arxiv.org/pdf/2307.08492.pdf), [[Code]](https://github.com/czvvd/SVDFormer)
- (arXiv 2023.07) PSGformer: Enhancing 3D Point Cloud Instance Segmentation via Precise Semantic Guidance, [[Paper]](https://arxiv.org/pdf/2307.07708.pdf)
- (arXiv 2023.07) PG-RCNN: Semantic Surface Point Generation for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2307.12637.pdf), [[Code](https://github.com/quotation2520/PG-RCNN)
- (arXiv 2023.07) Two-stream Multi-level Dynamic Point Transformer for Two-person Interaction Recognition, [[Paper]](https://arxiv.org/pdf/2307.11973.pdf)
- (arXiv 2023.07) pCTFusion: Point Convolution-Transformer Fusion with Semantic Aware Loss for Outdoor LiDAR Point Cloud Segmentation, [[Paper]](https://arxiv.org/pdf/2307.14777.pdf)
- (arXiv 2023.08) Self-supervised Learning of Rotation-invariant 3D Point Set Features using Transformer and its Self-distillation, [[Paper]](https://arxiv.org/pdf/2308.04725.pdf)
- (arXiv 2023.09) Weakly Supervised Point Clouds Transformer for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2309.04105.pdf)
- (arXiv 2023.09) Research on self-cross transformer model of point cloud change detecter, [[Paper]](https://arxiv.org/pdf/2309.07444.pdf)
- (arXiv 2023.09) Radar Instance Transformer: Reliable Moving Instance Segmentation in Sparse Radar Point Clouds, [[Paper]](https://arxiv.org/pdf/2309.16435.pdf)
- (arXiv 2023.10) Uni3D: Exploring Unified 3D Representation at Scale, [[Paper]](https://arxiv.org/pdf/2310.06773.pdf), [[Code]](https://github.com/baaivision/Uni3D)
- (arXiv 2023.10) 2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision, [[Paper]](https://arxiv.org/pdf/2310.12817.pdf), [[Code]](https://jimmy15923.github.io/mit_web/)
- (arXiv 2023.11) DeepEMD: A Transformer-based Fast Estimation of the Earth Mover鈥檚 Distance, [[Paper]](https://arxiv.org/pdf/2310.09998.pdf), [[Code]](https://github.com/atulkumarin/DeepEMD)
- (arXiv 2023.11) OneFormer3D: One Transformer for Unified Point Cloud Segmentation, [[Paper]](https://arxiv.org/pdf/2311.14405.pdf)
- (arXiv 2023.11) CalibFormer: A Transformer-based Automatic LiDAR-Camera Calibration Network, [[Paper]](https://arxiv.org/pdf/2311.15241.pdf)
- (arXiv 2023.12) Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation, [[Paper]](https://arxiv.org/pdf/2312.07231.pdf), [[Code]](https://dit-3d.github.io/FastDiT-3D/)
- (arXiv 2023.12) TULIP: Transformer for Upsampling of LiDAR Point Cloud, [[Paper]](https://arxiv.org/pdf/2312.07231.pdf)
- (arXiv 2023.12) PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2312.08371.pdf), [[Code]](https://github.com/kuanchihhuang/PTT)
- (arXiv 2023.12) Point Transformer V3: Simpler, Faster, Stronger, [[Paper]](https://arxiv.org/pdf/2312.10035.pdf), [[Code]](https://github.com/Pointcept/PointTransformerV3)
- (arXiv 2023.12) ConDaFormer: Disassembled Transformer with Local Structure Enhancement for 3D Point Cloud Understanding, [[Paper]](https://arxiv.org/pdf/2312.11112.pdf), [[Code]](https://github.com/LHDuan/ConDaFormer)
- (arXiv 2023.12) Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding, [[Paper]](https://arxiv.org/pdf/2312.16477.pdf)
- (arXiv 2024.01) 3D Landmark Detection on Human Point Clouds: A Benchmark and A Dual Cascade Point Transformer Framework, [[Paper]](https://arxiv.org/pdf/2401.07251.pdf)
- (arXiv 2024.01) CascadeV-Det: Cascade Point Voting for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2401.07477.pdf)
- (arXiv 2024.01) Adaptive Point Transformer, [[Paper]](https://arxiv.org/pdf/2401.14845.pdf)
- (arXiv 2024.02) AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual Vision Transformer, [[Paper]](https://arxiv.org/pdf/2402.07680.pdf)
- (arXiv 2024.02) DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT Based Diffusion Model, [[Paper]](https://arxiv.org/pdf/2402.11241.pdf)
- (arXiv 2024.02) CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer, [[Paper]](https://arxiv.org/pdf/2402.17360.pdf)
- (arXiv 2024.03) Region-Transformer: Self-Attention Region Based Class-Agnostic Point Cloud Segmentation, [[Paper]](https://arxiv.org/pdf/2403.01407.pdf)
- (arXiv 2024.03) FBPT: A Fully Binary Point Transformer, [[Paper]](https://arxiv.org/pdf/2403.09998.pdf)
- (arXiv 2024.03) Soft Masked Transformer for Point Cloud Processing with Skip Attention-Based Upsampling, [[Paper]](https://arxiv.org/pdf/2403.14124.pdf)

### Pose 
- (arXiv 2020.12) End-to-End Human Pose and Mesh Reconstruction with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09760)
- (arXiv 2020.12) TransPose: Towards Explainable Human Pose Estimation by Transformer, [[Paper]](https://arxiv.org/pdf/2012.14214)
- (arXiv 2021.03) 3D Human Pose Estimation with Spatial and Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2103.10455.pdf), [[Code]](https://github.com/zczcwh/PoseFormer)
- (arXiv 2021.03) End-to-End Trainable Multi-Instance Pose Estimation with Transformers, [[Paper]](https://arxiv.org/pdf/2103.12115.pdf) 
- (arXiv 2021.03) Lifting Transformer for 3D Human Pose Estimation in Video, [[Paper]](https://arxiv.org/abs/2103.14304)
- (arXiv 2021.03) TFPose: Direct Human Pose Estimation with Transformers, [[Paper]](https://arxiv.org/abs/2103.15320)
- (arXiv 2021.04) Pose Recognition with Cascade Transformers, [[Paper]](https://arxiv.org/pdf/2104.06976.pdf), [[Code]](https://github.com/mlpc-ucsd/PRTR)
- (arXiv 2021.04) TokenPose: Learning Keypoint Tokens for Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2104.03516.pdf)
- (arXiv 2021.04) Skeletor: Skeletal Transformers for Robust Body-Pose Estimation, [[Paper]](https://arxiv.org/pdf/2104.11712.pdf)
- (arXiv 2021.04) HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation of Hands and Object in Interaction, [[Paper]](https://arxiv.org/pdf/2104.14639.pdf)
- (arXiv 2021.07) Test-Time Personalization with a Transformer for Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2107.02133.pdf)
- (arXiv 2021.09) Pose Transformers (POTR): Human Motion Prediction with Non-Autoregressive Transformers, [[Paper]](https://arxiv.org/pdf/2109.07531.pdf), [[Code]](https://github.com/idiap/potr) 
- (arXiv 2021.09) GraFormer: Graph Convolution Transformer for 3D Pose Estimation, [[Paper]](https://arxiv.org/pdf/2109.08364.pdf), [[Code]](https://github.com/Graformer/GraFormer)
- (arXiv 2021.09) T6D-Direct: Transformers for Multi-Object 6D Pose Direct Regression, [[Paper]](https://arxiv.org/pdf/2109.10948.pdf)
- (arXiv 2021.10) 6D-ViT: Category-Level 6D Object Pose Estimation via Transformer-based Instance Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.04792.pdf)
- (arXiv 2021.10) Adaptively Multi-view and Temporal Fusing Transformer for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2110.05092.pdf), [[Code]](https://github.com/lelexx/MTF-Transformer) 
- (arXiv 2021.10) HRFormer: High-Resolution Transformer for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2110.09408.pdf), [[Code]](https://github.com/HRNet/HRFormer) 
- (arXiv 2021.10) TransFusion: Cross-view Fusion with Transformer for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2110.09554.pdf), [[Code]](https://github.com/HowieMa/TransFusion-Pose) 
- (arXiv 2021.11) MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2111.12707.pdf), [[Code]](https://github.com/Vegetebird/MHFormer) 
- (arXiv 2021.11) A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose, [[Paper]](https://arxiv.org/pdf/2111.12696.pdf)
- (arXiv 2021.12) PE-former: Pose Estimation Transformer, [[Paper]](https://arxiv.org/pdf/2112.04981.pdf), [[Code]](https://github.com/padeler/PE-former) 
- (arXiv 2021.12) Geometry-Contrastive Transformer for Generalized 3D Pose Transfer, [[Paper]](https://arxiv.org/pdf/2112.07374.pdf), [[Code]](https://github.com/mikecheninoulu/CGT)
- (arXiv 2021.12) DProST: 6-DoF Object Pose Estimation Using Space Carving and Dynamic Projective Spatial Transformer, [[Paper]](https://arxiv.org/pdf/2112.08775.pdf), [[Code]](https://github.com/mikecheninoulu/CGT)
- (arXiv 2021.12) Towards Deep Learning-based 6D Bin Pose Estimation in 3D Scans, [[Paper]](https://arxiv.org/pdf/2112.09598.pdf)
- (arXiv 2021.12) End-to-End Learning of Multi-category 3D Pose and Shape Estimation, [[Paper]](https://arxiv.org/pdf/2112.10196.pdf)
- (arXiv 2022.01) Swin-Pose: Swin Transformer Based Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2201.07384.pdf)
- (arXiv 2022.01) Poseur: Direct Human Pose Regression with Transformers, [[Paper]](https://arxiv.org/pdf/2201.07412.pdf)
- (arXiv 2022.02) HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders, [[Paper]](https://arxiv.org/pdf/2202.03548.pdf)
- (arXiv 2022.03) CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2203.13387.pdf)
- (arXiv 2022.04) BTranspose: Bottleneck Transformers for Human Pose Estimation with Self-Supervised Pre-Training, [[Paper]](https://arxiv.org/pdf/2204.10209.pdf)
- (arXiv 2022.04) ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2204.12484.pdf), [[Code]](https://github.com/ViTAE-Transformer/ViTPose)
- (arXiv 2022.05) YOLOPose: Transformer-based Multi-Object 6D Pose Estimation using Keypoint Regression, [[Paper]](https://arxiv.org/pdf/2205.02536.pdf)
- (arXiv 2022.05) AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation, [[Paper]](https://arxiv.org/pdf/2205.05277.pdf), [[Code]](https://github.com/SZAR-LAB/AggPose)
- (arXiv 2022.05) VTP: Volumetric Transformer for Multi-view Multi-person 3D Pose Estimation, [[Paper]](https://arxiv.org/pdf/2205.12602.pdf)
- (arXiv 2022.05) Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation, [[Paper]](https://arxiv.org/pdf/2206.00171.pdf)
- (arXiv 2022.07) OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos, [[Paper]](https://arxiv.org/pdf/2207.09725.pdf)
- (arXiv 2022.08) Pose Uncertainty Aware Movement Synchrony Estimation via Spatial-Temporal Graph Transformer, [[Paper]](https://arxiv.org/pdf/2208.01161.pdf)
- (arXiv 2022.08) IVT: An End-to-End Instance-guided Video Transformer for 3D Pose Estimation, [[Paper]](https://arxiv.org/pdf/2208.03431.pdf)
- (arXiv 2022.08) Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2208.03704.pdf)
- (arXiv 2022.08) The 8-Point Algorithm as an Inductive Bias for Relative Pose Prediction by ViTs, [[Paper]](https://arxiv.org/pdf/2208.08988.pdf), [[Code]](https://crockwell.github.io/rel_pose/)
- (arXiv 2022.08) PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling, [[Paper]](https://arxiv.org/pdf/2208.10211.pdf), [[Code]](https://github.com/naver/posebert)
- (arXiv 2022.08) K-Order Graph-oriented Transformer with GraAttention for 3D Pose and Shape Estimation, [[Paper]](https://arxiv.org/pdf/2208.11328.pdf)
- (arXiv 2022.08) SoMoFormer: Multi-Person Pose Forecasting with Transformers, [[Paper]](https://arxiv.org/pdf/2208.14023.pdf), [[Code]](https://somof.stanford.edu/)
- (arXiv 2022.09) DPIT: Dual-Pipeline Integrated Transformer for Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2209.02431.pdf)
- (arXiv 2022.09) PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation, [[Paper]](https://arxiv.org/pdf/2209.08194.pdf), [[Code]](https://github.com/HowieMa/PPT)
- (arXiv 2022.10) Exploiting the Joint Motion Synergy with Fusion Network Based On Transformer for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2210.04006.pdf)
- (arXiv 2022.10) Uplift and Upsample: Efficient 3D Human Pose Estimation with Uplifting Transformers, [[Paper]](https://arxiv.org/pdf/2210.06110.pdf), [[Code]](https://github.com/goldbricklemon/uplift-upsample-3dhpe)
- (arXiv 2022.10) Transformer-based Global 3D Hand Pose Estimation in Two Hands Manipulating Objects Scenarios, [[Paper]](https://arxiv.org/pdf/2210.11384.pdf)
- (arXiv 2022.10) CRT-6D: Fast 6D Object Pose Estimation with Cascaded Refinement Transformers, [[Paper]](https://arxiv.org/pdf/2210.11718.pdf), [[Code]](https://github.com/PedroCastro/CRT-6D)
- (arXiv 2022.10) Video based Object 6D Pose Estimation using Transformers, [[Paper]](https://arxiv.org/pdf/2210.13540.pdf), [[Code]](https://github.com/ApoorvaBeedu/VideoPose)
- (arXiv 2022.11) PoET: Pose Estimation Transformer for Single-View, Multi-Object 6D Pose Estimation, [[Paper]](https://arxiv.org/pdf/2211.14125.pdf), [[Code]](https://github.com/aau-cns/poet)
- (arXiv 2022.11) MPT: Mesh Pre-Training with Transformers for Human Pose and Mesh Reconstruction, [[Paper]](https://arxiv.org/pdf/2211.13357.pdf)
- (arXiv 2022.12) ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation, [[Paper]](https://arxiv.org/pdf/2212.04246.pdf), [[Code]](https://github.com/ViTAE-Transformer/ViTPose)
- (arXiv 2023.01) HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2301.07322.pdf), [[Code]](https://github.com/qianxiaoye825/HSTFormer)
- (arXiv 2023.01) A Modular Multi-stage Lightweight Graph Transformer Network for Human Pose and Shape Estimation from 2D Human Pose, [[Paper]](https://arxiv.org/pdf/2301.13403.pdf), [[Code]](https://github.com/qianxiaoye825/HSTFormer)
- (arXiv 2023.02) HDFormer: High-order Directed Transformer for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2302.01825.pdf), [[Code]](https://github.com/hyer/HDFormer)
- (arXiv 2023.02) HybrIK-Transformer, [[Paper]](https://arxiv.org/pdf/2302.04774.pdf), [[Code]](https://github.com/boreshkinai/hybrik-transformer)
- (arXiv 2023.02) Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2302.07408.pdf)
- (arXiv 2023.03) Depth-based 6DoF Object Pose Estimation using Swin Transformer, [[Paper]](https://arxiv.org/pdf/2303.02133.pdf), [[Code]](https://github.com/zhujunli1993/SwinDePose)
- (arXiv 2023.03) Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting, [[Paper]](https://arxiv.org/pdf/2303.05095.pdf)
- (arXiv 2023.03) Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation, [[Paper]](https://arxiv.org/pdf/2303.04991.pdf)
- (arXiv 2023.03) Human Pose Estimation from Ambiguous Pressure Recordings with Spatio-temporal Masked Transformers, [[Paper]](https://arxiv.org/pdf/2303.05691.pdf)
- (arXiv 2023.03) PoseRAC: Pose Saliency Transformer for Repetitive Action Counting, [[Paper]](https://arxiv.org/pdf/2303.08450.pdf), [[Code]](https://github.com/MiracleDance/PoseRAC)
- (arXiv 2023.03) TransPoser: Transformer as an Optimizer for Joint Object Shape and Pose Estimation, [[Paper]](https://arxiv.org/pdf/2303.13477.pdf)
- (arXiv 2023.03) PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2303.17472.pdf), [[Code]](https://github.com/QitaoZhao/PoseFormerV2)
- (arXiv 2023.04) ConvFormer: Parameter Reduction in Transformer Models for 3D Human Pose Estimation by Leveraging Dynamic Multi-Headed Convolutional Attention, [[Paper]](https://arxiv.org/pdf/2304.02147.pdf)
- (arXiv 2023.04) A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image, [[Paper]](https://arxiv.org/pdf/2304.03635.pdf), [[Code]](https://github.com/ChanglongJiangGit/A2J-Transformer)
- (arXiv 2023.05) Poses as Queries: Image-to-LiDAR Map Localization with Transformers, [[Paper]](https://arxiv.org/pdf/2305.04298.pdf), [[Code]](https://github.com/ChanglongJiangGit/A2J-Transformer)
- (arXiv 2023.06) Self-supervised Vision Transformers for 3D Pose Estimation of Novel Objects, [[Paper]](https://arxiv.org/pdf/2306.00129.pdf), [[Code]](https://github.com/sThalham/TraM3D)
- (arXiv 2023.06) Efficient Vision Transformer for Human Pose Estimation via Patch Selection, [[Paper]](https://arxiv.org/pdf/2306.04225.pdf)
- (arXiv 2023.06) A Dual-Source Attention Transformer for Multi-Person Pose Tracking, [[Paper]](https://arxiv.org/pdf/2306.05807.pdf), [[Code]](https://github.com/sThalham/TraM3D)
- (arXiv 2023.06) Seeing the Pose in the Pixels: Learning Pose-Aware Representations in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.09331.pdf), [[Code]](https://github.com/dominickrei/PoseAwareVT)
- (arXiv 2023.06) LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network, [[Paper]](https://arxiv.org/pdf/2306.12525.pdf)
- (arXiv 2023.07) TransPose: A Transformer-based 6D Object Pose Estimation Network with Depth Refinement, [[Paper]](https://arxiv.org/pdf/2307.05561.pdf)
- (arXiv 2023.07) YOLOPose V2: Understanding and Improving Transformer-based 6D Pose Estimation, [[Paper]](https://arxiv.org/pdf/2307.11550.pdf)
- (arXiv 2023.07) TransNet: Transparent Object Manipulation Through Category-Level Pose Estimation, [[Paper]](https://arxiv.org/pdf/2307.12400.pdf), [[Code]](https://progress.eecs.umich.edu/projects/transnet/)
- (arXiv 2023.08) Scene-aware Human Pose Generation using Transformer, [[Paper]](https://arxiv.org/pdf/2308.02177.pdf)
- (arXiv 2023.08) Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation, [[Paper]](https://arxiv.org/pdf/2308.05438.pdf), [[Code]](https://github.com/junzastar/DFTr_Voting)
- (arXiv 2023.08) Double-chain Constraints for 3D Human Pose Estimation in Images and Videos, [[Paper]](https://arxiv.org/pdf/2308.05298.pdf), [[Code]](https://github.com/KHB1698/DC-GCT)
- (arXiv 2023.08) Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation, [[Paper]](https://arxiv.org/pdf/2308.07313.pdf), [[Code1]](https://github.com/Michel-liu/GroupPose), [[Code2]](https://github.com/Michel-liu/GroupPose-Paddle)
- (arXiv 2023.08) EgoPoser: Robust Real-Time Ego-Body Pose Estimation in Large Scenes, [[Paper]](https://arxiv.org/pdf/2308.06493.pdf)
- (arXiv 2023.08) Coarse-to-Fine Multi-Scene Pose Regression with Transformers, [[Paper]](https://arxiv.org/pdf/2308.11783.pdf), [[Code]](https://github.com/yolish/c2f-ms-transformer)
- (arXiv 2023.08) Two-Stage Violence Detection Using ViTPose and Classification Models at Smart Airports, [[Paper]](https://arxiv.org/pdf/2308.16325.pdf), [[Code]](https://github.com/Asami-1/GDP)
- (arXiv 2023.09) Refined Temporal Pyramidal Compression-and-Amplification Transformer for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2309.01365.pdf), [[Code]](https://github.com/hbing-l/RTPCA)
- (arXiv 2023.09) ZS6D: Zero-shot 6D Object Pose Estimation using Vision Transformers, [[Paper]](https://arxiv.org/pdf/2309.11986.pdf)
- (arXiv 2023.10) LEAP: Liberate Sparse-view 3D Modeling from Camera Poses, [[Paper]](https://arxiv.org/pdf/2310.01410.pdf), [[Code]](https://github.com/hwjiang1510/LEAP)
- (arXiv 2023.10) MFOS: Model-Free & One-Shot Object Pose Estimation, [[Paper]](https://arxiv.org/pdf/2310.01897.pdf)
- (arXiv 2023.10) UniPose: Detecting Any Keypoints, [[Paper]](https://arxiv.org/pdf/2310.08530.pdf), [[Code]](https://github.com/IDEA-Research/UniPose)
- (arXiv 2023.10) MoEmo Vision Transformer: Integrating Cross-Attention and Movement Vectors in 3D Pose Estimation for HRI Emotion Detection, [[Paper]](https://arxiv.org/pdf/2310.09757.pdf), [[Code]](https://github.com/IDEA-Research/UniPose)
- (arXiv 2023.10) MotionAGFormer: Enhancing 3D Human Pose Estimation with a Transformer-GCNFormer Network, [[Paper]](https://arxiv.org/pdf/2310.16288.pdf), [[Code]](https://github.com/TaatiTeam/MotionAGFormer)
- (arXiv 2023.10) TransPose: 6D Object Pose Estimation with Geometry-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2310.16279.pdf)
- (arXiv 2023.10) A Spatial-Temporal Transformer based Framework For Human Pose Assessment And Correction in Education Scenarios, [[Paper]](https://arxiv.org/pdf/2311.00401.pdf)
- (arXiv 2023.11) Multiple View Geometry Transformers for 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2311.10983.pdf), [[Code]](https://github.com/XunshanMan/MVGFormer)
- (arXiv 2023.11) Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2311.12028.pdf)
- (arXiv 2023.11) Fingerspelling PoseNet: Enhancing Fingerspelling Translation with Pose-Based Transformer Models, [[Paper]](https://arxiv.org/pdf/2311.12128.pdf), [[Code]](https://github.com/pooyafayyaz/Fingerspelling-PoseNet)
- (arXiv 2023.11) HEViTPose: High-Efficiency Vision Transformer for Human Pose Estimation, [[Paper]](https://arxiv.org/pdf/2311.13615.pdf), [[Code]](https://github.com/T1sweet/HEViTPose/tree/main)
- (arXiv 2023.11) SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation, [[Paper]](https://arxiv.org/pdf/2311.15707.pdf), [[Code]](https://github.com/JiehongLin/SAM-6D)
- (arXiv 2023.11) Pose Anything: A Graph-Based Approach for Category-Agnostic Pose Estimation, [[Paper]](https://arxiv.org/pdf/2311.17891.pdf), [[Code]](https://orhir.github.io/pose-anything/)
- (arXiv 2023.11) PViT-6D: Overclocking Vision Transformers for 6D Pose Estimation with Confidence-Level Prediction and Pose Tokens, [[Paper]](https://arxiv.org/pdf/2311.17504.pdf)
- (arXiv 2023.12) PoseViNet: Distracted Driver Action Recognition Framework Using Multi-View Pose Estimation and Vision Transformer, [[Paper]](https://arxiv.org/pdf/2312.14577.pdf)
- (arXiv 2023.12) Geometry-Biased Transformer for Robust Multi-View 3D Human Pose Reconstruction, [[Paper]](https://arxiv.org/pdf/2312.17106.pdf)
- (arXiv 2024.01) 6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation, [[Paper]](https://arxiv.org/pdf/2401.00029.pdf)
- (arXiv 2024.01) Towards Real-World Aerial Vision Guidance with Categorical 6D Pose Tracker, [[Paper]](https://arxiv.org/pdf/2401.04377.pdf), [[Code]](https://github.com/S-JingTao/Categorical_Pose_Tracking)
- (arXiv 2024.01) Towards Precise 3D Human Pose Estimation with Multi-Perspective Spatial-Temporal Relational Transformers, [[Paper]](https://arxiv.org/pdf/2401.16700.pdf)
- (arXiv 2024.02) Cameras as Rays: Pose Estimation via Ray Diffusion, [[Paper]](https://arxiv.org/pdf/2402.14817.pdf), [[Code]](https://jasonyzhang.com/RayDiffusion)
- (arXiv 2024.03) FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation, [[Paper]](https://arxiv.org/pdf/2403.03221.pdf), [[Code]](https://crockwell.github.io/far/)

### Planning
- (arXiv 2021.12) Differentiable Spatial Planning using Transformers, [[Paper]](https://arxiv.org/pdf/2112.01010.pdf), [[Project]](https://devendrachaplot.github.io/projects/spatial-planning-transformers)

### Pruning & Quantization
- (arXiv 2021.04) Visual Transformer Pruning, [[Paper]](https://arxiv.org/pdf/2104.08500.pdf)
- (arXiv 2021.06) Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.14156.pdf)
- (arXiv 2021.11) PTQ4ViT: Post-Training Quantization Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12293.pdf), [[Code]](https://github.com/hahnyuan/PTQ4ViT)
- (arXiv 2021.11) FQ-ViT: Fully Quantized Vision Transformer without Retraining, [[Paper]](https://arxiv.org/pdf/2111.15127.pdf)
- (arXiv 2022.01) Q-ViT: Fully Differentiable Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.07703.pdf)
- (arXiv 2022.03) Patch Similarity Aware Data-Free Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.02250.pdf)
- (arXiv 2022.03) CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction, [[Paper]](https://arxiv.org/pdf/2203.04570.pdf)
- (arXiv 2022.07) I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference, [[Paper]](https://arxiv.org/pdf/2207.01405.pdf)
- (arXiv 2022.08) Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization, [[Paper]](https://arxiv.org/pdf/2208.05163.pdf)
- (arXiv 2022.09) PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.05687.pdf), [[Code]](https://github.com/zkkli/PSAQ-ViT)
- (arXiv 2022.10) EAPruning: Evolutionary Pruning for Vision Transformers and CNNs, [[Paper]](https://arxiv.org/pdf/2210.00181.pdf)
- (arXiv 2022.10) SaiT: Sparse Vision Transformers through Adaptive Token Pruning, [[Paper]](https://arxiv.org/pdf/2210.05832.pdf)
- (arXiv 2022.10) Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.06707.pdf), [[Code]](https://github.com/YanjingLi0202/Q-ViT)
- (arXiv 2022.10) oViT: An Accurate Second-Order Pruning Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.09223.pdf)
- (arXiv 2022.11) CPT-V: A Contrastive Approach to Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.09643.pdf)
- (arXiv 2022.11) NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.16056.pdf)
- (arXiv 2022.12) Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis, [[Paper]](https://arxiv.org/pdf/2212.03185.pdf), [[Code]](https://github.com/TencentARC/BasicVQ-GEN)
- (arXiv 2022.12) RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.08254.pdf)
- (arXiv 2023.02) Oscillation-free Quantization for Low-bit Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.02210.pdf)
- (arXiv 2023.03) Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction, [[Paper]](https://arxiv.org/pdf/2303.12557.pdf), [[Code]](https://github.com/Q-HyViT)
- (arXiv 2023.03) Scaled Quantization for the Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.13601.pdf)
- (arXiv 2023.03) Towards Accurate Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.14341.pdf)
- (arXiv 2023.04) Q-DETR: An Efficient Low-Bit Quantized Detection Transformer, [[Paper]](https://arxiv.org/pdf/2304.00253.pdf)
- (arXiv 2023.04) Attention Map Guided Transformer Pruning for Edge Device, [[Paper]](https://arxiv.org/pdf/2304.01452.pdf)
- (arXiv 2023.05) Patch-wise Mixed-Precision Quantization of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.06559.pdf)
- (arXiv 2023.05) Boost Vision Transformer with GPU-Friendly Sparsity and Quantization, [[Paper]](https://arxiv.org/pdf/2305.10727.pdf)
- (arXiv 2023.05) Bi-ViT: Pushing the Limit of Vision Transformer Quantization, [[Paper]](https://arxiv.org/pdf/2305.12354.pdf)
- (arXiv 2023.07) Variation-aware Vision Transformer Quantization, [[Paper]](https://arxiv.org/pdf/2307.00331.pdf), [[Code]](https://github.com/HuangOwen/VVTQ)
- (arXiv 2023.08) Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers,  [[Paper]](https://arxiv.org/pdf/2308.10814.pdf), [[Code]](https://github.com/enyac-group/evol-q)
- (arXiv 2023.08) Vision Transformer Pruning Via Matrix Decomposition,  [[Paper]](https://arxiv.org/pdf/2308.10839.pdf)
- (arXiv 2023.09) Transformer-VQ: Linear-Time Transformers via Vector Quantization,  [[Paper]](https://arxiv.org/pdf/2309.16354.pdf), [[Code]](https://github.com/transformer-vq/transformer_vq)
- (arXiv 2023.10) LLM-FP4: 4-Bit Floating-Point Quantized Transformers, [[Paper]](https://arxiv.org/pdf/2310.16836.pdf), [[Code]](https://github.com/nbasyl/LLM-FP4)
- (arXiv 2023.12) QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.02220.pdf)
- (arXiv 2024.01) LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation, [[Paper]](https://arxiv.org/pdf/2401.11243.pdf)
- (arXiv 2024.01) MPTQ-ViT: Mixed-Precision Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.14895.pdf)
- (arXiv 2024.03) Accelerating ViT Inference on FPGA through Static and Dynamic Pruning, [[Paper]](https://arxiv.org/pdf/2403.14047.pdf)

### Recognition
- (arXiv 2021.03) Global Self-Attention Networks for Image Recognition, [[Paper]](https://arxiv.org/abs/2010.03019)
- (arXiv 2021.03) TransFG: A Transformer Architecture for Fine-grained Recognition, [[Paper]](https://arxiv.org/pdf/2103.07976.pdf)
- (arXiv 2021.05) Are Convolutional Neural Networks or Transformers more like human vision, [[Paper]](https://arxiv.org/pdf/2103.07976.pdf)
- (arXiv 2021.07) Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition, [[Paper]](https://arxiv.org/pdf/2107.06538.pdf)
- (arXiv 2021.07) RAMS-Trans: Recurrent Attention Multi-scale Transformer for Fine-grained Image Recognition, [[Paper]](https://arxiv.org/pdf/2107.06538.pdf)
- (arXiv 2021.08) DPT: Deformable Patch-based Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2107.14467.pdf), [[Code]](https://github.com/CASIA-IVA-Lab/DPT)
- (arXiv 2021.10) A free lunch from ViT: Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition, [[Paper]](https://arxiv.org/pdf/2110.01240.pdf)
- (arXiv 2021.10) MVT: Multi-view Vision Transformer for 3D Object Recognition, [[Paper]](https://arxiv.org/pdf/2111.09492.pdf)
- (arXiv 2021.11) AdaViT: Adaptive Vision Transformers for Efficient Image Recognition, [[Paper]](https://arxiv.org/pdf/2111.15668.pdf)
- (arXiv 2022.01) TransVPR: Transformer-based place recognition with multi-level attention aggregation, [[Paper]](https://arxiv.org/pdf/2201.02001.pdf)
- (arXiv 2022.03) MetaFormer : A Unified Meta Framework for Fine-Grained Recognition, [[Paper]](https://arxiv.org/pdf/2203.02751.pdf), [[Code]](https://github.com/dqshuai/MetaFormer)
- (arXiv 2022.04) Diverse Instance Discovery: Vision-Transformer for Instance-Aware Multi-Label Image Recognition, [[Paper]](https://arxiv.org/pdf/2204.10731.pdf), [[Code]](https://github.com/dqshuai/MetaFormer)
- (arXiv 2022.07) Forensic License Plate Recognition with Compression-Informed Transformers, [[Paper]](https://arxiv.org/pdf/2207.14686.pdf), [[Code]](https://www.cs1.tf.fau.de/research/multimedia-security/)
- (arXiv 2022.08) TSRFormer: Table Structure Recognition with Transformers, [[Paper]](https://arxiv.org/pdf/2208.04921.pdf)
- (arXiv 2022.08) GSRFormer: Grounded Situation Recognition Transformer with Alternate Semantic Attention Refinement, [[Paper]](https://arxiv.org/pdf/2208.08965.pdf), [[Code]](https://github.com/zhiqic/GSRFormer)
- (arXiv 2022.09) SeqOT: A Spatial-Temporal Transformer Network for Place Recognition Using Sequential LiDAR Data, [[Paper]](https://arxiv.org/pdf/2209.07951.pdf), [[Code]](https://github.com/BIT-MJY/SeqOT)
- (arXiv 2022.12) Part-guided Relational Transformers for Fine-grained Visual Recognition, [[Paper]](https://arxiv.org/pdf/2212.13685.pdf), [[Code]](https://github.com/iCVTEAM/PART)
- (arXiv 2023.02) CVTNet: A Cross-View Transformer Network for Place Recognition Using LiDAR Data, [[Paper]](https://arxiv.org/pdf/2302.01665.pdf), [[Code]](https://github.com/BIT-MJY/CVTNet)
- (arXiv 2023.02) Rethink Long-tailed Recognition with Vision Transforms, [[Paper]](https://arxiv.org/pdf/2302.14284.pdf)
- (arXiv 2023.04) R2Former: Unified Retrieval and Reranking Transformer for Place Recognition, [[Paper]](https://arxiv.org/pdf/2304.03410.pdf), [[Code]](https://github.com/Jeff-Zilence/R2Former)
- (arXiv 2023.05) MASK-CNN-Transformer For Real-Time Multi-Label Weather Recognition, [[Paper]](https://arxiv.org/pdf/2304.14857.pdf)
- (arXiv 2023.05) TReR: A Lightweight Transformer Re-Ranking Approach for 3D LiDAR Place Recognition, [[Paper]](https://arxiv.org/pdf/2305.18013.pdf)
- (arXiv 2023.07) Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions, [[Paper]](https://arxiv.org/pdf/2307.01530.pdf)
- (arXiv 2023.08) M2Former: Multi-Scale Patch Selection for Fine-Grained Visual Recognition, [[Paper]](https://arxiv.org/pdf/2308.02161.pdf)
- (arXiv 2023.09) Parameter-Efficient Long-Tailed Recognition, [[Paper]](https://arxiv.org/pdf/2309.10019.pdf), [[Code]](https://github.com/shijxcs/PEL)
- (arXiv 2023.09) MAGIC-TBR: Multiview Attention Fusion for Transformer-based Bodily Behavior Recognition in Group Settings, [[Paper]](https://arxiv.org/pdf/2309.10765.pdf), [[Code]](https://github.com/surbhimadan92/MAGIC-TBR)
- (arXiv 2023.10) ClusVPR: Efficient Visual Place Recognition with Clustering-based Weighted Transformer, [[Paper]](https://arxiv.org/pdf/2310.04099.pdf), [[Code]](https://github.com/surbhimadan92/MAGIC-TBR)
- (arXiv 2023.10) FaultSeg Swin-UNETR: Transformer-Based Self-Supervised Pretraining Model for Fault Recognition, [[Paper]](https://arxiv.org/pdf/2310.17974.pdf)
- (arXiv 2023.12) Are Vision Transformers More Data Hungry Than Newborn Visual Systems, [[Paper]](https://arxiv.org/pdf/2312.02843.pdf)
- (arXiv 2024.01) PlaceFormer: Transformer-based Visual Place Recognition using Multi-Scale Patch Selection and Fusion, [[Paper]](https://arxiv.org/pdf/2401.13082.pdf)
- (arXiv 2024.01) Regressing Transformers for Data-efficient Visual Place Recognition, [[Paper]](https://arxiv.org/pdf/2401.16304.pdf)
- (arXiv 2024.01) A New Method for Vehicle Logo Recognition Based on Swin Transformer, [[Paper]](https://arxiv.org/pdf/2401.15458.pdf)

### Reconstruction 
- (arXiv 2021.03) Multi-view 3D Reconstruction with Transformer, [[Paper]](https://arxiv.org/pdf/2103.12957.pdf)
- (arXiv 2021.06) THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers, [[Paper]](https://arxiv.org/pdf/2106.09336.pdf)
- (arXiv 2021.06) LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction, [[Paper]](https://arxiv.org/pdf/2106.12102.pdf)
- (arXiv 2021.07) TransformerFusion: Monocular RGB Scene Reconstruction using Transformers, [[Paper]](https://arxiv.org/pdf/2107.08192.pdf)
- (arXiv 2021.10) 3D-RETR: End-to-End Single and Multi-View 3D Reconstruction with Transformers, [[Paper]](https://arxiv.org/pdf/2110.08861.pdf), [[Code]](https://github.com/FomalhautB/3D-RETR)
- (arXiv 2021.11) Reference-based Magnetic Resonance Image Reconstruction Using Texture Transformer, [[Paper]](https://arxiv.org/pdf/2111.09492.pdf)
- (arXiv 2021.11) HEAT: Holistic Edge Attention Transformer for Structured Reconstruction, [[Paper]](https://arxiv.org/pdf/2111.15143.pdf)
- (arXiv 2021.12) VoRTX: Volumetric 3D Reconstruction With Transformers for Voxelwise View Selection and Fusion, [[Paper]](https://arxiv.org/pdf/2112.00236.pdf), [[Code]](https://noahstier.github.io/vortx/)
- (arXiv 2022.03) RayTran: 3D pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers, [[Paper]](https://arxiv.org/pdf/2203.13296.pdf)
- (arXiv 2022.05) 3D-C2FT: Coarse-to-fine Transformer for Multi-view 3D Reconstruction, [[Paper]](https://arxiv.org/pdf/2205.14575.pdf)
- (arXiv 2022.05) HeatER: An Efficient and Unified Network for Human Reconstruction via Heatmap-based TransformER, [[Paper]](https://arxiv.org/pdf/2205.15448.pdf)
- (arXiv 2022.06) Extreme Floorplan Reconstruction by Structure-Hallucinating Transformer Cascades, [[Paper]](https://arxiv.org/pdf/2206.00645.pdf)
- (arXiv 2022.08) PlaneFormers: From Sparse View Planes to 3D Reconstruction, [[Paper]](https://arxiv.org/pdf/2208.04307.pdf)
- (arXiv 2023.01) Monocular Scene Reconstruction with 3D SDF Transformers, [[Paper]](https://arxiv.org/pdf/2301.13510.pdf), [[Project]](https://weihaosky.github.io/sdfformer)
- (arXiv 2023.02) Efficient 3D Object Reconstruction using Visual Transformers, [[Paper]](https://arxiv.org/pdf/2302.08474.pdf), [[Project]](https://weihaosky.github.io/sdfformer)
- (arXiv 2023.02) UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction, [[Paper]](https://arxiv.org/pdf/2302.13987.pdf)
- (arXiv 2023.03) CryoFormer: Continuous Reconstruction of 3D Structures from Cryo-EM Data using Transformer-based Neural Representations, [[Paper]](https://arxiv.org/pdf/2303.16254.pdf), [[Project]](https://cryoformer.github.io/)
- (arXiv 2023.04) CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction, [[Paper]](https://arxiv.org/pdf/2304.07072.pdf)
- (arXiv 2023.07) Image Reconstruction using Enhanced Vision Transformer, [[Paper]](https://arxiv.org/pdf/2307.05616.pdf)
- (arXiv 2023.08) Long-Range Grouping Transformer for Multi-View 3D Reconstruction, [[Paper]](https://arxiv.org/pdf/2308.08724.pdf),[[Code]](https://github.com/LiyingCV/Long-Range-Grouping-Transformer)
- (arXiv 2023.08) A Transformer-Conditioned Neural Fields Pipeline with Polar Coordinate Representation for Astronomical Radio Interferometric Data Reconstruction, [[Paper]](https://arxiv.org/pdf/2308.14610.pdf)
- (arXiv 2023.09) Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction, [[Paper]](https://arxiv.org/pdf/2309.13524.pdf),[[Code]](https://github.com/River-Zhang/GTA)
- (arXiv 2023.10) Sketch2CADScript: 3D Scene Reconstruction from 2D Sketch using Visual Transformer and Rhino Grasshopper, [[Paper]](https://arxiv.org/pdf/2309.16850.pdf)
- (arXiv 2023.10) ShapeGraFormer: GraFormer-Based Network for Hand-Object Reconstruction from a Single Depth Map, [[Paper]](https://arxiv.org/pdf/2310.11811.pdf)
- (arXiv 2023.10) DIAR: Deep Image Alignment and Reconstruction using Swin Transformers, [[Paper]](https://arxiv.org/pdf/2310.11605.pdf)
- (arXiv 2023.12) Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers, [[Paper]](https://arxiv.org/pdf/2312.09147.pdf),[[Project]](https://zouzx.github.io/TriplaneGaussian/)
- (arXiv 2024.01) GridFormer: Point-Grid Transformer for Surface Reconstruction, [[Paper]](https://arxiv.org/pdf/2401.02292.pdf),[[Code]](https://github.com/list17/GridFormer)

### Referring
- (arXiv 2021.08) Vision-Language Transformer and Query Generation for Referring Segmentation, [[Paper]](https://arxiv.org/pdf/2108.05565.pdf), [[Code]](https://github.com/henghuiding/Vision-Language-Transformer)
- (arXiv 2021.12) LAVT: Language-Aware Vision Transformer for Referring Image Segmentation, [[Paper]](https://arxiv.org/pdf/2112.02244.pdf),[[Code]](https://github.com/yz93/LAVT-RIS)
- (arXiv 2022.03) ReSTR: Convolution-free Referring Image Segmentation Using Transformers, [[Paper]](https://arxiv.org/pdf/2203.16768.pdf), [[Code]](http://cvlab.postech.ac.kr/research/restr/)
- (arXiv 2022.10) VLT: Vision-Language Transformer and Query Generation for Referring Segmentation, [[Paper]](https://arxiv.org/pdf/2210.15871.pdf), [[Code]](https://github.com/henghuiding/Vision-Language-Transformer)
- (arXiv 2023.09) Contrastive Grouping with Transformer for Referring Image Segmentation, [[Paper]](https://arxiv.org/pdf/2309.01017.pdf),[[Code]](https://github.com/Toneyaya/CGFormer)

### Registration
- (arXiv 2021.04) ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration, [[Paper]](https://arxiv.org/pdf/2104.06468.pdf), [[Code]](https://bit.ly/3bWDynR)
- (arXiv 2022.02) A Transformer-based Network for Deformable Medical Image Registration, [[Paper]](https://arxiv.org/pdf/2202.12104.pdf)
- (arXiv 2022.03) Affine Medical Image Registration with Coarse-to-Fine Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.15216.pdf), [[Code]](https://github.com/cwmok/C2FViT)
- (arXiv 2022.04) Symmetric Transformer-based Network for Unsupervised Image Registration, [[Paper]](https://arxiv.org/pdf/2204.13575.pdf), [[Code]](https://github.com/MingR-Ma/SymTrans)
- (arXiv 2023.03) Spatially-varying Regularization with Conditional Transformer for Unsupervised Image Registration, [[Paper]](https://arxiv.org/pdf/2303.06168.pdf)
- (arXiv 2023.03) RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration, [[Paper]](https://arxiv.org/pdf/2303.12384.pdf)
- (arXiv 2023.07) Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration, [[Paper]](https://arxiv.org/pdf/2307.03421.pdf)
- (arXiv 2023.08) 2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds, [[Paper]](https://arxiv.org/pdf/2308.05667.pdf), [[Code]](https://github.com/minhaolee/2D3DMATR)
- (arXiv 2023.08) GeoTransformer: Fast and Robust Point Cloud Registration with Geometric Transformer, [[Paper]](https://arxiv.org/pdf/2308.03768.pdf), [[Code]](https://github.com/qinzheng93/GeoTransformer)
- (arXiv 2023.10) OAAFormer: Robust and Efficient Point Cloud Registration Through Overlapping-Aware Attention in Transformer, [[Paper]](https://arxiv.org/pdf/2310.09817.pdf)
- (arXiv 2023.12) VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning, [[Paper]](https://arxiv.org/pdf/2312.08774.pdf), [[Code]](https://github.com/sugar-fly/VSFormer)
- (arXiv 2023.12) D3Former: Jointly Learning Repeatable Dense Detectors and Feature-enhanced Descriptors via Saliency-guided Transformer,  [[Paper]](https://arxiv.org/pdf/2312.12970.pdf)
- (arXiv 2024.03) EfficientMorph: Parameter-Efficient Transformer-Based Architecture for 3D Image Registration,  [[Paper]](https://arxiv.org/pdf/2403.11026.pdf)

### Re-identification
- (arXiv 2021.02) TransReID: Transformer-based Object Re-Identification, [[Paper]](https://arxiv.org/abs/2102.04378)
- (arXiv 2021.03) Spatiotemporal Transformer for Video-based Person Re-identification, [[Paper]](https://arxiv.org/abs/2103.16469)
- (arXiv 2021.04) AAformer: Auto-Aligned Transformer for Person Re-Identification, [[Paper]](https://arxiv.org/abs/2104.00921)
- (arXiv 2021.04) A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person Re-identification, [[Paper]](https://arxiv.org/abs/2104.01745)
- (arXiv 2021.06) Transformer-Based Deep Image Matching for Generalizable Person Re-identification, [[Paper]](https://arxiv.org/pdf/2105.14432.pdf)
- (arXiv 2021.06) Diverse Part Discovery: Occluded Person Re-identification with Part-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2106.04095.pdf)
- (arXiv 2021.06) Person Re-Identification with a Locally Aware Transformer, [[Paper]](https://arxiv.org/pdf/2106.03720.pdf)
- (arXiv 2021.07) Learning Disentangled Representation Implicitly via Transformer for Occluded Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2107.02380.pdf), [[Code]](https://github.com/Anonymous-release-code/DRL-Net)
- (arXiv 2021.07) GiT: Graph Interactive Transformer for Vehicle Re-identification, [[Paper]](https://arxiv.org/pdf/2107.05448.pdf)
- (arXiv 2021.07) HAT: Hierarchical Aggregation Transformers for Person Re-identification, [[Paper]](https://arxiv.org/pdf/2107.05946.pdf)
- (arXiv 2021.09) Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2109.03483.pdf)
- (arXiv 2021.09) OH-Former: Omni-Relational High-Order Transformer for Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2109.11159.pdf)
- (arXiv 2021.10) CMTR: Cross-modality Transformer for Visible-infrared Person Re-identification, [[Paper]](https://arxiv.org/pdf/2110.08994.pdf)
- (arXiv 2021.11) Self-Supervised Pre-Training for Transformer-Based Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2111.12084.pdf), [[Code]](https://github.com/michuanhaohao/TransReID-SSL)
- (arXiv 2021.12) Pose-guided Feature Disentangling for Occluded Person Re-identification Based on Transformer, [[Paper]](https://arxiv.org/pdf/2112.02466.pdf), [[Code]](https://github.com/WangTaoAs/PFD_Net)
- (arXiv 2022.01) Short Range Correlation Transformer for Occluded Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2201.01090.pdf)
- (arXiv 2022.02) Motion-Aware Transformer For Occluded Person Re-identification, [[Paper]](https://arxiv.org/pdf/2202.04243.pdf)
- (arXiv 2022.04) PSTR: End-to-End One-Step Person Search With Transformers, [[Paper]](https://arxiv.org/pdf/2204.03340.pdf), [[Code]](https://github.com/JialeCao001/PSTR)
- (arXiv 2022.04) NFormer: Robust Person Re-identification with Neighbor Transformer, [[Paper]](https://arxiv.org/pdf/2204.09331.pdf), [[Code]](https://github.com/haochenheheda/NFormer)
- (arXiv 2022.09) Uncertainty Aware Multitask Pyramid Vision Transformer For UAV-Based Object Re-Identification, [[Paper]](https://arxiv.org/pdf/2209.08686.pdf)
- (arXiv 2022.11) Sequential Transformer for End-to-End Person Search, [[Paper]](https://arxiv.org/pdf/2211.04323.pdf)
- (arXiv 2022.11) Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2211.04323.pdf), [[Code]](https://github.com/RikoLi/WACV23-workshop-TMGF)
- (arXiv 2022.11) Learning Progressive Modality-shared Transformers for Effective Visible-Infrared Person Re-identification, [[Paper]](https://arxiv.org/pdf/2212.00226.pdf), [[Code]](https://github.com/hulu88/PMT)
- (arXiv 2023.01) Multi-Stage Spatio-Temporal Aggregation Transformer for Video Person Re-identification, [[Paper]](https://arxiv.org/pdf/2301.00531.pdf)
- (arXiv 2023.02) X-ReID: Cross-Instance Transformer for Identity-Level Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2302.02075.pdf)
- (arXiv 2023.02) DC-Former: Diverse and Compact Transformer for Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2302.14335.pdf)
- (arXiv 2023.03) Feature Completion Transformer for Occluded Person Re-identification, [[Paper]](https://arxiv.org/pdf/2303.01656.pdf)
- (arXiv 2023.03) TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2303.06819.pdf), [[Code]](https://github.com/Kali-Hac/TranSG)
- (arXiv 2023.04) Deeply-Coupled Convolution-Transformer with Spatial-temporal Complementary Learning for Video-based Person Re-identification, [[Paper]](https://arxiv.org/pdf/2304.14122.pdf), [[Code]](https://github.com/Kali-Hac/TranSG)
- (arXiv 2023.08) Part-Aware Transformer for Generalizable Person Re-identification, [[Paper]](https://arxiv.org/pdf/2308.03322.pdf), [[Code]](https://github.com/liyuke65535/Part-Aware-Transformer)
- (arXiv 2023.10) GraFT: Gradual Fusion Transformer for Multimodal Re-Identification, [[Paper]](https://arxiv.org/pdf/2310.16856.pdf)
- (arXiv 2024.02) Dynamic Patch-aware Enrichment Transformer for Occluded Person Re-Identification, [[Paper]](https://arxiv.org/pdf/2402.10435.pdf)
- (arXiv 2024.03) View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network, [[Paper]](https://arxiv.org/pdf/2403.14513.pdf), [[Code]](https://github.com/LinlyAC/VDT-AGPReID)

### Remote Sensing
- (arXiv 2021.07) Looking Outside the Window: Wider-Context Transformer for the ation of High-Resolution Remote Sensing Images, [[Paper]](https://arxiv.org/pdf/2106.15754.pdf)
- (arXiv 2022.07) SiamixFormer: A Siamese Transformer Network For Building Detection And Change Detection From Bi-Temporal Remote Sensing Images, [[Paper]](https://arxiv.org/pdf/2208.00657.pdf)
- (arXiv 2022.08) Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model, [[Paper]](https://arxiv.org/pdf/2208.03987.pdf), [[Code]](https://github.com/ViTAE-Transformer/Remote-Sensing-RVSA)
- (arXiv 2022.09) Transfer Learning with Pretrained Remote Sensing Transformers, [[Paper]](https://arxiv.org/pdf/2209.14969.pdf), [[Code]](https://github.com/antofuller/SatViT)
- (arXiv 2022.10) MCTNet: A Multi-Scale CNN-Transformer Network for Change Detection in Optical Remote Sensing Images, [[Paper]](https://arxiv.org/pdf/2210.07601.pdf)
- (arXiv 2022.10) Fully Transformer Network for Change Detection of Remote Sensing Images, [[Paper]](https://arxiv.org/pdf/2210.00757.pdf), [[Code]](https://github.com/AI-Zhpp/FTN)
- (arXiv 2022.12) RCDT: Relational Remote Sensing Change Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2212.04869.pdf)
- (arXiv 2023.04) Remote Sensing Change Detection With Transformers Trained from Scratch, [[Paper]](https://arxiv.org/pdf/2304.06710.pdf)
- (arXiv 2023.06) Lightweight Structure-aware Transformer Network for VHR Remote Sensing Image Change Detection, [[Paper]](https://arxiv.org/pdf/2306.01988.pdf)
- (arXiv 2023.06) CD-CTFM: A Lightweight CNN-Transformer Network for Remote Sensing Cloud Detection Fusing Multiscale Features, [[Paper]](https://arxiv.org/pdf/2306.07186.pdf)
- (arXiv 2023.06) RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model, [[Paper]](https://arxiv.org/pdf/2306.16269.pdf), [[Code]](https://kyanchen.github.io/RSPrompter)
- (arXiv 2023.07) General-Purpose Multimodal Transformer meets Remote Sensing ation, [[Paper]](https://arxiv.org/pdf/2307.03388.pdf), [[Code]](https://github.com/nhikieu/SpatialVolumetricMultimodal)
- (arXiv 2023.07) Cross-Spatial Pixel Integration and Cross-Stage Feature Fusion Based Transformer Network for Remote Sensing Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2307.02974.pdf)
- (arXiv 2023.08) LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake Extraction from Remote Sensing Imagery, [[Paper]](https://arxiv.org/pdf/2308.04397.pdf)
- (arXiv 2023.08) SwinV2DNet: Pyramid and Self-Supervision Compounded Feature Learning for Remote Sensing Images Change Detection, [[Paper]](https://arxiv.org/pdf/2308.11159.pdf), [[Code]](https://github.com/DalongZ/SwinV2DNet)
- (arXiv 2023.08) RingMo-lite: A Remote Sensing Multi-task Lightweight Network with CNN-Transformer Hybrid Framework, [[Paper]](https://arxiv.org/pdf/2309.09003.pdf), [[Code]](https://github.com/DalongZ/SwinV2DNet)
- (arXiv 2023.10) Efficient Remote Sensing Segmentation With Generative Adversarial Transformer, [[Paper]](https://arxiv.org/pdf/2310.01292.pdf)
- (arXiv 2023.10) HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images, [[Paper]](https://arxiv.org/pdf/2310.07995.pdf), [[Code]](https://github.com/qbxfcz/HeightFormer)
- (arXiv 2023.10) VcT: Visual change Transformer for Remote Sensing Image Change Detection, [[Paper]](https://arxiv.org/pdf/2310.11417.pdf), [[Code]](https://github.com/Event-AHU/VcT_Remote_Sensing_Change_Detection)
- (arXiv 2023.10) Multimodal Transformer Using Cross-Channel attention for Object Detection in Remote Sensing Images, [[Paper]](https://arxiv.org/pdf/2310.13876.pdf)
- (arXiv 2023.10) SolarFormer: Multi-scale Transformer for Solar PV Profiling, [[Paper]](https://arxiv.org/pdf/2310.20057.pdf)
- (arXiv 2023.11) CLiSA: A Hierarchical Hybrid Transformer Model using Orthogonal Cross Attention for Satellite Image Cloud Segmentation, [[Paper]](https://arxiv.org/pdf/2311.17475.pdf)
- (arXiv 2023.11) SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints, [[Paper]](https://arxiv.org/pdf/2312.02464.pdf), [[Code]](https://github.com/sstary/SSRS)
- (arXiv 2024.02) On Convolutional Vision Transformers for Yield Prediction, [[Paper]](https://arxiv.org/pdf/2402.05557.pdf)
- (arXiv 2024.02) AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers, [[Paper]](https://arxiv.org/pdf/2402.05602.pdf), [[Code]](https://github.com/rachtibat/LRP-for-Transformers)
- (arXiv 2024.02) Cross-Resolution Land Cover Classification Using Outdated Products and Transformers, [[Paper]](https://arxiv.org/pdf/2402.16001.pdf), [[Code]](https://github.com/yu-ni1989/ANLC-Former)
- (arXiv 2024.03) Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery, [[Paper]](https://arxiv.org/pdf/2403.05419.pdf), [[Code]](https://github.com/techmn/satmae_pp)

### Restoration
- (arXiv 2021.06) Uformer: A General U-Shaped Transformer for Image Restoration, [[Paper]](https://arxiv.org/pdf/2106.03106.pdf), [[Code]](https://github.com/ZhendongWang6/Uformer)
- (arXiv 2021.08) SwinIR: Image Restoration Using Swin Transformer, [[Paper]](https://arxiv.org/pdf/2108.10257.pdf), [[Code]](https://github.com/JingyunLiang/SwinIR)
- (arXiv 2021.11) Restormer: Efficient Transformer for High-Resolution Image Restoration, [[Paper]](https://arxiv.org/pdf/2111.09881.pdf), [[Code]](https://github.com/swz30/Restormer)
- (arXiv 2021.12) U2-Former: A Nested U-shaped Transformer for Image Restoration, [[Paper]](https://arxiv.org/pdf/2112.02279.pdf), [[Code]](https://github.com/swz30/Restormer)
- (arXiv 2021.12) SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-Trained Siamese Transformers, [[Paper]](https://arxiv.org/pdf/2112.09426.pdf)
- (arXiv 2022.08) ELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer, [[Paper]](https://arxiv.org/pdf/2208.14704.pdf), [[Code]](https://github.com/leonmakise/ELMformer)
- (arXiv 2022.09) LRT: An Efficient Low-Light Restoration Transformer for Dark Light Field Images, [[Paper]](https://arxiv.org/pdf/2209.02197.pdf)
- (arXiv 2022.09) Dual-former: Hybrid Self-attention Transformer for Efficient Image Restoration, [[Paper]](https://arxiv.org/pdf/2210.01069.pdf)
- (arXiv 2022.10) Accurate Image Restoration with Attention Retractable Transformer, [[Paper]](https://arxiv.org/pdf/2210.01427.pdf), [[Code]](https://github.com/gladzhang/ART)
- (arXiv 2022.11) Cross Aggregation Transformer for Image Restoration, [[Paper]](https://arxiv.org/pdf/2211.13654.pdf), [[Code]](https://github.com/zhengchen1999/CAT)
- (arXiv 2023.01) Towards Vision Transformer Unrolling Fixed-Point Algorithm: a Case Study on Image Restoration, [[Paper]](https://arxiv.org/pdf/2301.12332.pdf)
- (arXiv 2023.03) Retinal Image Restoration using Transformer and Cycle-Consistent Generative Adversarial Network, [[Paper]](https://arxiv.org/pdf/2303.01939.pdf), [[Code]](https://github.com/AAleka/Transformer-Cycle-GAN)
- (arXiv 2023.03) SANDFORMER: CNN and Transformer under Gated Fusion for Sand Dust Image Restoration, [[Paper]](https://arxiv.org/pdf/2303.04365.pdf)
- (arXiv 2023.04) Burstormer: Burst Image Restoration and Enhancement Transformer, [[Paper]](https://arxiv.org/pdf/2304.01194.pdf), [[Code]](http://github.com/akshaydudhane16/Burstormer)
- (arXiv 2023.05) RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration, [[Paper]](https://arxiv.org/pdf/2305.11474.pdf)
- (arXiv 2023.05) GridFormer: Residual Dense Transformer with Grid Structure for Image Restoration in Adverse Weather Conditions, [[Paper]](https://arxiv.org/pdf/2305.17863.pdf)
- (arXiv 2023.07) On the unreasonable vulnerability of transformers for image restoration 鈥? and an easy fix, [[Paper]](https://arxiv.org/pdf/2307.13856.pdf)
- (arXiv 2023.08) Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration, [[Paper]](https://arxiv.org/pdf/2308.08730.pdf), [[Code]](https://github.com/wlydlut/C2F-DFT)
- (arXiv 2023.09) Prompt-based All-in-One Image Restoration using CNNs and Transformer, [[Paper]](https://arxiv.org/pdf/2309.03063.pdf), [[Code]](https://github.com/wlydlut/C2F-DFT)
- (arXiv 2023.09) HAT: Hybrid Attention Transformer for Image Restoration, [[Paper]](https://arxiv.org/pdf/2309.05239.pdf), [[Code]](https://github.com/XPixelGroup/HAT)
- (arXiv 2023.12) ViStripformer: A Token-Efficient Transformer for Versatile Video Restoration,  [[Paper]](https://arxiv.org/pdf/2312.14502.pdf)
- (arXiv 2024.02) Key-Graph Transformer for Image Restoration,  [[Paper]](https://arxiv.org/pdf/2402.02634.pdf)

### Retrieval
- (CVPR'21') Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers,  [[Paper]](https://arxiv.org/abs/2103.16553)
- (arXiv 2021.01) Investigating the Vision Transformer Model for Image Retrieval Tasks, [[Paper]](https://arxiv.org/pdf/2101.03771)
- (arXiv 2021.02) Training Vision Transformers for Image Retrieval, [[Paper]](https://arxiv.org/pdf/2102.05644.pdf)
- (arXiv 2021.03) Instance-level Image Retrieval using Reranking Transformers, [[Paper]](https://arxiv.org/abs/2103.12424)
- (arXiv 2021.04) Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval, [[Paper]](https://arxiv.org/pdf/2104.00650.pdf)
- (arXiv 2021.04) Self-supervised Video Retrieval Transformer Network, [[Paper]](https://arxiv.org/pdf/2104.07993.pdf)
- (arXiv 2021.05) TransHash: Transformer-based Hamming Hashing for Efficient Image Retrieval, [[Paper]](https://arxiv.org/pdf/2105.07197.pdf), [[Code]](https://github.com/shikhartuli/cnn_txf_bias)
- (arXiv 2021.06) Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features, [[Paper]](https://arxiv.org/pdf/2106.00358.pdf)
- (arXiv 2021.06) All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2106.10153.pdf), [[Code]](https://github.com/cscribano/AYCE_2021)
- (arXiv 2021.09) Vision Transformer Hashing for Image Retrieval, [[Paper]](https://arxiv.org/pdf/2109.12564.pdf)
- (arXiv 2022.01) Zero-Shot Sketch Based Image Retrieval using Graph Transformer, [[Paper]](https://arxiv.org/pdf/2203.06429.pdf)
- (arXiv 2022.07) TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval, [[Paper]](https://arxiv.org/pdf/2207.07852.pdf), [[Code]](https://github.com/yuqi657/ts2_net)
- (arXiv 2022.08) EViT: Privacy-Preserving Image Retrieval via Encrypted Vision Transformer in Cloud Computing, [[Paper]](https://arxiv.org/pdf/2208.14657.pdf), [[Code]](https://github.com/onlinehuazai/EViT)
- (arXiv 2022.10) ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval, [[Paper]](https://arxiv.org/pdf/2210.04341.pdf)
- (arXiv 2022.10) General Image Descriptors for Open World Image Retrieval using ViT CLIP, [[Paper]](https://arxiv.org/pdf/2210.11141.pdf)
- (arXiv 2022.10) Boosting vision transformers for image retrieval, [[Paper]](https://arxiv.org/pdf/2210.11909.pdf), [[Code]](https://github.com/dealicious-inc/DToP)
- (arXiv 2023.04) STIR: Siamese Transformer for Image Retrieval Postprocessing, [[Paper]](https://arxiv.org/pdf/2304.13393.pdf), [[Code]](https://github.com/OML-Team/open-metric-learning/tree/main/pipelines/postprocessing/)
- (arXiv 2023.08) Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval, [[Paper]](https://arxiv.org/pdf/2308.04343.pdf), [[Code]](https://github.com/LuminosityX/HAT)
- (arXiv 2023.10) GMMFormer: Gaussian-Mixture-Model based Transformer for Efficient Partially Relevant Video Retrieval, [[Paper]](https://arxiv.org/pdf/2310.05195.pdf)
- (arXiv 2024.01) Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval, [[Paper]](https://arxiv.org/pdf/2401.15362.pdf)

### Robotic
- (arXiv 2022.01) Look Closer: Bridging Egocentric and Third-Person Views with Transformers for Robotic Manipulation, [[Paper]](https://arxiv.org/pdf/2201.07779.pdf), [[Code]](https://github.com/jangirrishabh/look-closer)
- (arXiv 2022.02) When Transformer Meets Robotic Grasping: Exploits Context for Efficient Grasp Detection, [[Paper]](https://arxiv.org/pdf/2202.11911.pdf), [[Code]](https://github.com/WangShaoSUN/grasp-transformer)
- (arXiv 2022.07) 3D Part Assembly Generation with Instance Encoded Transformer, [[Paper]](https://arxiv.org/pdf/2207.01779.pdf)
- (arXiv 2022.09) Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation, [[Paper]](https://arxiv.org/pdf/2209.05451.pdf), [[Project]](https://peract.github.io/)
- (arXiv 2022.09) PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training, [[Paper]](https://arxiv.org/pdf/2209.11133.pdf)
- (arXiv 2022.12) RT-1: Robotics Transformer for Real-World Control at Scale, [[Paper]](https://arxiv.org/pdf/2212.06817.pdf), [[Project]](http://robotics-transformer.github.io/)
- (arXiv 2023.06) RVT: Robotic View Transformer for 3D Object Manipulation, [[Paper]](https://arxiv.org/pdf/2306.14896.pdf), [[Project]](https://robotic-view-transformer.github.io/)
- (arXiv 2023.09) AnyOKP: One-Shot and Instance-Aware Object Keypoint Extraction with Pretrained ViT, [[Paper]](https://arxiv.org/pdf/2309.08134.pdf)
- (arXiv 2023.09) PolarNet: 3D Point Clouds for Language-Guided Robotic Manipulation, [[Paper]](https://arxiv.org/pdf/2309.15596.pdf), [[Project]](https://www.di.ens.fr/willow/research/polarnet/)
- (arXiv 2023.10) Knolling bot: A Transformer-based Approach to Organizing a Messy Table, [[Paper]](https://arxiv.org/pdf/2310.04566.pdf)
- (arXiv 2023.11) M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place, [[Paper]](https://arxiv.org/pdf/2311.00926.pdf), [[Project]](https://m2-t2.github.io/)
- (arXiv 2023.11) FViT-Grasp: Grasping Objects With Using Fast Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.13986.pdf)
- (arXiv 2024.01) MResT: Multi-Resolution Sensing for Real-Time Control with Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2401.14502.pdf), [[Code]](http://tinyurl.com/multi-res-realtime-control)
- (arXiv 2024.02) EffLoc: Lightweight Vision Transformer for Efficient 6-DOF Camera Relocalization, [[Paper]](https://arxiv.org/pdf/2402.13537.pdf)

### Salient Detection
- (arXiv 2021.04) Transformer Transforms Salient Object Detection and Camouflaged Object Detection, [[Paper]](https://arxiv.org/pdf/2104.10127.pdf)
- (arXiv 2021.04) Visual Saliency Transformer, [[Paper]](https://arxiv.org/pdf/2104.12099.pdf)
- (arXiv 2021.04) CoSformer: Detecting Co-Salient Object with Transformers, [[Paper]](https://arxiv.org/pdf/2104.14729.pdf)
- (arXiv 2021.08) Unifying Global-Local Representations in Salient Object Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2108.02759.pdf), [[Code]](https://github.com/OliverRensu/GLSTR)
- (arXiv 2021.08) TriTransNet: RGB-D Salient Object Detection with a Triplet Transformer Embedding Network, [[Paper]](https://arxiv.org/pdf/2108.03990.pdf), [[Code]](https://github.com/liuzywen/TriTransNet)
- (arXiv 2021.08) Boosting Salient Object Detection with Transformer-based Asymmetric Bilateral U-Net, [[Paper]](https://arxiv.org/pdf/2108.07851.pdf)
- (arXiv 2021.12) Transformer-based Network for RGB-D Saliency Detection, [[Paper]](https://arxiv.org/pdf/2112.00582.pdf)
- (arXiv 2021.12) MTFNet: Mutual-Transformer Fusion Network for RGB-D Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2112.01177.pdf)
- (arXiv 2021.12) Learning Generative Vision Transformer with Energy-Based Latent Space for Saliency Prediction, [[Paper]](https://arxiv.org/pdf/2112.13528.pdf)
- (arXiv 2022.03) DFTR: Depth-supervised Hierarchical Feature Fusion Transformer for Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2112.13528.pdf)
- (arXiv 2022.03) GroupTransNet: Group Transformer Network for RGB-D Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2203.10785.pdf)
- (arXiv 2022.03) Unsupervised Salient Object Detection with Spectral Cluster Voting, [[Paper]](https://arxiv.org/pdf/2203.12614.pdf), [[Code]](https://github.com/NoelShin/selfmask)
- (arXiv 2022.05) SelfReformer: Self-Refined Network with Transformer for Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2205.11283.pdf)
- (arXiv 2022.06) Dual Swin-Transformer based Mutual Interactive Network for RGB-D Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2206.03105.pdf)
- (arXiv 2022.07) TANet: Transformer-based Asymmetric Network for RGB-D Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2207.01172.pdf), [[Code]](https://github.com/lc012463/TANet)
- (arXiv 2022.07) Mirror Complementary Transformer Network for RGB-thermal Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2207.03558.pdf), [[Code]](https://github.com/jxr326/SwinMCNet)
- (arXiv 2022.07) SiaTrans: Siamese Transformer Network for RGB-D Salient Object Detection with Depth Image Classification, [[Paper]](https://arxiv.org/pdf/2207.04224.pdf)
- (arXiv 2022.07) Panoramic Vision Transformer for Saliency Detection in 360掳 Videos, [[Paper]](https://arxiv.org/pdf/2209.08956.pdf)
- (arXiv 2023.01) HRTransNet: HRFormer-Driven Two-Modality Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2301.03036.pdf), [[Code]](https://github.com/liuzywen/HRTransNet)
- (arXiv 2023.02) Hierarchical Cross-modal Transformer for RGB-D Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2302.08052.pdf)
- (arXiv 2023.05) Discriminative Co-Saliency and Background Mining Transformer for Co-Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2305.00514.pdf), [[Code]](https://github.com/dragonlee258079/DMT)
- (arXiv 2023.05) Salient Mask-Guided Vision Transformer for Fine-Grained Classification, [[Paper]](https://arxiv.org/pdf/2305.07102.pdf)
- (arXiv 2023.08) Recurrent Multi-scale Transformer for High-Resolution Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2308.03826.pdf),[[Code]](https://github.com/DrowsyMon/RMFormer)
- (arXiv 2023.08) Distortion-aware Transformer in 360掳 Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2308.03359.pdf),[[Code]](https://github.com/yjzhao19981027/DATFormer/)
- (arXiv 2023.09) UniST: Towards Unifying Saliency Transformer for Video Saliency Prediction and Detection, [[Paper]](https://arxiv.org/pdf/2309.08220.pdf)
- (arXiv 2023.09) Salient Object Detection in Optical Remote Sensing Images Driven by Transformer, [[Paper]](https://arxiv.org/pdf/2309.08206.pdf), [[Code]](https://github.com/LiJiahang617/Road-Former)
- (arXiv 2023.10) VST++: Efficient and Stronger Visual Saliency Transformer, [[Paper]](https://arxiv.org/pdf/2310.11725.pdf), [[Code]](https://github.com/LiJiahang617/Road-Former)
- (arXiv 2024.03) A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2402.18922.pdf), [[Code]](https://github.com/linuxsino/SENet)

### Scene
- (arXiv 2020.12) SceneFormer: Indoor Scene Generation with Transformers, [[Paper]](https://arxiv.org/pdf/2012.09793.pdf)
- (arXiv 2021.05) SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation, [[Paper]](https://arxiv.org/pdf/2105.04447.pdf)
- (arXiv 2021.06) P2T: Pyramid Pooling Transformer for Scene Understanding, [[Paper]](https://arxiv.org/pdf/2106.12011.pdf), [[Code]](https://github.com/yuhuan-wu/P2T)
- (arXiv 2021.07) Scenes and Surroundings: Scene Graph Generation using Relation Transformer, [[Paper]](https://arxiv.org/pdf/2107.05448.pdf)
- (arXiv 2021.07) Spatial-Temporal Transformer for Dynamic Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2107.12309.pdf)
- (arXiv 2021.09) BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2109.05346.pdf)
- (arXiv 2021.11) Compositional Transformers for Scene Generation, [[Paper]](https://arxiv.org/pdf/2111.08960.pdf)
- (arXiv 2021.11) Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations, [[Paper]](https://arxiv.org/pdf/2111.13152.pdf), [[Project]](https://srt-paper.github.io/)
- (arXiv 2021.12) SGTR: End-to-end Scene Graph Generation with Transformer, [[Paper]](https://arxiv.org/pdf/2112.12970.pdf)
- (arXiv 2022.01) RelTR: Relation Transformer for Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2201.11460.pdf), [[Code]](https://github.com/yrcong/RelTR)
- (arXiv 2022.03) Relationformer: A Unified Framework for Image-to-Graph Generation, [[Paper]](https://arxiv.org/pdf/2203.10202.pdf)
- (arXiv 2022.05) ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions, [[Paper]](https://arxiv.org/pdf/2205.12231.pdf), [[Code]](https://github.com/DifanLiu/ASSET)
- (arXiv 2022.06) Object Scene Representation Transformer, [[Paper]](https://arxiv.org/pdf/2206.06922.pdf), [[Project]](https://osrt-paper.github.io/)
- (arXiv 2022.11) SG-Shuffle: Multi-aspect Shuffle Transformer for Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2211.04773.pdf)
- (arXiv 2022.11) Iterative Scene Graph Generation with Generative Transformers, [[Paper]](https://arxiv.org/pdf/2211.16636.pdf)
- (arXiv 2022.12) SrTR: Self-reasoning Transformer with Visual-linguistic Knowledge for Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2212.09329.pdf)
- (arXiv 2023.03) Transformer-based Image Generation from Scene Graphs, [[Paper]](https://arxiv.org/pdf/2303.04634.pdf), [[Code]](https://github.com/perceivelab/trf-sg2im)
- (arXiv 2023.03) Revisiting Transformer for Point Cloud-based 3D Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2303.11048.pdf)
- (arXiv 2023.03) Learning Similarity between Scene Graphs and Images with Transformers, [[Paper]](https://arxiv.org/pdf/2304.00590.pdf)
- (arXiv 2023.04) RePAST: Relative Pose Attention Scene Representation Transformer, [[Paper]](https://arxiv.org/pdf/2304.00947.pdf)
- (arXiv 2023.05) HSCNet++: Hierarchical Scene Coordinate Classification and Regression for Visual Localization with Transformer, [[Paper]](https://arxiv.org/pdf/2305.03595.pdf)
- (arXiv 2023.05) PanoContext-Former: Panoramic Total Scene Understanding with a Transformer, [[Paper]](https://arxiv.org/pdf/2305.12497.pdf)
- (arXiv 2023.06) InvPT++: Inverted Pyramid Multi-Task Transformer for Visual Scene Understanding, [[Paper]](https://arxiv.org/pdf/2306.04842.pdf), [[Code]](https://github.com/prismformore/Multi-Task-Transformer/tree/main/InvPT)
- (arXiv 2023.06) ViTEraser: Harnessing the Power of Vision Transformers for Scene Text Removal with SegMIM Pretraining, [[Paper]](https://arxiv.org/pdf/2306.12106.pdf), [[Code]](https://github.com/shannanyinxiang/ViTEraser)
- (arXiv 2023.08) Generalized Unbiased Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2308.04802.pdf)
- (arXiv 2023.08) Vision Relation Transformer for Unbiased Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2308.09472.pdf),[[Code]](https://github.com/visinf/veto)
- (arXiv 2023.09) RoadFormer: Duplex Transformer for RGB-Normal Semantic Road Scene Parsing, [[Paper]](https://arxiv.org/pdf/2309.10356.pdf),[[Code]](https://github.com/visinf/veto)
- (arXiv 2023.09) Spatial-Temporal Knowledge-Embedded Transformer for Video Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2309.13237.pdf)
- (arXiv 2023.10) Towards Grouping in Large Scenes with Occlusion-aware Spatio-temporal Transformers, [[Paper]](https://arxiv.org/pdf/2310.19447.pdf),[[Code]](http://cic.tju.edu.cn/faculty/likun/projects/GroupTrans)
- (arXiv 2023.11) Towards a Unified Transformer-based Framework for Scene Graph Generation and Human-object Interaction Detection, [[Paper]](https://arxiv.org/pdf/2311.01755.pdf)
- (arXiv 2023.11) TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding, [[Paper]](https://arxiv.org/pdf/2311.03427.pdf),[[Code]](https://github.com/tb2-sy/TSP-Transformer)
- (arXiv 2023.11) VLPrompt: Vision-Language Prompting for Panoptic Scene Graph Generation, [[Paper]](https://arxiv.org/pdf/2311.16492.pdf)
- (arXiv 2023.12) Gaussian Grouping: Segment and Edit Anything in 3D Scenes, [[Paper]](https://arxiv.org/pdf/2312.00732.pdf),[[Code]](https://github.com/lkeab/gaussian-grouping)
- (arXiv 2024.01) Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via Transformer-Based 360 Image Outpainting, [[Paper]](https://arxiv.org/pdf/2401.10564.pdf)
- (arXiv 2024.01) SGTR+: End-to-end Scene Graph Generation with Transformer, [[Paper]](https://arxiv.org/pdf/2401.12835.pdf),[[Code]](https://github.com/Scarecrow0/SGTR)
- (arXiv 2024.02) S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR, [[Paper]](https://arxiv.org/pdf/2402.14461.pdf)
- (arXiv 2024.02) Vision Transformers with Natural Language Semantics, [[Paper]](https://arxiv.org/pdf/2402.17863.pdf)
- (arXiv 2024.03) Can Transformers Capture Spatial Relations between Objects, [[Paper]](https://arxiv.org/pdf/2403.00729.pdf),[[Code]](https://sites.google.com/view/spatial-relation)

### Self-supervised Learning
- (arXiv 2021.03) Can Vision Transformers Learn without Natural Images? [[Paper]](https://arxiv.org/abs/2103.13023), [[Code]](https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/)
- (arXiv 2021.04) An Empirical Study of Training Self-Supervised Visual Transformers, [[Paper]](https://arxiv.org/abs/2104.02057)
- (arXiv 2021.04) SiT: Self-supervised vIsion Transformer, [[Paper]](https://arxiv.org/abs/2104.03602)], [[Code]](https://github.com/Sara-Ahmed/SiT)
- (arXiv 2021.04) VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text, [[Paper]](https://arxiv.org/abs/2104.11178), [[Code]](https://github.com/Sara-Ahmed/SiT)
- (arXiv 2021.04) Emerging Properties in Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.14294.pdf), [[Code]](https://github.com/facebookresearch/dino)
- (arXiv 2021.05) Self-Supervised Learning with Swin Transformers, [[Paper]](https://arxiv.org/pdf/2105.04553.pdf), [[Code]](https://github.com/SwinTransformer/Transformer-SSL)
- (arXiv 2021.06) MST: Masked Self-Supervised Transformer for Visual Representation, [[Paper]](https://arxiv.org/pdf/2106.05656.pdf)
- (arXiv 2021.06) Efficient Self-supervised Vision Transformers for Representation Learning, [[Paper]](https://arxiv.org/pdf/2106.09785.pdf)
- (arXiv 2021.09) Localizing Objects with Self-Supervised Transformers and no Labels, [[Paper]](https://arxiv.org/pdf/2109.14279.pdf)
- (arXiv 2021.10) Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.05340.pdf), [[Code]](https://github.com/ChongjianGE/CARE)
- (arXiv 2022.01) RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training, [[Paper]](https://arxiv.org/pdf/2201.06857.pdf), [[Code]](https://github.com/ChongjianGE/CARE)
- (arXiv 2022.02) Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut, [[Paper]](https://arxiv.org/pdf/2202.11539.pdf), [[Project]](https://www.m-psi.fr/Papers/TokenCut2022/)
- (arXiv 2022.03) Mugs: A Multi-Granular Self-Supervised Learning Framework, [[Paper]](https://arxiv.org/pdf/2203.14415.pdf), [[Code]](https://github.com/sail-sg/mugs)
- (arXiv 2022.04) A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition, [[Paper]](https://arxiv.org/pdf/2204.02803.pdf)
- (arXiv 2022.04) DILEMMA: Self-Supervised Shape and Texture Learning with Transformers, [[Paper]](https://arxiv.org/pdf/2204.04788.pdf)
- (arXiv 2022.04) Self-supervised Vision Transformers for Joint SAR-optical Representation Learning, [[Paper]](https://arxiv.org/pdf/2204.05381.pdf)
- (arXiv 2022.05) UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog, [[Paper]](https://arxiv.org/pdf/2205.00423.pdf)
- (arXiv 2022.05) Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality, [[Paper]](https://arxiv.org/pdf/2205.10063.pdf), [[Code]](https://github.com/implus/UM-MAE)
- (arXiv 2022.05) Self-Supervised Pre-training of Vision Transformers for Dense Prediction Tasks, [[Paper]](https://arxiv.org/pdf/2205.15173.pdf), [[Code]](https://github.com/implus/UM-MAE)
- (arXiv 2022.05) A Closer Look at Self-supervised Lightweight Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.14443.pdf)
- (arXiv 2022.06) Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.00481.pdf), [[Code]](https://github.com/guglielmocamporese/relvit)
- (arXiv 2022.06) Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning, [[Paper]](https://arxiv.org/pdf/2206.02647.pdf), [[Code]](https://github.com/mahmoodlab/HIPT)
- (arXiv 2022.06) Exploring Feature Self-relation for Self-supervised Transformer, [[Paper]](https://arxiv.org/pdf/2206.05184.pdf)
- (arXiv 2022.06) Position Labels for Self-Supervised Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.04981.pdf)
- (arXiv 2022.06) Adapting Self-Supervised Vision Transformers by Probing Attention-Conditioned Masking Consistency, [[Paper]](https://arxiv.org/pdf/2206.08222.pdf), [[Code]](https://github.com/virajprabhu/PACMAC)
- (arXiv 2022.06) Patch-level Representation Learning for Self-supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.07990.pdf), [[Code]](https://github.com/alinlab/SelfPatch)
- (arXiv 2022.07) Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning, [[Paper]](https://arxiv.org/pdf/2207.09644.pdf), [[Code]](https://github.com/yuxiaochen1103/Hi-TRS)
- (arXiv 2022.08) Self-Supervised Vision Transformers for Malware Detection, [[Paper]](https://arxiv.org/pdf/2208.07049.pdf)
- (arXiv 2022.09) Prior Knowledge-Guided Attention in Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.03745.pdf)
- (arXiv 2022.10) Attention Distillation: self-supervised vision transformer students need more guidance, [[Paper]](https://arxiv.org/pdf/2210.00944.pdf), [[Code]](https://github.com/wangkai930418/attndistill)
- (arXiv 2022.10) Histopathological Image Classification based on Self-Supervised Vision Transformer and Weak Labels, [[Paper]](https://arxiv.org/pdf/2210.09021.pdf), [[Code]](https://github.com/gokberkgul/selflearning-transformer-mil)
- (arXiv 2022.10) Learning Self-Regularized Adversarial Views for Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.08458.pdf), [[Code]](https://github.com/Trent-tangtao/AutoView)
- (arXiv 2022.10) SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic Retinopathy Grading, [[Paper]](https://arxiv.org/pdf/2210.10969.pdf), [[Code]](https://github.com/YijinHuang/SSiT)
- (arXiv 2022.10) PatchRot: A Self-Supervised Technique for Training Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.15722.pdf), [[Code]](https://github.com/s-chh/patchrot)
- (arXiv 2022.10) Foreign Object Debris Detection for Airport Pavement Images based on Self-supervised Localization and Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.16901.pdf)
- (arXiv 2022.12) Location-Aware Self-Supervised Transformers, [[Paper]](https://arxiv.org/pdf/2212.02400.pdf), [[Code]](https://github.com/google-research/scenic/tree/main/scenic/projects/loca)
- (arXiv 2023.02) Real Estate Property Valuation using Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.00117.pdf)
- (arXiv 2023.02) Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations, [[Paper]](https://arxiv.org/pdf/2302.14138.pdf), [[Code]](https://github.com/VITA-Group/layerGraftedPretraining_ICLR23.git)
- (arXiv 2023.03) ST-KeyS: Self-Supervised Transformer for Keyword Spotting in Historical Handwritten Documents, [[Paper]](https://arxiv.org/pdf/2303.03127.pdf)
- (arXiv 2023.03) AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+, [[Paper]](https://arxiv.org/pdf/2303.07598.pdf), [[Code]](https://github.com/maple-research-lab/AdPE)
- (arXiv 2023.03) Contrastive Transformer: Contrastive Learning Scheme with Transformer innate Patches, [[Paper]](https://arxiv.org/pdf/2303.14806.pdf)
- (arXiv 2023.04) Token Boosting for Robust Self-Supervised Visual Transformer Pre-training, [[Paper]](https://arxiv.org/pdf/2304.04175.pdf)
- (arXiv 2023.04) MOST: Multiple Object localization with Self-supervised Transformers for object discovery, [[Paper]](https://arxiv.org/pdf/2304.05387.pdf)
- (arXiv 2023.05) LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input, [[Paper]](https://arxiv.org/pdf/2304.14765.pdf)
- (arXiv 2023.05) What Do Self-Supervised Vision Transformers Learn, [[Paper]](https://arxiv.org/pdf/2305.00729.pdf), [[Code]](https://github.com/naver-ai/cl-vs-mim)
- (arXiv 2023.06) Improving Visual Prompt Tuning for Self-supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.05067.pdf), [[Code]](https://github.com/ryongithub/GatedPromptTuning)
- (arXiv 2023.06) DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency, [[Paper]](https://arxiv.org/pdf/2306.04654.pdf)
- (arXiv 2023.07) Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation, [[Paper]](https://arxiv.org/pdf/2307.03407.pdf)
- (arXiv 2023.07) Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments, [[Paper]](https://arxiv.org/pdf/2307.09361.pdf)
- (arXiv 2023.08) Emergence of Segmentation with Minimalistic White-Box Transformers, [[Paper]](https://arxiv.org/pdf/2308.16271.pdf), [[Code]](https://ma-lab-berkeley.github.io/CRATE)
- (arXiv 2023.10) Limited Data, Unlimited Potential:A Study on ViTs Augmented by Masked Autoencoders, [[Paper]](https://arxiv.org/pdf/2310.20704.pdf), [[Code]](https://github.com/dominickrei/Limited-data-vits)
- (arXiv 2023.11) LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs, [[Paper]](https://arxiv.org/pdf/2311.04069.pdf), [[Code]](https://github.com/dominickrei/Limited-data-vits)
- (arXiv 2024.01) Analyzing Local Representations of Self-supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.00463.pdf)

### Semantic Segmentation 
- (arXiv 2020.12) Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers, [[Paper]](https://arxiv.org/pdf/2012.15840), [[Code]](https://github.com/fudan-zvg/SETR)
- (arXiv 2021.01) Trans2Seg: Transparent Object Segmentation with Transformer, [[Paper]](https://arxiv.org/pdf/2101.08461), [[Code]](https://github.com/xieenze/Trans2Seg)
- (arXiv 2021.05) Segmenter: Transformer for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2105.05633.pdf), [[Code]](https://github.com/rstrudel/segmenter)
- (arXiv 2021.06) SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2105.15203.pdf), [[Code]](https://github.com/NVlabs/SegFormer)
- (arXiv 2021.06) Fully Transformer Networks for Semantic Image Segmentation, [[Paper]](https://arxiv.org/pdf/2106.04108.pdf)
- (arXiv 2021.06) Transformer Meets Convolution: A Bilateral Awareness Network for Semantic Segmentation of Very Fine Resolution Urban Scene Images, [[Paper]](https://arxiv.org/pdf/2106.12413.pdf)
- (arXiv 2021.06) OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on OffRoad environments, [[Paper]](https://arxiv.org/pdf/2106.13963.pdf)
- (arXiv 2021.07) A Unified Efficient Pyramid Transformer for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2107.14209.pdf)
- (arXiv 2021.08) Boosting Few-shot Semantic Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2108.02266.pdf), [[Code]](https://github.com/GuoleiSun/TRFS)
- (arXiv 2021.08) Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer, [[Paper]](https://arxiv.org/pdf/2108.03032.pdf), [[Code]](https://github.com/zhiheLu/CWTfor-FSS)
- (arXiv 2021.08) Flying Guide Dog: Walkable Path Discovery for the Visually Impaired Utilizing Drones and Transformer-based Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2108.07007.pdf), [[Code]](https://github.com/EckoTan0804/flying-guide-dog)
- (arXiv 2021.08) Trans4Trans: Efficient Transformer for Transparent Object and Semantic Scene Segmentation in Real-World Navigation Assistance, [[Paper]](https://arxiv.org/pdf/2108.09174.pdf), [[Code]](https://github.com/jamycheung/Trans4Trans)
- (arXiv 2021.08) Evaluating Transformer based Semantic Segmentation Networks for Pathological Image Segmentation, [[Paper]](https://arxiv.org/pdf/2108.11993.pdf)
- (arXiv 2021.08) Semantic Segmentation on VSPW Dataset through Aggregation of Transformer Models, [[Paper]](https://arxiv.org/pdf/2109.01316.pdf)
- (arXiv 2021.09) Efficient Hybrid Transformer: Learning Global-local Context for Urban Sence Segmentation, [[Paper]](https://arxiv.org/pdf/2109.08937.pdf)
- (arXiv 2021.11) HRViT: Multi-Scale High-Resolution Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.01236.pdf)
- (arXiv 2021.11) Dynamically pruning segformer for efficient semantic segmentation, [[Paper]](https://arxiv.org/pdf/2111.09499.pdf)
- (arXiv 2021.11) APANet: Adaptive Prototypes Alignment Network for Few-Shot Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2111.12263.pdf)
- (arXiv 2021.11) Adaptive Fourier Neural Operators: Efficient Token Mixers for Transformers, [[Paper]](https://arxiv.org/pdf/2111.13587.pdf)
- (arXiv 2021.11) GETAM: Gradient-weighted Element-wise Transformer Attention Map for Weakly-supervised Semantic segmentation, [[Paper]](https://arxiv.org/pdf/2112.02841.pdf)
- (arXiv 2021.12) iSegFormer: Interactive Image Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2112.11325.pdf), [[Code]](https://github.com/qinliuliuqin/iSegFormer.git)
- (arXiv 2021.12) SeMask: Semantically Masked Transformers for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2112.12782.pdf), [[Code]](https://github.com/Picsart-AI-Research/SeMask-Segmentation)
- (arXiv 2022.01) Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention, [[Paper]](https://arxiv.org/pdf/2201.01615.pdf), [[Code]](https://github.com/yan-hao-tian/lawin)
- (arXiv 2022.01) Pyramid Fusion Transformer for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2201.04019.pdf)
- (arXiv 2022.01) Dual-Flattening Transformers through Decomposed Row and Column Queries for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2201.09139.pdf)
- (arXiv 2022.01) GroupViT: Semantic Segmentation Emerges from Text Supervision, [[Paper]](https://arxiv.org/pdf/2202.11094.pdf), [[Code]](https://jerryxu.net/GroupViT/)
- (arXiv 2022.03) Transformer-based Knowledge Distillation for Efficient Semantic Segmentation of Road-driving Scenes, [[Paper]](https://arxiv.org/pdf/2202.13393.pdf), [[Code]](https://github.com/RuipingL/SKR_PEA)
- (arXiv 2022.03) Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2203.01452.pdf), [[Code]](https://github.com/jamycheung/Trans4PASS)
- (arXiv 2022.03) Multi-class Token Transformer for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2203.02891.pdf), [[Code]](https://github.com/xulianuwa/MCTformer)
- (arXiv 2022.03) Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2203.02664.pdf), [[Code]](https://github.com/rulixiang/afa)
- (arXiv 2022.03) BEVSegFormer: Bird鈥檚 Eye View Semantic Segmentation From Arbitrary Camera Rigs, [[Paper]](https://arxiv.org/pdf/2203.04050.pdf)
- (arXiv 2022.03) CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2203.04838.pdf), [[Code]](https://github.com/huaaaliu/RGBX_Semantic_Segmentation)
- (arXiv 2022.03) TransCAM: Transformer Attention-based CAM Refinement for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2203.07239.pdf), [[Code]](https://github.com/liruiwen/TransCAM)
- (arXiv 2022.03) Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2203.07988.pdf), [[Code]](https://github.com/alpc91/TransDA)
- (arXiv 2022.03) WegFormer: Transformers for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2203.08421.pdf)
- (arXiv 2022.03) StructToken : Rethinking Semantic Segmentation with Structural Prior, [[Paper]](https://arxiv.org/pdf/2203.12612.pdf)
- (arXiv 2022.03) Feature Selective Transformer for Semantic Image Segmentation, [[Paper]](https://arxiv.org/pdf/2203.14124.pdf)
- (arXiv 2022.03) Semantic Segmentation by Early Region Proxy, [[Paper]](https://arxiv.org/pdf/2203.14043.pdf), [[Code]](https://github.com/YiF-Zhang/RegionProxy)
- (arXiv 2022.03) Dynamic Focus-aware Positional Queries for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2204.01244.pdf), [[Code]](https://github.com/zip-group/FASeg)
- (arXiv 2022.03) TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2204.05525.pdf), [[Code]](https://github.com/hustvl/TopFormer)
- (arXiv 2022.04) Unsupervised Hierarchical Semantic Segmentation with Multiview Cosegmentation and Clustering Transformers, [[Paper]](https://arxiv.org/pdf/2204.11432.pdf), [[Code]](https://github.com/twke18/HSG)
- (arXiv 2022.05) Cross-view Transformers for real-time Map-view Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2205.02833.pdf), [[Code]](https://github.com/bradyz/cross_view_transformers)
- (arXiv 2022.05) Transformer Scale Gate for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2205.07056.pdf)
- (arXiv 2022.06) Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2206.06363.pdf), [[Code]](https://github.com/wvangansbeke/MaskDistill)
- (arXiv 2022.06) Semantic Labeling of High Resolution Images Using EfficientUNets and Transformers, [[Paper]](https://arxiv.org/pdf/2206.09731.pdf)
- (arXiv 2022.07) Improving Semantic Segmentation in Transformers using Hierarchical Inter-Level Attention, [[Paper]](https://arxiv.org/pdf/2207.02126.pdf), [[Code]](https://www.cs.toronto.edu/~garyleung/hila/)
- (arXiv 2022.07) Self-attention on Multi-Shifted Windows for Scene Segmentation, [[Paper]](https://arxiv.org/pdf/2207.04403.pdf), [[Code]](https://github.com/yutao1008/MSwin)
- (arXiv 2022.07) eX-ViT: A Novel eXplainable Vision Transformer for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2207.05358.pdf)
- (arXiv 2022.07) Visual Representation Learning with Transformer: A Sequence-to-Sequence Perspective, [[Paper]](https://arxiv.org/pdf/2207.09339.pdf), [[Code]](https://github.com/fudan-zvg/SETR)
- (arXiv 2022.07) SSformer: A Lightweight Transformer for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2208.02034.pdf), [[Code]](https://github.com/shiwt03/SSformer)
- (arXiv 2022.08) TokenCut: Segmenting Objects in Images and Videos with Self-supervised Transformer and Normalized Cut, [[Paper]](https://arxiv.org/pdf/2209.00383.pdf), [[Code]](https://www.m-psi.fr/Papers/TokenCut2022/)
- (arXiv 2022.09) Transformer-CNN Cohort: Semi-supervised Semantic Segmentation by the Best of Both Students, [[Paper]](https://arxiv.org/pdf/2209.02178.pdf)
- (arXiv 2022.09) Transformer-based Flood Scene Segmentation for Developing Countries, [[Paper]](https://arxiv.org/pdf/2210.04218.pdf)
- (arXiv 2022.10) SegViT: Semantic Segmentation with Plain Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.05844.pdf)
- (arXiv 2022.10) RTFormer: Efficient Design for Real-Time Semantic Segmentation with Transformer, [[Paper]](https://arxiv.org/pdf/2210.07124.pdf), [[Code]](https://github.com/PaddlePaddle/PaddleSeg)
- (arXiv 2022.10) Intermediate Prototype Mining Transformer for Few-Shot Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2210.06780.pdf), [[Code]](https://github.com/LIUYUANWEI98/IPMT)
- (arXiv 2022.10) SemFormer: Semantic Guided Activation Transformer for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2210.14618.pdf), [[Code]](https://github.com/JLChen-C/SemFormer)
- (arXiv 2022.10) RGB-T Semantic Segmentation with Location, Activation, and Sharpening, [[Paper]](https://arxiv.org/pdf//2210.14530.pdf), [[Code]](https://github.com/MathLee/LASNet)
- (arXiv 2022.10) Max Pooling with Vision Transformers reconciles class and shape in weakly supervised semantic segmentation, [[Paper]](https://arxiv.org/pdf/2210.17400.pdf), [[Code]](https://github.com/deepplants/vit-pcm)
- (arXiv 2022.10) DepthFormer: Multimodal Positional Encodings and Cross-Input Attention for Transformer-Based Segmentation Networks, [[Paper]](https://arxiv.org/pdf/2211.04188.pdf)
- (arXiv 2022.11) Delving into Transformer for Incremental Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2211.10253.pdf)
- (arXiv 2022.11) CoMFormer: Continual Learning in Semantic and Panoptic Segmentation, [[Paper]](https://arxiv.org/pdf/2211.13999.pdf)
- (arXiv 2022.11) UperFormer: A Multi-scale Transformer-based Decoder for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2211.13928.pdf), [[Code]](https://github.com/shiwt03/UperFormer)
- (arXiv 2022.11) Exploring Consistency in Cross-Domain Transformer for Domain Adaptive Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2211.14703.pdf), [[Code]](https://github.com/shiwt03/UperFormer)
- (arXiv 2022.12) SASFormer: Transformers for Sparsely Annotated Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2212.02019.pdf), [[Code]](https://github.com/su-hui-zz/SASFormer)
- (arXiv 2022.12) IncepFormer: Efficient Inception Transformer with Pyramid Pooling for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2212.03035.pdf), [[Code]](https://github.com/shendu0321/IncepFormer)
- (arXiv 2022.12) Gaussian Radar Transformer for Semantic Segmentation in Noisy Radar Data, [[Paper]](https://arxiv.org/pdf/2212.03690.pdf)
- (arXiv 2022.12) One-Shot Domain Adaptive and Generalizable Semantic Segmentation with Class-Aware Cross-Domain Transformers, [[Paper]](https://arxiv.org/pdf/2212.07292.pdf)
- (arXiv 2022.12) Full Contextual Attention for Multi-resolution Transformers in Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2212.07890.pdf)
- (arXiv 2022.12) CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2212.09506.pdf)
- (arXiv 2022.12) Representation Separation for Semantic Segmentation with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.13764.pdf)
- (arXiv 2022.12) AttEntropy: Segmenting Unknown Objects in Complex Scenes using the Spatial Attention Entropy of Semantic Segmentation Transformers, [[Paper]](https://arxiv.org/pdf/2212.14397.pdf)
- (arXiv 2023.01) Head-Free Lightweight Semantic Segmentation with Linear Transformer, [[Paper]](https://arxiv.org/pdf/2301.04648.pdf), [[Code]](https://github.com/dongbo811/AFFormer)
- (arXiv 2023.01) RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2301.10222.pdf), [[Code]](https://github.com/valeoai/rangevit)
- (arXiv 2023.01) Semantic Segmentation Enhanced Transformer Model for Human Attention Prediction, [[Paper]](https://arxiv.org/pdf/2301.11022.pdf)
- (arXiv 2023.01) Squeeze-enhanced Axial Transformer for mobile semantic segmentation, [[Paper]](https://arxiv.org/pdf/2301.13156.pdf), [[Code]](https://github.com/fudan-zvg/SeaFormer)
- (arXiv 2023.02) Lithium Metal Battery Quality Control via Transformer-CNN Segmentation, [[Paper]](https://arxiv.org/pdf/2302.04824.pdf)
- (arXiv 2023.02) TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2302.08594.pdf)
- (arxiv 2023.02) Lightweight Real-time Semantic Segmentation Network with Efficient Transformer and CNN, [[Paper]](https://arxiv.org/pdf/2302.10484.pdf), [[Code]](https://github.com/IVIPLab/LETNet)
- (arxiv 2023.02) A Convolutional Vision Transformer for Semantic Segmentation of Side-Scan Sonar Data, [[Paper]](https://arxiv.org/pdf/2302.12416.pdf), [[Code]](https://github.com/hayatrajani/s3seg-vit)
- (arxiv 2023.02) Self Correspondence Distillation for End-to-End Weakly-Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2302.13765.pdf), [[Code]](https://github.com/Rongtao-Xu/RepresentationLearning/tree/main/SCD-AAAI2023)
- (arxiv 2023.02) TransAdapt: A Transformative Framework for Online Test Time Adaptive Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2302.14611.pdf)
- (arxiv 2023.02) Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors, [[Paper]](https://arxiv.org/pdf/2302.14746.pdf)
- (arxiv 2023.03) DMSA: Dynamic Multi-scale Unsupervised Semantic Segmentation Based on Adaptive Affinity, [[Paper]](https://arxiv.org/pdf/2303.00199.pdf)
- (arxiv 2023.03) MP-Former: Mask-Piloted Transformer for Image Segmentation, [[Paper]](https://arxiv.org/pdf/2303.07336.pdf), [[Code]](https://github.com/IDEA-Research/MP-Former)
- (arxiv 2023.03) MECPformer: Multi-estimations Complementary Patch with CNN-Transformers for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2303.10689.pdf), [[Code]](https://github.com/ChunmengLiu1/MECPformer)
- (arxiv 2023.03) Few-Shot 3D Point Cloud Semantic Segmentation via Stratified Class-Specific Attention Based Transformer Network, [[Paper]](https://arxiv.org/pdf/2303.15654.pdf), [[Code]](https://github.com/czzhang179/SCAT)
- (arxiv 2023.04) WeakTr: Exploring Plain Vision Transformer for Weakly-supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2304.01184.pdf), [[Code]](https://github.com/hustvl/WeakTr)
- (arxiv 2023.04) OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction, [[Paper]](https://arxiv.org/pdf/2304.05316.pdf), [[Code]](https://github.com/zhangyp15/OccFormer)
- (arxiv 2023.04) DeepSegmenter: Temporal Action Localization for Detecting Anomalies in Untrimmed Naturalistic Driving Videos, [[Paper]](https://arxiv.org/pdf/2304.08261.pdf)
- (arxiv 2023.05) Exploring vision transformer layer choosing for semantic segmentation, [[Paper]](https://arxiv.org/pdf/2305.01279.pdf)
- (arxiv 2023.05) MTLSegFormer: Multi-task Learning with Transformers for Semantic Segmentation in Precision Agriculture, [[Paper]](https://arxiv.org/pdf/2305.02813.pdf)
- (arxiv 2023.05) Mitigating Undisciplined Over-Smoothing in Transformer for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2305.03112.pdf)
- (arxiv 2023.05) AdaptiveClick: Clicks-aware Transformer with Adaptive Focal Loss for Interactive Image Segmentation, [[Paper]](https://arxiv.org/pdf/2305.04276.pdf), [[Code]](https://github.com/lab206/AdaptiveClick)
- (arxiv 2023.05) DBAT: Dynamic Backward Attention Transformer for Material Segmentation with Cross-Resolution Patches, [[Paper]](https://arxiv.org/pdf/2305.03919.pdf), [[Code]](https://github.com/heng-yuwen/Dynamic-Backward-Attention-Transformer)
- (arxiv 2023.05) Radious: Unveiling the Enigma of Dental Radiology with BEIT Adaptor and Mask2Former in Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2305.06236.pdf)
- (arxiv 2023.05) HGFormer: Hierarchical Grouping Transformer for Domain Generalized Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2305.13031.pdf), [[Code]](https://github.com/dingjiansw101/HGFormer)
- (arxiv 2023.05) Source-Free Domain Adaptation for RGB-D Semantic Segmentation with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.14269.pdf)
- (arxiv 2023.06) Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.02095.pdf), [[Code]](https://tue-mps.github.io/CTS/)
- (arxiv 2023.06) SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2306.03403.pdf), [[Code]](https://github.com/TencentARC/SGAT4PASS)
- (arxiv 2023.06) SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.06289.pdf), [[Code]](https://github.com/zbwxp/SegVit)
- (arxiv 2023.06) VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation, [[Paper]](https://arxiv.org/pdf/2306.06656.pdf), [[Code]](https://github.com/XuZhang1211/VPUFormer)
- (arxiv 2023.06) AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation, [[Paper]](https://arxiv.org/pdf/2306.06842.pdf)
- (arxiv 2023.06) Sea Ice Segmentation From SAR Data by Convolutional Transformer Networks, [[Paper]](https://arxiv.org/pdf/2306.07649.pdf)
- (arxiv 2023.07) Guided Patch-Grouping Wavelet Transformer with Spatial Congruence for Ultra-High Resolution Segmentation, [[Paper]](https://arxiv.org/pdf/2307.00711.pdf)
- (arxiv 2023.07) Learning Content-enhanced Mask Transformer for Domain Generalized Urban-Scene Segmentation, [[Paper]](https://arxiv.org/pdf/2307.00371.pdf)
- (arxiv 2023.07) LEST: Large-scale LiDAR Semantic Segmentation with Transformer, [[Paper]](https://arxiv.org/pdf/2307.09367.pdf)
- (arXiv 2023.07) A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2307.12574.pdf)
- (arXiv 2023.08) Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2307.12574.pdf)
- (arXiv 2023.08) Dynamic Token-Pass Transformers for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2308.01944.pdf)
- (arXiv 2023.08) Category Feature Transformer for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2308.05581.pdf)
- (arXiv 2023.08) Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2308.05493.pdf), [[Project]](https://vlislab22.github.io/DATR/)
- (arXiv 2023.08) MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2308.04829.pdf)
- (arXiv 2023.08) MCTformer+: Multi-Class Token Transformer for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2308.03005.pdf), [[Project]](https://github.com/xulianuwa/MCTformer)
- (arXiv 2023.08) Graph-Segmenter: Graph Transformer with Boundary-aware Attention for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2308.07592.pdf), [[Code]](https://github.com/addmu/Graph-Segmenter)
- (arXiv 2023.08) A Re-Parameterized Vision Transformer (ReVT) for Domain-Generalized Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2308.13331.pdf), [[Code]](https://github.com/ifnspaml/ReVT)
- (arXiv 2023.09) IBAFormer: Intra-batch Attention Transformer for Domain Generalized Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2309.06282.pdf)
- (arXiv 2023.09) CINFormer: Transformer network with multi-stage CNN feature injection for surface defect segmentation, [[Paper]](https://arxiv.org/pdf/2309.12639.pdf)
- (arXiv 2023.09) AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2309.14065.pdf), [[Code]](https://github.com/Fourier7754/AsymFormer)
- (arXiv 2023.10) Superpixel Transformers for Efficient Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2309.16889.pdf)
- (arXiv 2023.10) Dual-Augmented Transformer Network for Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2310.00307.pdf)
- (arXiv 2023.10) Win-Win: Training High-Resolution Vision Transformers from Two Windows, [[Paper]](https://arxiv.org/pdf/2310.00632.pdf)
- (arXiv 2023.10) TransRadar: Adaptive-Directional Transformer for Real-Time Multi-View Radar Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2310.02260.pdf), [[Code]](https://github.com/YahiDar/TransRadar)
- (arXiv 2023.10) Low-Resolution Self-Attention for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2310.05026.pdf)
- (arXiv 2023.10) CLIP for Lightweight Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2310.07394.pdf)
- (arXiv 2023.10) SegmATRon: Embodied Adaptive Semantic Segmentation for Indoor Environment, [[Paper]](https://arxiv.org/pdf/2310.12031.pdf), [[Code]](https://github.com/wingrune/SegmATRon)
- (arXiv 2023.10) Minimalist and High-Performance Semantic Segmentation with Plain Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.12755.pdf), [[Code]](https://github.com/ydhongHIT/PlainSeg)
- (arXiv 2023.10) Cross-attention Spatio-temporal Context Transformer for Semantic Segmentation of Historical Maps, [[Paper]](https://arxiv.org/pdf/2310.12616.pdf), [[Code]](https://github.com/chenyizi086/wu.2023.sigspatial.git)
- (arXiv 2023.10) P2AT: Pyramid Pooling Axial Transformer for Real-time Semantic Segmentation, [[Paper]](https://arxiv.org/abs/2310.15025)
- (arXiv 2023.10) SmooSeg: Smoothness Prior for Unsupervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2310.17874.pdf), [[Code]](https://github.com/mc-lan/SmooSeg)
- (arXiv 2023.10) Mask Propagation for Efficient Video Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2310.18954.pdf), [[Code]](https://github.com/ziplab/MPVSS)
- (arXiv 2023.10) Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2310.19001.pdf), [[Code]](https://github.com/Ferenas/PGSeg)
- (arXiv 2023.10) PAUMER: Patch Pausing Transformer for Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2311.00586.pdf), [[Code]](https://github.com/Ferenas/PGSeg)
- (arXiv 2023.11) Non-Hierarchical Transformers for Pedestrian Segmentation, [[Paper]](https://arxiv.org/pdf/2311.02506.pdf)
- (arXiv 2023.11) SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction Transformers trained under memory constraints, [[Paper]](https://arxiv.org/pdf/2311.11371.pdf)
- (arXiv 2023.11) GIFT: Generative Interpretable Fine-Tuning Transformers, [[Paper]](https://arxiv.org/pdf/2312.00700.pdf), [[Code]](https://savadikarc.github.io/gift)
- (arXiv 2023.12) U-MixFormer: UNet-like Transformer with Mix-Attention for Efficient Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2312.06272.pdf), [[Code]](https://github.com/julian-klitzing/u-mixformer)
- (arXiv 2023.12) SCTNet: Single-Branch CNN with Transformer Semantic Information for Real-Time Segmentation, [[Paper]](https://arxiv.org/pdf/2312.17071.pdf), [[Code]](https://github.com/xzz777/SCTNet)
- (arXiv 2024.01) DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.12820.pdf)
- (arXiv 2024.01) Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2401.17828.pdf), [[Code]](https://github.com/RozhanAhmadi/SWTformer)
- (arXiv 2024.02) ConSept: Continual Semantic Segmentation via Adapter-based Vision Transformer, [[Paper]](https://arxiv.org/pdf/2402.16674.pdf), [[Code]](https://github.com/DongSky/ConSept)
- (arXiv 2024.03) PEM: Prototype-based Efficient MaskFormer for Image Segmentation, [[Paper]](https://arxiv.org/pdf/2402.19422.pdf), [[Code]](https://github.com/NiccoloCavagnero/PEM)
- (arXiv 2024.03) AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2403.01818.pdf), [[Code]](https://github.com/xmed-lab/AllSpark)
- (arXiv 2024.03) Multi-Grained Cross-modal Alignment for Learning Open-vocabulary Semantic Segmentation from Text Supervision, [[Paper]](https://arxiv.org/pdf/2403.03707.pdf)
- (arXiv 2024.03) DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2403.11184.pdf), [[Code]](https://github.com/Wu0409/DuPL)

### Shape
- (WACV'21) End-to-end Lane Shape Prediction with Transformers,  [[Paper]](https://arxiv.org/abs/2011.04233), [[Code]](https://github.com/liuruijin17/LSTR)
- (arXiv 2022.01) ShapeFormer: Transformer-based Shape Completion via Sparse Representation,  [[Paper]](https://arxiv.org/abs/2201.10326), [[Project]](https://shapeformer.github.io/)
- (arXiv 2022.10) AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation, [[Paper]](https://arxiv.org/pdf/2210.12381.pdf)
- (arXiv 2022.10) mm-Wave Radar Hand Shape Classification Using Deformable Transformers, [[Paper]](https://arxiv.org/pdf/2210.13079.pdf)
- (arxiv 2023.07) DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation, [[Paper]](https://arxiv.org/pdf/2307.01831.pdf)
- (arxiv 2023.09) DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image, [[Paper]](https://arxiv.org/pdf/2309.12594.pdf)

### SLAM
- (arXiv 2022.06) AFT-VO: Asynchronous Fusion Transformers for Multi-View Visual Odometry Estimation,  [[Paper]](https://arxiv.org/abs/2206.12946)
- (arXiv 2023.04) TransFusionOdom: Interpretable Transformer-based LiDAR-Inertial Fusion Odometry Estimation,  [[Paper]](https://arxiv.org/abs/2304.07728)

### SNN
- (arXiv 2022.10) Spikformer: When Spiking Neural Network Meets Transformer,  [[Paper]](https://arxiv.org/abs/2209.15425)
- (arxiv 2023.07) Spike-driven Transformer,  [[Paper]](https://arxiv.org/abs/2307.01694), [[Code]](https://github.com/BICLab/Spike-Driven-Transformer)
- (arxiv 2023.08) SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition,  [[Paper]](https://arxiv.org/pdf/2308.04369.pdf), [[Code]](https://github.com/Event-AHU/SSTFormer)
- (arxiv 2023.08) Attention-free Spikformer: Mixing Spike Sequences with Simple Linear Transforms,  [[Paper]](https://arxiv.org/pdf/2308.02557.pdf)
- (arxiv 2023.11) SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer,  [[Paper]](https://arxiv.org/pdf/2311.08806.pdf)
- (arxiv 2023.11) Spiking Neural Networks with Dynamic Time Steps for Vision Transformers,  [[Paper]](https://arxiv.org/pdf/2311.16456.pdf)
- (arxiv 2024.01) Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket,  [[Paper]](https://arxiv.org/pdf/2401.02020.pdf)
- (arxiv 2024.01) TIM: An Efficient Temporal Interaction Module for Spiking Transformer,  [[Paper]](https://arxiv.org/pdf/2401.11687.pdf)
- (arxiv 2024.02) Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer,  [[Paper]](https://arxiv.org/pdf/2402.04798.pdf)
- (arxiv 2024.02) SDiT: Spiking Diffusion Model with Transformer,  [[Paper]](https://arxiv.org/pdf/2402.11588.pdf)
- (arxiv 2024.03) SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks,  [[Paper]](https://arxiv.org/pdf/2403.14302.pdf), [[Code]](https://github.com/xyshi2000/SpikingResformer)

### Style Transfer
- (arXiv 2022.10) Fine-Grained Image Style Transfer with Visual Transformers,  [[Paper]](https://arxiv.org/abs/2210.05176), [[Code]](https://yccyenchicheng.github.io/AutoSDF/)
- (arXiv 2022.10) S2WAT: Image Style Transfer via Hierarchical Vision Transformer using Strips Window Attention,  [[Paper]](https://arxiv.org/abs/2210.12381), [[Code]](https://yccyenchicheng.github.io/AutoSDF/)
- (arXiv 2022.11) Learning Visual Representation of Underwater Acoustic Imagery Using Transformer-Based Style Transfer Method,  [[Paper]](https://arxiv.org/abs/2211.05396)
- (arXiv 2023.01) Edge Enhanced Image Style Transfer via Transformers,  [[Paper]](https://arxiv.org/abs/2301.00592)
- (arXiv 2023.04) Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer,  [[Paper]](https://arxiv.org/abs/2304.11818),[[Code]](https://github.com/ZK-Zhou/spikformer)

### Super-Resolution
- (CVPR'20) Learning Texture Transformer Network for Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2006.04139), [[Code]](https://github.com/researchmm/TTSR)
- (arXiv 2021.06) LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation, [[Paper]](https://arxiv.org/pdf/2006.04139)
- (arXiv 2021.06) Video Super-Resolution Transformer, [[Paper]](https://arxiv.org/pdf/2106.06847.pdf), [[Code]](https://github.com/caojiezhang/VSR-Transformer)
- (arXiv 2021.08) Light Field Image Super-Resolution with Transformers, [[Paper]](https://arxiv.org/pdf/2108.07597.pdf), [[Code]](https://github.com/ZhengyuLiang24/LFT)
- (arXiv 2021.08) Efficient Transformer for Single Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2108.11084.pdf)
- (arXiv 2021.09) Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution, [[Paper]](https://arxiv.org/pdf/2109.02079.pdf)
- (arXiv 2021.12) Implicit Transformer Network for Screen Content Image Continuous Super-Resolution, [[Paper]](https://arxiv.org/pdf/2112.06174.pdf)
- (arXiv 2021.12) On Efficient Transformer and Image Pre-training for Low-level Vision, [[Paper]](https://arxiv.org/pdf/2112.10175.pdf), [[Code]](https://github.com/fenglinglwb/EDT)
- (arXiv 2022.01) Detail-Preserving Transformer for Light Field Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2201.00346.pdf), [[Code]](https://github.com/BITszwang/DPT)
- (arXiv 2022.03) Rich CNN-Transformer Feature Aggregation Networks for Super-Resolution, [[Paper]](https://arxiv.org/pdf/2203.07682.pdf)
- (arXiv 2022.03) HIPA: Hierarchical Patch Transformer for Single Image Super Resolution, [[Paper]](https://arxiv.org/pdf/2203.10247.pdf)
- (arXiv 2022.03) RSTT: Real-time Spatial Temporal Transformer for Space-Time Video Super-Resolution, [[Paper]](https://arxiv.org/pdf/2203.14186.pdf), [[Code]](https://github.com/llmpass/RSTT)
- (arXiv 2022.04) Learning Trajectory-Aware Transformer for Video Super-Resolution, [[Paper]](https://arxiv.org/pdf/2204.04216.pdf), [[Code]](https://github.com/researchmm/TTVSR)
- (arXiv 2022.04) BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment, [[Paper]](https://arxiv.org/pdf/2204.08332.pdf), [[Code]](https://github.com/Algolzw/BSRT)
- (arXiv 2022.04) Self-Calibrated Efficient Transformer for Lightweight Super-Resolution, [[Paper]](https://arxiv.org/pdf/2204.08913.pdf), [[Code]](https://github.com/AlexZou14/SCET)
- (arXiv 2022.04) CTCNet: A CNN-Transformer Cooperation Network for Face Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2204.08696.pdf)
- (arXiv 2022.04) A New Dataset and Transformer for Stereoscopic Video Super-Resolution, [[Paper]](https://arxiv.org/pdf/2204.10039.pdf), [[Code]](https://github.com/H-deep/Trans-SVSR/)
- (arXiv 2022.04) Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer, [[Paper]](https://arxiv.org/pdf/2204.13286.pdf), [[Code]](https://github.com/IVIPLab/LBNet)
- (arXiv 2022.05) Activating More Pixels in Image Super-Resolution Transformer, [[Paper]](https://arxiv.org/pdf/2205.04437.pdf), [[Code]](https://github.com/chxy95/HAT)
- (arXiv 2022.06) Bilateral Network with Channel Splitting Network and Transformer for Thermal Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2206.12046.pdf)
- (arXiv 2022.07) Rethinking Alignment in Video Super-Resolution Transformers,  [[Paper]](https://arxiv.org/pdf/2207.08494.pdf), [[Code]](https://github.com/XPixelGroup/RethinkVSRAlignment)
- (arXiv 2022.07) Reference-based Image Super-Resolution with Deformable Attention Transformer,  [[Paper]](https://arxiv.org/pdf/2207.11938.pdf), [[Code]](https://github.com/caojiezhang/DATSR)
- (arXiv 2022.08) Learning Spatiotemporal Frequency-Transformer for Compressed Video Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2208.03012.pdf), [[Code]](https://github.com/researchmm/FTVSR)
- (arXiv 2022.08) HST: Hierarchical Swin Transformer for Compressed Image Super-resolution,  [[Paper]](https://arxiv.org/pdf/2208.09885.pdf)
- (arXiv 2022.09) Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration,  [[Paper]](https://arxiv.org/pdf/2209.11345.pdf)
- (arXiv 2022.10) Scene Text Image Super-Resolution via Content Perceptual Loss and Criss-Cross Transformer Blocks,  [[Paper]](https://arxiv.org/pdf/2210.06924.pdf)
- (arXiv 2022.10) Learning Texture Transformer Network for Light Field Super-Resolution,  [[Paper]](https://arxiv.org/ftp/arxiv/papers/2210/2210.09293.pdf)
- (arXiv 2022.10) ITSRN++: Stronger and Better Implicit Transformer Network for Continuous Screen Content Image Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2210.08812.pdf)
- (arXiv 2022.10) Single Image Super-Resolution Using Lightweight Networks Based on Swin Transformer,  [[Paper]](https://arxiv.org/pdf/2210.11019.pdf)
- (arXiv 2022.11) N-Gram in Swin Transformers for Efficient Lightweight Image Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2211.11436.pdf)
- (arXiv 2022.12) Learning Spatiotemporal Frequency-Transformer for Low-Quality Video Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2212.14046.pdf), [[Code]](https://github.com/researchmm/FTVSR)
- (arXiv 2023.01) Image Super-Resolution using Efficient Striped Window Transformer,  [[Paper]](https://arxiv.org/pdf/2301.09869.pdf), [[Code]](https://github.com/Fried-Rice-Lab/FriedRiceLab)
- (arXiv 2023.02) OSRT: Omnidirectional Image Super-Resolution with Distortion-aware Transformer,  [[Paper]](https://arxiv.org/pdf/2302.03453.pdf), [[Code]](https://github.com/Fanghua-Yu/OSRT)
- (arXiv 2023.03) CoT-MISR:Marrying Convolution and Transformer for Multi-Image Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2303.06548.pdf)
- (arXiv 2023.03) Recursive Generalization Transformer for Image Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2303.06373.pdf)
- (arXiv 2023.03) SRFormer: Permuted Self-Attention for Single Image Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2303.06373.pdf), [[Code]](https://github.com/HVision-NKU/SRFormer)
- (arXiv 2023.03) LSwinSR: UAV Imagery Super-Resolution based on Linear Swin Transformer,  [[Paper]](https://arxiv.org/pdf/2303.10232.pdf), [[Code]](https://github.com/lironui/LSwinSR)
- (arXiv 2023.03) PFT-SSR: Parallax Fusion Transformer for Stereo Image Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2303.13807.pdf), [[Code]](https://github.com/MIVRC/PFT-PyTorch)
- (arXiv 2023.03) Incorporating Transformer Designs into Convolutions for Lightweight Image Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2303.14324.pdf), [[Code]](https://github.com/Aitical/TCSR)
- (arXiv 2023.03) Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution,  [[Paper]](https://arxiv.org/pdf/2303.16513.pdf), [[Code]](https://github.com/jaroslaw1007/CLIT)
- (arXiv 2023.03) SOSR: Source-Free Image Super-Resolution with Wavelet Augmentation Transformer,  [[Paper]](https://arxiv.org/pdf/2303.17783.pdf)
- (arXiv 2023.05) Local-Global Transformer Enhanced Unfolding Network for Pan-sharpening, [[Paper]](https://arxiv.org/pdf/2304.14612.pdf)
- (arXiv 2023.05) A Vision Transformer Approach for Efficient Near-Field Irregular SAR Super-Resolution, [[Paper]](https://arxiv.org/pdf/2305.02074.pdf)
- (arXiv 2023.05) Hybrid Transformer and CNN Attention Network for Stereo Image Super-resolution, [[Paper]](https://arxiv.org/pdf/2305.05177.pdf)
- (arXiv 2023.05) Efficient Mixed Transformer for Single Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2305.11403.pdf), [[Code]](https://github.com/Fried-Rice-Lab/EMTT)
- (arXiv 2023.05) FIT: Far-reaching Interleaved Transformers, [[Paper]](https://arxiv.org/pdf/2305.12689.pdf), [[Code]](https://github.com/google-research/pix2seq)
- (arXiv 2023.07) MaxSR: Image Super-Resolution Using Improved MaxViT, [[Paper]](https://arxiv.org/pdf/2307.07240.pdf)
- (arXiv 2023.07) DARTS: Double Attention Reference-based Transformer for Super-resolution, [[Paper]](https://arxiv.org/pdf/2307.08837.pdf)
- (arXiv 2023.07) ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution, [[Paper]](https://arxiv.org/pdf/2307.14010.pdf), [[Code]](https://github.com/Rexzhan/ESSAformer/tree/main)
- (arXiv 2023.08) Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2308.05022.pdf), [[Code]](https://github.com/AVC2-UESTC/CRAFT-SR.git)
- (arXiv 2023.08) Dual Aggregation Transformer for Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2308.03364.pdf), [[Code]](https://github.com/zhengchen1999/DAT)
- (arXiv 2023.08) Unfolding Once is Enough: A Deployment-Friendly Transformer Unit for Super-Resolution, [[Paper]](https://arxiv.org/pdf/2308.02794.pdf), [[Code]](https://github.com/yongliuy/DITN)
- (arXiv 2023.08) S2R: Exploring a Double-Win Transformer-Based Framework for Ideal and Blind Super-Resolution, [[Paper]](https://arxiv.org/pdf/2308.08142.pdf), [[Code]](https://github.com/berumotto-vermouth/S2R.12)
- (arXiv 2023.10) Degradation-Aware Self-Attention Based Transformer for Blind Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2310.04180.pdf), [[Code]](https://github.com/I2-Multimedia-Lab/DSAT/tree/main)
- (arXiv 2023.12) SRTransGAN: Image Super-Resolution using Transformer based Generative Adversarial Network, [[Paper]](https://arxiv.org/pdf/2312.01999.pdf)
- (arXiv 2023.12) Transformer-based Selective Super-Resolution for Efficient Image Refinement, [[Paper]](https://arxiv.org/pdf/2312.05803.pdf), [[Code]](https://github.com/destiny301/SSR)
- (arXiv 2023.12) Learn From Orientation Prior for Radiograph Super-Resolution: Orientation Operator Transformer, [[Paper]](https://arxiv.org/pdf/2312.16455.pdf)
- (arXiv 2024.01) Image Super-resolution Reconstruction Network based on Enhanced Swin Transformer via Alternating Aggregation of Local-Global Features, [[Paper]](https://arxiv.org/pdf/2401.00241.pdf)
- (arXiv 2024.01) Beyond Subspace Isolation: Many-to-Many Transformer for Light Field Image Super-resolution, [[Paper]](https://arxiv.org/pdf/2401.00740.pdf)
- (arXiv 2024.01) Transforming Image Super-Resolution: A ConvFormer-based Efficient Approach, [[Paper]](https://arxiv.org/pdf/2401.05633.pdf),[[Code]](https://github.com/Aitical/CFSR)
- (arXiv 2024.01) Video Super-Resolution Transformer with Masked Inter&Intra-Frame Attention, [[Paper]](https://arxiv.org/pdf/2401.06312.pdf),[[Code]](https://github.com/LabShuHangGU/MIA-VSR)
- (arXiv 2024.01) Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary, [[Paper]](https://arxiv.org/pdf/2401.08209.pdf),[[Code]](https://github.com/LabShuHangGU/Adaptive-Token-Dictionary)
- (arXiv 2024.01) LKFormer: Large Kernel Transformer for Infrared Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2401.11589.pdf),[[Code]](https://github.com/sad192/large-kernel-Transformer)

### Synthesis
- (arXiv 2020.12) Taming Transformers for High-Resolution Image Synthesis, [[Paper]](https://arxiv.org/abs/2012.09841), [[Code]](https://compvis.github.io/taming-transformers/)
- (arXiv 2021.04) Geometry-Free View Synthesis: Transformers and no 3D Priors, [[Paper]](https://arxiv.org/pdf/2104.07652.pdf)
- (arXiv 2021.05) High-Resolution Complex Scene Synthesis with Transformers, [[Paper]](https://arxiv.org/pdf/2105.06458.pdf)
- (arXiv 2021.06) The Image Local Autoregressive Transformer, [[Paper]](https://arxiv.org/pdf/2106.02514.pdf)
- (arXiv 2021.10) ATISS: Autoregressive Transformers for Indoor Scene Synthesis, [[Paper]](https://arxiv.org/pdf/2110.03675.pdf), [[Project]](https://nv-tlabs.github.io/ATISS/)
- (arXiv 2021.11) Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers, [[Paper]](https://arxiv.org/pdf/2111.03481.pdf)
- (arXiv 2022.02) MaskGIT: Masked Generative Image Transformer, [[Paper]](https://arxiv.org/pdf/2202.04200.pdf)
- (arXiv 2022.07) Diverse Dance Synthesis via Keyframes with Transformer Controllers, [[Paper]](https://arxiv.org/pdf/2207.05906.pdf)
- (arXiv 2022.10) Style-Guided Inference of Transformer for High-resolution Image Synthesis, [[Paper]](https://arxiv.org/pdf/2210.05533.pdf)
- (arXiv 2022.10) FontTransformer: Few-shot High-resolution Chinese Glyph Image Synthesis via Stacked Transformers, [[Paper]](https://arxiv.org/pdf/2210.06301.pdf)
- (arXiv 2023.01) Geometry-biased Transformers for Novel View Synthesis, [[Paper]](https://arxiv.org/pdf/2301.04650.pdf), [[Project]](https://mayankgrwl97.github.io/gbt)
- (arXiv 2023.12) NViST: In the Wild New View Synthesis from a Single Image with Transformers, [[Paper]](https://arxiv.org/pdf/2312.08568.pdf), [[Project]](https://wbjang.github.io/nvist_webpage)

### Text-to-Image/Video
- (arXiv 2021.01) VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search, [[Paper]](https://arxiv.org/abs/2101.00265)
- (arXiv 2021.05) CogView: Mastering Text-to-Image Generation via Transformers, [[Paper]](https://arxiv.org/pdf/2105.13290.pdf)
- (arXiv 2022.02) DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers, [[Paper]](https://arxiv.org/pdf/2201.11316.pdf),[[Code]](https://github.com/j-min/DallEval)
- (arXiv 2022.05) CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers, [[Paper]](https://arxiv.org/pdf/2204.14217.pdf),[[Code]](https://github.com/THUDM/CogView2)
- (arXiv 2022.05) CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers, [[Paper]](https://arxiv.org/pdf/2205.15868.pdf),[[Code]](https://github.com/THUDM/CogVideo)
- (arXiv 2022.09) StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation, [[Paper]](https://arxiv.org/pdf/2209.06192.pdf),[[Code]](https://github.com/adymaharana/storydalle)
- (arXiv 2022.10) Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation, [[Paper]](https://arxiv.org/pdf/2210.09549.pdf)
- (arXiv 2022.12) Exploring Vision Transformers as Diffusion Learners, [[Paper]](https://arxiv.org/pdf/2212.13771.pdf)
- (arXiv 2023.01) Muse: Text-To-Image Generation via Masked Generative Transformers, [[Paper]](https://arxiv.org/pdf/2301.00704.pdf),[[Code]](http://muse-model.github.io/)
- (arXiv 2023.03) Lformer: Text-to-Image Generation with L-shape Block Parallel Decoding, [[Paper]](https://arxiv.org/pdf/2303.03800.pdf)
- (arXiv 2023.09) A Simple Text to Video Model via Transformer, [[Paper]](https://arxiv.org/pdf/2309.14683.pdf)
- (arXiv 2023.10) Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis, [[Paper]](https://arxiv.org/pdf/2310.00426.pdf)
- (arXiv 2023.11) VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning, [[Paper]](https://arxiv.org/pdf/2311.00990.pdf),[[Code]](https://videodreamer23.github.io/)
- (arXiv 2023.11) MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture, [[Paper]](https://arxiv.org/pdf/2311.10123.pdf),[[Code]](https://metadreamer3d.github.io/)
- (arXiv 2023.12) X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation, [[Paper]](https://arxiv.org/pdf/2312.00085.pdf),[[Project]](https://xmuxiaoma666.github.io/Projects/X-Dreamer)
- (arXiv 2023.12) GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation, [[Paper]](https://arxiv.org/pdf/2312.04557.pdf),[[Code]](https://www.shoufachen.com/gentron_website/)
- (arXiv 2024.02) Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis, [[Paper]](https://arxiv.org/pdf/2402.14797.pdf),[[Code]](https://snap-research.github.io/snapvideo/)
- (arXiv 2024.03) PixArt-危: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation, [[Paper]](https://arxiv.org/pdf/2403.04692.pdf),[[Code]](https://pixart-alpha.github.io/PixArt-sigma-project/)
- (arXiv 2024.03) BrightDreamer: Generic 3D Gaussian Generative Framework for Fast Text-to-3D Synthesis, [[Paper]](https://arxiv.org/pdf/2403.11273.pdf),[[Code]](https://vlislab22.github.io/BrightDreamer/)

### Texture
- (arXiv 2021.09) 3D Human Texture Estimation from a Single Image with Transformers, [[Paper]](https://arxiv.org/pdf/2109.02563.pdf), [[Code]](https://www.mmlab-ntu.com/project/texformer/)
- (arXiv 2022.02) Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer for Universal Texture Synthesis, [[Paper]](https://arxiv.org/pdf/2202.11703.pdf)
- (arXiv 2023.11) 3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual Transformer Learning, [[Paper]](https://arxiv.org/pdf/2311.10651.pdf),[[Code]](https://videodreamer23.github.io/)
- (arXiv 2024.02) Dynamic Texture Transfer using PatchMatch and Transformers, [[Paper]](https://arxiv.org/pdf/2402.00606.pdf)
- (arXiv 2024.03) 3DTextureTransformer: Geometry Aware Texture Generation for Arbitrary Mesh Topology, [[Paper]](https://arxiv.org/pdf/2403.04225.pdf)
- (arXiv 2024.03) TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation, [[Paper]](https://arxiv.org/pdf/2403.12906.pdf),[[Code]](https://ggxxii.github.io/texdreamer/)

### Time Series
- (arXiv 2023.03) Time Series as Images: Vision Transformer for Irregularly Sampled Time Series, [[Paper]](https://arxiv.org/pdf/2303.12799.pdf), [[Code]](https://github.com/Leezekun/ViTST)
- (arXiv 2023.05) Improving Position Encoding of Transformers for Multivariate Time Series Classification, [[Paper]](https://arxiv.org/pdf/2305.16642.pdf), [[Code]](https://github.com/Navidfoumani/ConvTran)
- (arXiv 2023.07) U-shaped Transformer: Retain High Frequency Context in Time Series Analysis, [[Paper]](https://arxiv.org/pdf/2307.09019.pdf)
- (arXiv 2024.02) Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images, [[Paper]](https://arxiv.org/pdf/2402.13475.pdf)
- (arXiv 2024.03) From Pixels to Predictions: Spectrogram and Vision Transformer for Better Time Series Forecasting, [[Paper]](https://arxiv.org/pdf/2403.11047.pdf)

### Tracking
- (EMNLP'19) Effective Use of Transformer Networks for Entity Tracking, [[Paper]](https://arxiv.org/pdf/1909.02635), [[Code]](https://github.com/aditya2211/transformer-entity-tracking)
- (CVPR'21) Transformer Tracking, [[Paper]](https://arxiv.org/pdf/2103.15436), [[Code]](https://github.com/chenxin-dlut/TransT)
- (CVPR'21) Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking, [[Paper]](https://arxiv.org/pdf/2103.11681.pdf), [[Code]](https://github.com/594422814/TransformerTrack)
- (arXiv 2020.12) TransTrack: Multiple-Object Tracking with Transformer, [[Paper]](https://arxiv.org/pdf/2012.15460), [[Code]](https://github.com/PeizeSun/TransTrack)
- (arXiv 2021.01) TrackFormer: Multi-Object Tracking with Transformers, [[Paper]](https://arxiv.org/pdf/2101.02702)
- (arXiv 2021.03) TransCenter: Transformers with Dense Queries for Multiple-Object Tracking, [[Paper]](https://arxiv.org/abs/2103.15145)
- (arXiv 2021.03) Learning Spatio-Temporal Transformer for Visual Tracking, [[Paper]](https://arxiv.org/abs/2103.17154), [[Code]](https://github.com/researchmm/Stark)
- (arXiv 2021.04) Multitarget Tracking with Transformers, [[Paper]](https://arxiv.org/pdf/2104.00734.pdf) 
- (arXiv 2021.04) Spatial-Temporal Graph Transformer for Multiple Object Tracking, [[Paper]](https://arxiv.org/abs/2104.00194)
- (arXiv 2021.05) MOTR: End-to-End Multiple-Object Tracking with TRansformer, [[Paper]](https://arxiv.org/pdf/2105.03247.pdf), [[Code]](https://github.com/megvii-model/MOTR)
- (arXiv 2021.05) TrTr: Visual Tracking with Transformer, [[Paper]](https://arxiv.org/pdf/2105.03817.pdf), [[Code]](https://github.com/tongtybj/TrTr)
- (arXiv 2021.08) HiFT: Hierarchical Feature Transformer for Aerial Tracking, [[Paper]](https://arxiv.org/pdf/2108.00202.pdf), [[Code]](https://github.com/vision4robotics/HiFT)
- (arXiv 2021.10) Siamese Transformer Pyramid Networks for Real-Time UAV Tracking, [[Paper]](https://arxiv.org/pdf/2110.08822.pdf), [[Code]](https://github.com/RISCNYUAD/SiamTPNTracker)
- (arXiv 2021.10) 3D Object Tracking with Transformer, [[Paper]](https://arxiv.org/pdf/2110.14921.pdf), [[Code]](https://github.com/3bobo/lttr)
- (arXiv 2021.12) SwinTrack: A Simple and Strong Baseline for Transformer Tracking, [[Paper]](https://arxiv.org/pdf/2112.00995.pdf), [[Code]](https://github.com/LitingLin/SwinTrack)
- (arXiv 2021.12) PTTR: Relational 3D Point Cloud Object Tracking with Transformer, [[Paper]](https://arxiv.org/pdf/2112.02857.pdf), [[Code]](https://github.com/Jasonkks/PTTR)
- (arXiv 2021.12) Learning Tracking Representations via Dual-Branch Fully Transformer Networks, [[Paper]](https://arxiv.org/pdf/2112.02571.pdf), [[Code]](https://github.com/phiphiphi31/DualTFR)
- (arXiv 2021.12) Efficient Visual Tracking with Exemplar Transformers, [[Paper]](https://arxiv.org/pdf/2112.09686.pdf), [[Code]](https://github.com/visionml/pytracking)
- (arXiv 2022.03) Transforming Model Prediction for Tracking, [[Paper]](https://arxiv.org/pdf/2203.11192.pdf), [[Code]](https://github.com/visionml/pytracking)
- (arXiv 2022.03) MixFormer: End-to-End Tracking with Iterative Mixed Attention, [[Paper]](https://arxiv.org/pdf/2203.11082.pdf), [[Code]](https://github.com/MCG-NJU/MixFormer)
- (arXiv 2022.03) Global Tracking Transformers, [[Paper]](https://arxiv.org/pdf/2203.13250.pdf), [[Code]](https://github.com/xingyizhou/GTR)
- (arXiv 2022.03) Keypoints Tracking via Transformer Networks, [[Paper]](https://arxiv.org/pdf/2203.12848.pdf), [[Code]](https://github.com/LexaNagiBator228/Keypoints-Tracking-via-Transformer-Networks/)
- (arXiv 2022.03) High-Performance Transformer Tracking, [[Paper]](https://arxiv.org/pdf/2203.13533.pdf), [[Code]](https://github.com/chenxin-dlut/TransT-M)
- (arXiv 2022.03) Efficient Visual Tracking via Hierarchical Cross-Attention Transformer, [[Paper]](https://arxiv.org/pdf/2203.13537.pdf), [[Code]](https://github.com/chenxin-dlut/HCAT)
- (arXiv 2022.03) Unified Transformer Tracker for Object Tracking, [[Paper]](https://arxiv.org/pdf/2203.15175.pdf), [[Code]](https://github.com/Flowerfan/Trackron)
- (arXiv 2022.03) Global Tracking via Ensemble of Local Trackers, [[Paper]](https://arxiv.org/pdf/2203.16092.pdf)
- (arXiv 2022.03) MeMOT: Multi-Object Tracking with Memory, [[Paper]](https://arxiv.org/pdf/2203.16761.pdf)
- (arXiv 2022.05) SparseTT: Visual Tracking with Sparse Transformers, [[Paper]](https://arxiv.org/pdf/2205.03776.pdf), [[Code]](https://github.com/fzh0917/SparseTT)
- (arXiv 2022.05) Transformer Tracking with Cyclic Shifting Window Attention, [[Paper]](https://arxiv.org/pdf/2205.03806.pdf), [[Code]](https://github.com/SkyeSong38/CSWinTT)
- (arXiv 2022.05) Transformers for Multi-Object Tracking on Point Clouds, [[Paper]](https://arxiv.org/pdf/2205.15730.pdf)
- (arXiv 2022.05) Joint Spatial-Temporal and Appearance Modeling with Transformer for Multiple Object Tracking, [[Paper]](https://arxiv.org/pdf/2205.15495.pdf), [[Code]](https://github.com/icicle4/TranSTAM)
- (arXiv 2022.07) AiATrack: Attention in Attention for Transformer Visual Tracking, [[Paper]](https://arxiv.org/pdf/2207.09603.pdf), [[Code]](https://github.com/Little-Podi/AiATrack)
- (arXiv 2022.07) 3D Siamese Transformer Network for Single Object Tracking on Point Clouds, [[Paper]](https://arxiv.org/pdf/2207.11995.pdf), [[Code]](https://github.com/fpthink/STNet)
- (arXiv 2022.08) Local Perception-Aware Transformer for Aerial Tracking, [[Paper]](https://arxiv.org/pdf/2208.00662.pdf), [[Code]](https://github.com/vision4robotics/LPAT)
- (arXiv 2022.08) Transformer-based assignment decision network for multiple object tracking, [[Paper]](https://arxiv.org/pdf/2208.03571.pdf), [[Code]](https://github.com/psaltaath/tadn-mot)
- (arXiv 2022.08) InterTrack: Interaction Transformer for 3D Multi-Object Tracking, [[Paper]](https://arxiv.org/pdf/2208.08041.pdf)
- (arXiv 2022.08) Learning Spatial-Frequency Transformer for Visual Object Tracking, [[Paper]](https://arxiv.org/pdf/2208.08829.pdf), [[Code]](https://github.com/Tchuanm/SFTransT.git)
- (arXiv 2022.09) Real-time 3D Single Object Tracking with Transformer, [[Paper]](https://arxiv.org/pdf/2209.00860.pdf), [[Code]](https://github.com/shanjiayao/PTT)
- (arXiv 2022.10) Strong-TransCenter: Improved Multi-Object Tracking based on Transformers with Dense Representations, [[Paper]](https://arxiv.org/pdf/2210.13570.pdf), [[Code]](https://github.com/amitgalor18/STC_Tracker)
- (arXiv 2022.10) End-to-end Tracking with a Multi-query Transformer, [[Paper]](https://arxiv.org/pdf/2210.14601.pdf)
- (arXiv 2022.10) Can Transformer Attention Spread Give Insights Into Uncertainty of Detected and Tracked Objects, [[Paper]](https://arxiv.org/pdf/2210.14391.pdf)
- (arXiv 2022.10) ProContEXT: Exploring Progressive Context Transformer for Tracking, [[Paper]](https://arxiv.org/pdf/2210.15511.pdf), [[Code]](https://shorturl.at/jnNT2)
- (arXiv 2022.11) GLT-T: Global-Local Transformer Voting for 3D Single Object Tracking in Point Clouds, [[Paper]](https://arxiv.org/pdf/2211.10927.pdf), [[Code]](https://github.com/haooozi/GLT-T)
- (arXiv 2023.01) Multi-target multi-camera vehicle tracking using transformer-based camera link model and spatial-temporal information, [[Paper]](https://arxiv.org/pdf/2301.07805.pdf)
- (arXiv 2023.01) Compact Transformer Tracker with Correlative Masked Modeling, [[Paper]](https://arxiv.org/pdf/2301.10938.pdf), [[Project]](https://github.com/HUSTDML/CTTrack)
- (arXiv 2023.01) MixFormer: End-to-End Tracking withIterative Mixed Attention, [[Paper]](https://arxiv.org/pdf/2302.02814.pdf), [[Code]](https://github.com/MCG-NJU/MixFormer)
- (arXiv 2023.02) Transformers in Single Object Tracking: An Experimental Survey, [[Paper]](https://arxiv.org/pdf/2302.11867.pdf)
- (arXiv 2023.03) SGDViT: Saliency-Guided Dynamic Vision Transformer for UAV Tracking, [[Paper]](https://arxiv.org/pdf/2303.04378.pdf), [[Code]](https://github.com/vision4robotics/SGDViT)
- (arXiv 2023.03) Event-based Human Pose Tracking by Spiking Spatiotemporal Transformer, [[Paper]](https://arxiv.org/pdf/2303.09681.pdf), [[Code]](https://github.com/JimmyZou/HumanPoseTracking_SNN)
- (arXiv 2023.03) Tracker Meets Night: A Transformer Enhancer for UAV Tracking, [[Paper]](https://arxiv.org/pdf/2303.10951.pdf), [[Code]](https://github.com/vision4robotics/SCT)
- (arXiv 2023.03) RGBT Tracking via Progressive Fusion Transformer with Dynamically Guided Learning, [[Paper]](https://arxiv.org/pdf/2303.14778.pdf)
- (arXiv 2023.03) Generalized Relation Modeling for Transformer Tracking, [[Paper]](https://arxiv.org/pdf/2303.16580.pdf), [[Code]](https://github.com/Little-Podi/GRM)
- (arXiv 2023.04) GLT-T++: Global-Local Transformer for 3D Siamese Tracking with Ranking Loss, [[Paper]](https://arxiv.org/pdf/2304.00242.pdf), [[Code]](https://github.com/haooozi/GLT-T)
- (arXiv 2023.05) MixFormerV2: Efficient Fully Transformer Tracking, [[Paper]](https://arxiv.org/pdf/2305.15896.pdf), [[Code]](https://github.com/MCG-NJU/MixFormerV2)
- (arXiv 2023.05) Humans in 4D: Reconstructing and Tracking Humans with Transformers, [[Paper]](https://arxiv.org/pdf/2305.20091.pdf), [[Code]](https://shubham-goel.github.io/4dhumans/)
- (arXiv 2023.06) TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses, [[Paper]](https://arxiv.org/pdf/2306.05888.pdf)
- (arXiv 2023.06) MotionTrack: End-to-End Transformer-based Multi-Object Tracing with LiDAR-Camera Fusion, [[Paper]](https://arxiv.org/pdf/2306.17000.pdf)
- (arXiv 2023.07) Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers, [[Paper]](https://arxiv.org/pdf/2307.04129.pdf)
- (arXiv 2023.07) CoTracker: It is Better to Track Together, [[Paper]](https://arxiv.org/pdf/2307.07635.pdf), [[Code]](https://co-tracker.github.io/)
- (arXiv 2023.07) ConTrack: Contextual Transformer for Device Tracking in X-ray, [[Paper]](https://arxiv.org/pdf/2307.07541.pdf)
- (arXiv 2023.07) MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking, [[Paper]](https://arxiv.org/pdf/2307.15700.pdf)
- (arXiv 2023.08) Robust Object Modeling for Visual Tracking, [[Paper]](https://arxiv.org/pdf/2308.05140.pdf), [[Code]](https://github.com/dawnyc/ROMTrack)
- (arXiv 2023.08) Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking, [[Paper]](https://arxiv.org/pdf/2308.06904.pdf), [[Code]](https://github.com/kangben258/HiT)
- (arXiv 2023.08) 3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking, [[Paper]](https://arxiv.org/pdf/2308.06635.pdf), [[Code]](https://github.com/dsx0511/3DMOTFormer)
- (arXiv 2023.08) BOTT: Box Only Transformer Tracker for 3D Object Tracking, [[Paper]](https://arxiv.org/pdf/2308.08753.pdf)
- (arXiv 2023.08) Delving into Motion-Aware Matching for Monocular 3D Object Tracking, [[Paper]](https://arxiv.org/pdf/2308.11607.pdf), [[Code]](https://github.com/kuanchihhuang/MoMA-M3T)
- (arXiv 2023.08) CiteTracker: Correlating Image and Text for Visual Tracking, [[Paper]](https://arxiv.org/pdf/2308.11322.pdf), [[Code]](https://github.com/NorahGreen/CiteTracker)
- (arXiv 2023.08) Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.13969.pdf), [[Code]](https://github.com/schko/fixatt)
- (arXiv 2023.08) Unified Single-Stage Transformer Network for Efficient RGB-T Tracking, [[Paper]](https://arxiv.org/pdf/2308.13764.pdf)
- (arXiv 2023.08) Group Regression for Query Based Object Detection and Tracking, [[Paper]](https://arxiv.org/pdf/2308.14481.pdf)
- (arXiv 2023.08) RGB-T Tracking via Multi-Modal Mutual Prompt Learning, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2308/2308.16386.pdf), [[Code]](https://github.com/HusterYoung/MPLT)
- (arXiv 2023.09) Efficient Training for Visual Tracking with Deformable Transformer, [[Paper]](https://arxiv.org/pdf/2309.02676.pdf), [[Code]](https://github.com/HusterYoung/MPLT)
- (arXiv 2023.09) Separable Self and Mixed Attention Transformers for Efficient Object Tracking, [[Paper]](https://arxiv.org/pdf/2309.03979.pdf), [[Code]](https://github.com/goutamyg/SMAT)
- (arXiv 2023.09) Transparent Object Tracking with Enhanced Fusion Module, [[Paper]](https://arxiv.org/pdf/2309.06701.pdf), [[Code]](https://github.com/kalyan0510/TOTEM)
- (arXiv 2023.09) Leveraging the Power of Data Augmentation for Transformer-based Tracking, [[Paper]](https://arxiv.org/pdf/2309.08264.pdf)
- (arXiv 2023.09) LiteTrack: Layer Pruning with Asynchronous Feature Extraction for Lightweight and Efficient Visual Tracking, [[Paper]](https://arxiv.org/pdf/2309.09249.pdf), [[Code]](https://github.com/TsingWei/LiteTrack)
- (arXiv 2023.10) Lightweight Full-Convolutional Siamese Tracker, [[Paper]](https://arxiv.org/pdf/2310.05392.pdf), [[Code]](https://github.com/LiYunfengLYF/LightFC)
- (arXiv 2023.10) MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and MOTR, [[Paper]](https://arxiv.org/pdf/2310.17170.pdf)
- (arXiv 2023.10) Siamese-DETR for Generic Multi-Object Tracking, [[Paper]](https://arxiv.org/pdf/2310.17875.pdf)
- (arXiv 2023.11) LabelFormer: Object Trajectory Refinement for Offboard Perception from LiDAR Point Clouds, [[Paper]](https://arxiv.org/pdf/2311.01444.pdf)
- (arXiv 2023.11) Contrastive Learning for Multi-Object Tracking with Transformers, [[Paper]](https://arxiv.org/pdf/2311.08043.pdf)
- (arXiv 2023.11) Single-Model and Any-Modality for Video Object Tracking, [[Paper]](https://arxiv.org/pdf/2311.15851.pdf), [[Code]](https://github.com/Zongwei97/UnTrack)
- (arXiv 2023.12) Multi-Correlation Siamese Transformer Network with Dense Connection for 3D Single Object Tracking, [[Paper]](https://arxiv.org/pdf/2312.11051.pdf), [[Code]](https://github.com/liangp/MCSTN-3DSOT)
- (arXiv 2023.12) Transformer Network for Multi-Person Tracking and Re-Identification in Unconstrained Environment, [[Paper]](https://arxiv.org/pdf/2312.11929.pdf)
- (arXiv 2024.01) Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens, [[Paper]](https://arxiv.org/pdf/2401.01674.pdf)
- (arXiv 2024.01) Correlation-Embedded Transformer Tracking: A Single-Branch Framework, [[Paper]](https://arxiv.org/pdf/2401.12743.pdf), [[Code]](https://github.com/phiphiphi31/SBT)
- (arXiv 2024.02) Optimized Information Flow for Transformer Tracking, [[Paper]](https://arxiv.org/pdf/2402.08195.pdf), [[Code]](https://github.com/JananiKugaa/OIFTrack)
- (arXiv 2024.02) OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog, [[Paper]](https://arxiv.org/pdf/2402.13146.pdf)
- (arXiv 2024.03) Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance, [[Paper]](https://arxiv.org/pdf/2403.05231.pdf)
- (arXiv 2024.03) Autoregressive Queries for Adaptive Tracking with Spatio-TemporalTransformers, [[Paper]](https://arxiv.org/pdf/2403.10574.pdf), [[Code]](https://github.com/orgs/GXNU-ZhongLab)
- (arXiv 2024.03) TAPTR: Tracking Any Point with Transformers as Detection, [[Paper]](https://arxiv.org/pdf/2403.13042.pdf), [[Code]](https://github.com/IDEA-Research/TAPTR)
- (arXiv 2024.03) ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras Based on Transformer, [[Paper]](https://arxiv.org/pdf/2403.14626.pdf)

### Traffic
- (arXiv 2021.05) Novelty Detection and Analysis of Traffic Scenario Infrastructures in the Latent Space of a Vision Transformer-Based Triplet Autoencoder, [[Paper]](https://arxiv.org/pdf/2105.01924.pdf)
- (arXiv 2021.11) DetectorNet: Transformer-enhanced Spatial Temporal Graph Neural Network for Traffic Prediction, [[Paper]](https://arxiv.org/pdf/2111.00869.pdf)
- (arXiv 2021.11) ProSTformer: Pre-trained Progressive Space-Time Self-attention Model for Traffic Flow Forecasting, [[Paper]](https://arxiv.org/pdf/2111.03459.pdf)
- (arXiv 2022.01) SwinUNet3D -- A Hierarchical Architecture for Deep Traffic Prediction using Shifted Window Transformers, [[Paper]](https://arxiv.org/pdf/2201.06390.pdf), [[Code]](https://github.com/bojesomo/Traffic4Cast2021-SwinUNet3D)
- (arXiv 2022.02) TransFollower: Long-Sequence Car-Following Trajectory Prediction through Transformer, [[Paper]](https://arxiv.org/pdf/2202.03183.pdf)
- (arXiv 2022.03) LatentFormer: Multi-Agent Transformer-Based Interaction Modeling and Trajectory Prediction, [[Paper]](https://arxiv.org/pdf/2203.01880.pdf)
- (arXiv 2022.03) Under the Hood of Transformer Networks for Trajectory Forecasting, [[Paper]](https://arxiv.org/pdf/2203.11878.pdf)
- (arXiv 2022.05) HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding, [[Paper]](https://arxiv.org/pdf/2205.09753.pdf)
- (arXiv 2022.06) S2TNet: Spatio-Temporal Transformer Networks for Trajectory Prediction in Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2206.10902.pdf), [[Code]](https://github.com/chenghuang66/s2tnet)
- (arXiv 2022.07) Pyramid Transformer for Traffic Sign Detection, [[Paper]](https://arxiv.org/pdf/2207.06067.pdf), [[Code]](https://github.com/chenghuang66/s2tnet)
- (arXiv 2022.07) Applying Spatiotemporal Attention to Identify Distracted and Drowsy Driving with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.12148.pdf)
- (arXiv 2022.07) Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer, [[Paper]](https://arxiv.org/pdf/2207.14024.pdf)
- (arXiv 2022.09) Traffic Accident Risk Forecasting using Contextual Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.11180.pdf)
- (arXiv 2022.10) PlanT: Explainable Planning Transformers via Object-Level Representations, [[Paper]](https://arxiv.org/pdf/2210.14222.pdf), [[Code]](https://www.katrinrenz.de/plant)
- (arXiv 2023.05) CEMFormer: Learning to Predict Driver Intentions from In-Cabin and External Cameras via Spatial-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2305.07840.pdf)
- (arXiv 2023.06) TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem via Transformer-Based Architecture, [[Paper]](https://arxiv.org/pdf/2306.05419.pdf)
- (arXiv 2023.06) Towards predicting Pedestrian Evacuation Time and Density from Floorplans using a Vision Transformer, [[Paper]](https://arxiv.org/pdf/2306.15318.pdf)
- (arXiv 2023.07) Separated RoadTopoFormer, [[Paper]](https://arxiv.org/pdf/2307.01557.pdf)
- (arXiv 2023.07) Context-aware Pedestrian Trajectory Prediction with Multimodal Transformer, [[Paper]](https://arxiv.org/pdf/2307.03786.pdf)
- (arXiv 2023.07) LightFormer: An End-to-End Model for Intersection Right-of-Way Recognition Using Traffic Light Signals and an Attention Mechanism, [[Paper]](https://arxiv.org/pdf/2307.07196.pdf)
- (arXiv 2023.07) Light-Weight Vision Transformer with Parallel Local and Global Self-Attention, [[Paper]](https://arxiv.org/pdf/2307.09120.pdf)
- (arXiv 2023.07) OCTraN: 3D Occupancy Convolutional Transformer Network in Unstructured Traffic Scenarios, [[Paper]](https://arxiv.org/pdf/2307.10934.pdf)
- (arXiv 2023.07) ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation, [[Paper]](https://arxiv.org/pdf/2307.14187.pdf), [[Code]](https://kuis-ai.github.io/adapt)
- (arXiv 2023.08) SST: A Simplified Swin Transformer-based Model for Taxi Destination Prediction based on Existing Trajectory, [[Paper]](https://arxiv.org/pdf/2308.07555.pdf)
- (arXiv 2023.08) MacFormer: Map-Agent Coupled Transformer for Real-time and Robust Trajectory Prediction, [[Paper]](https://arxiv.org/pdf/2308.10280.pdf)
- (arXiv 2023.09) Mobile Vision Transformer-based Visual Object Tracking, [[Paper]](https://arxiv.org/pdf/2309.05829.pdf), [[Code]](https://github.com/goutamyg/MVT)
- (arXiv 2023.09) DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2309.09777.pdf), [[Project]](https://drivedreamer.github.io/)
- (arXiv 2023.09) HPL-ViT: A Unified Perception Framework for Heterogeneous Parallel LiDARs in V2V, [[Paper]](https://arxiv.org/pdf/2309.15572.pdf)
- (arXiv 2023.11) Efficient Vision Transformer for Accurate Traffic Sign Detection, [[Paper]](https://arxiv.org/pdf/2311.01429.pdf)
- (arXiv 2023.11) DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding, [[Paper]](https://arxiv.org/pdf/2311.06497.pdf)
- (arXiv 2023.11) VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems, [[Paper]](https://arxiv.org/pdf/2311.06623.pdf)
- (arXiv 2023.11) Traffic Sign Recognition Using Local Vision Transformer, [[Paper]](https://arxiv.org/pdf/2311.06651.pdf)
- (arXiv 2023.11) Categorical Traffic Transformer: Interpretable and Diverse Behavior Prediction with Tokenized Latent, [[Paper]](https://arxiv.org/pdf/2311.18307.pdf)
- (arXiv 2024.01) Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction, [[Paper]](https://arxiv.org/pdf/2401.04872.pdf)
- (arXiv 2024.01) Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning, [[Paper]](https://arxiv.org/pdf/2401.06344.pdf)

### Transfer learning
- (arXiv 2021.06) Transformer-Based Source-Free Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2105.14138.pdf), [[Code]](https://github.com/ygjwd12345/TransDA)
- (arXiv 2021.08) TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2108.05988.pdf)
- (arXiv 2021.09) CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2109.06165.pdf)
- (arXiv 2021.10) Investigating Transfer Learning Capabilities of Vision Transformers and CNNs by Fine-Tuning a Single Trainable Block, [[Paper]](https://arxiv.org/pdf/2110.05270.pdf)
- (arXiv 2021.10) Dispensed Transformer Network for Unsupervised Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2110.14944.pdf)
- (arXiv 2021.11) Exploiting Both Domain-specific and Invariant Knowledge via a Win-win Transformer for Unsupervised Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2111.12941.pdf)
- (arXiv 2021.12) Pre-Training Transformers for Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2112.09965.pdf)
- (arXiv 2022.01) Domain Adaptation via Bidirectional Cross-Attention Transformer, [[Paper]](https://arxiv.org/pdf/2201.05887.pdf)
- (arXiv 2022.03) Towards Unsupervised Domain Adaptation via Domain-Transformer, [[Paper]](https://arxiv.org/pdf/2202.13777.pdf)
- (arXiv 2022.03) DATR: Domain-adaptive transformer for multi-domain landmark detection, [[Paper]](https://arxiv.org/pdf/2203.06433.pdf)
- (arXiv 2022.03) simCrossTrans: A Simple Cross-Modality Transfer Learning for Object Detection with ConvNets or Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.10456.pdf), [[Code]](https://github.com/liketheflower/simCrossTrans)
- (arXiv 2022.04) Safe Self-Refinement for Transformer-based Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2204.07683.pdf), [[Code]](https://github.com/tsun/SSRT)
- (arXiv 2022.05) Improving Transferability for Domain Adaptive Detection Transformers, [[Paper]](https://arxiv.org/pdf/2204.14195.pdf), [[Code]](https://github.com/tsun/SSRT)
- (arXiv 2022.05) Cross-Domain Object Detection with Mean-Teacher Transformer, [[Paper]](https://arxiv.org/pdf/2205.01643.pdf)
- (arXiv 2022.06) Cross-domain Detection Transformer based on Spatial-aware and Semantic-aware Token Alignment, [[Paper]](https://arxiv.org/pdf/2206.00222.pdf)
- (arXiv 2022.08) Making the Best of Both Worlds: A Domain-Oriented Transformer for Unsupervised Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2208.01195.pdf), [[Code]](https://github.com/BIT-DA/Domain-Oriented-Transformer)
- (arXiv 2022.08) How Well Do Vision Transformers (VTs) Transfer To The Non-Natural Image Domain? An Empirical Study Involving Art Classification, [[Paper]](https://arxiv.org/pdf/2208.04693.pdf)
- (arXiv 2022.08) Prompt Vision Transformer for Domain Generalization, [[Paper]](https://arxiv.org/pdf/2208.08914.pdf)
- (arXiv 2022.08) Transfer Learning and Vision Transformer based State-of-Health prediction of Lithium-Ion Batteries, [[Paper]](https://arxiv.org/pdf/2209.05253.pdf)
- (arXiv 2022.11) QuadFormer: Quadruple Transformer for Unsupervised Domain Adaptation in Power Line Segmentation of Aerial Images, [[Paper]](https://arxiv.org/pdf/2211.16988.pdf)
- (arXiv 2023.03) Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective, [[Paper]](https://arxiv.org/pdf/2303.13434.pdf)
- (arXiv 2023.05) Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers, [[Paper]](https://arxiv.org/pdf/2305.10018.pdf)
- (arXiv 2023.07) Domain Generalisation with Bidirectional Encoder Representations from Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.08117.pdf)
- (arXiv 2023.08) Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation, [[Paper]](https://arxiv.org/pdf/2308.14023.pdf), [[Code]](http://val.cds.iisc.ac.in/DSiT-SFDA/)
- (arXiv 2023.08) Transfer Learning for Microstructure Segmentation with CS-UNet: A Hybrid Algorithm with Transformer and CNN Encoders, [[Paper]](https://arxiv.org/pdf/2308.13917.pdf)
- (arXiv 2023.09) Domain Adaptation for Efficiently Fine-tuning Vision Transformer with Encrypted Images, [[Paper]](https://arxiv.org/pdf/2309.02556.pdf)
- (arXiv 2023.09) Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning, [[Paper]](https://arxiv.org/pdf/2309.07911.pdf), [[Code]](https://github.com/alibaba-mmai-research/DiST)
- (arXiv 2023.10) ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to Video, [[Paper]](https://arxiv.org/pdf/2310.01324.pdf)
- (arXiv 2023.10) Mean Teacher DETR with Masked Feature Alignment: A Robust Domain Adaptive Detection Transformer Framework, [[Paper]](https://arxiv.org/pdf/2310.15646.pdf)
- (arXiv 2023.11) Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images, [[Paper]](https://arxiv.org/pdf/2311.12589.pdf)
- (arXiv 2024.01) Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.05126.pdf)
- (arXiv 2024.01) BlenDA: Domain Adaptive Object Detection through diffusion-based blending, [[Paper]](https://arxiv.org/pdf/2401.09921.pdf), [[Code]](https://github.com/aiiu-lab/BlenDA)

### Translation
- (arXiv 2021.10) Tensor-to-Image: Image-to-Image Translation with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2110.08037.pdf)
- (arXiv 2022.03) UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation, [[Paper]](https://arxiv.org/pdf/2203.02557.pdf), [[Code]](https://github.com/LS4GAN/uvcga)
- (arXiv 2022.03) InstaFormer: Instance-Aware Image-to-Image Translation with Transformer, [[Paper]](https://arxiv.org/pdf/2203.16248.pdf)
- (arXiv 2022.03) ITTR: Unpaired Image-to-Image Translation with Transformers, [[Paper]](https://arxiv.org/pdf/2203.16015.pdf)
- (arXiv 2023.03) Masked and Adaptive Transformer for Exemplar Based Image Translation, [[Paper]](https://arxiv.org/pdf/2303.17123.pdf)

### Unsupervised learning
- (arXiv 2022.02) Handcrafted Histological Transformer (H2T): Unsupervised Representation of Whole Slide Images, [[Paper]](https://arxiv.org/pdf/2202.07001.pdf)
- (arXiv 2022.07) SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery, [[Paper]](https://arxiv.org/pdf/2207.08051.pdf)
- (arXiv 2023.12) SPOT: Self-Training with Patch-Order Permutation for Object-Centric Learning with Autoregressive Transformers, [[Paper]](https://arxiv.org/pdf/2312.00648.pdf), [[Code]](https://github.com/gkakogeorgiou/spot)

### UAV
- (arXiv 2023.10) SoybeanNet: Transformer-Based Convolutional Neural Network for Soybean Pod Counting from Unmanned Aerial Vehicle (UAV) Images, [[Paper]](https://arxiv.org/pdf/2310.10861.pdf), [[Code]](https://github.com/JiajiaLi04/Soybean-Pod-Counting-from-UAV-Images)
- (arXiv 2023.11) Rotation Invariant Transformer for Recognizing Object in UAVs, [[Paper]](https://arxiv.org/pdf/2311.02559.pdf), [[Code]](https://github.com/whucsy/RotTrans)
- (arXiv 2023.11) SugarViT -- Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet, [[Paper]](https://arxiv.org/pdf/2311.03076.pdf), [[Code]](https://github.com/whucsy/RotTrans)
- (arXiv 2024.01) A Transformer-Based Adaptive Semantic Aggregation Method for UAV Visual Geo-Localization, [[Paper]](https://arxiv.org/pdf/2401.01574.pdf)

### Video
- (ECCV'20) Multi-modal Transformer for Video Retrieval, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490205.pdf)
- (ICLR'21) Support-set bottlenecks for video-text representation learning, [[Paper]](https://arxiv.org/pdf/2010.02824.pdf)
- (arXiv 2021.01) SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2101.08833.pdf)
- (arXiv 2021.02) Video Transformer Network, [[Paper]](https://arxiv.org/pdf/2102.00719.pdf)
- (arXiv 2021.02) Is Space-Time Attention All You Need for Video Understanding? [[Paper]](https://arxiv.org/pdf/2102.05095.pdf), [[Code]](https://github.com/lucidrains/TimeSformer-pytorch)
- (arXiv.2021.02) A Straightforward Framework For Video Retrieval Using CLIP, [[Paper]](https://arxiv.org/pdf/2102.12443.pdf), [[Code]](https://github.com/Deferf/CLIP_Video_Representation)
- (arXiv 2021.03) Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning, [[Paper]](https://arxiv.org/pdf/2103.10211.pdf)
- (arXiv 2021.03) Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training, [[Paper]](https://arxiv.org/pdf/2103.10043.pdf)
- (arXiv 2021.03) MDMMT: Multidomain Multimodal Transformer for Video Retrieval, [[Paper]](https://arxiv.org/pdf/2103.10699.pdf)
- (arXiv 2021.03) An Image is Worth 16x16 Words, What is a Video Worth? [[Paper]](https://arxiv.org/pdf/2103.13915.pdf)
- (arXiv 2021.03) ViViT: A Video Vision Transformer, [[paper]](https://arxiv.org/abs/2103.15691)
- (arXiv 2021.04) Composable Augmentation Encoding for Video Representation Learning, [[Paper]](https://arxiv.org/pdf/2104.00616.pdf)
- (arXiv 2021.04) Temporal Query Networks for Fine-grained Video Understanding, [[Paper]](https://arxiv.org/pdf/2104.09496.pdf), [[Project]](https://www.robots.ox.ac.uk/~vgg/research/tqn/)
- (arXiv 2021.04) Higher Order Recurrent Space-Time Transformer, [[Paper]](https://arxiv.org/pdf/2104.08665.pdf), [[Code]](https://github.com/CorcovadoMing/HORST)
- (arXiv 2021.04) VideoGPT: Video Generation using VQ-VAE and Transformers, [[Paper]](https://arxiv.org/pdf/2104.10157.pdf), [[Code]](https://wilson1yan.github.io/videogpt/index.html)
- (arXiv 2021.04) VidTr: Video Transformer Without Convolutions, [[Paper]](https://arxiv.org/pdf/2104.11746.pdf)
- (arXiv 2021.05) Local Frequency Domain Transformer Networks for Video Prediction, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2105/2105.04637.pdf)
- (arXiv 2021.05) End-to-End Video Object Detection with Spatial-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2105.10920.pdf), [[Code]](https://github.com/SJTU-LuHe/TransVOD)
- (arXiv 2021.06) Anticipative Video Transformer, [[Paper]](https://arxiv.org/pdf/2106.02036.pdf), [[Project]](https://facebookresearch.github.io/AVT/)
- (arXiv 2021.06) TransVOS: Video Object Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2106.00588.pdf)
- (arXiv 2021.06) Associating Objects with Transformers for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2106.02638.pdf)
- (arXiv 2021.06) Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers, [[Paper]](https://arxiv.org/pdf/2106.05392.pdf)
- (arXiv 2021.06) Space-time Mixing Attention for Video Transformer, [[Paper]](https://arxiv.org/pdf/2106.05968.pdf)
- (arXiv 2021.06) Video Instance Segmentation using Inter-Frame Communication Transformers, [[Paper]](https://arxiv.org/pdf/2106.03299.pdf)
- (arXiv 2021.06) Long-Short Temporal Contrastive Learning of Video Transformers, [[Paper]](https://arxiv.org/pdf/2106.09212.pdf)
- (arXiv 2021.06) Video Swin Transformer, [[Paper]](https://arxiv.org/pdf/2106.13230.pdf), [[Code]](https://github.com/SwinTransformer/Video-Swin-Transformer)
- (arXiv 2021.06) Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection, [[Paper]](https://arxiv.org/pdf/2106.14447.pdf)
- (arXiv 2021.07) Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation, [[Paper]](https://arxiv.org/pdf/2107.00977.pdf), [[Code]](https://github.com/HReynaud/UVT)
- (arXiv 2021.07) Generative Video Transformer: Can Objects be the Words, [[Paper]](https://arxiv.org/pdf/2107.09240.pdf)
- (arXiv 2021.07) Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2107.13720.pdf)
- (arXiv 2021.08) Token Shift Transformer for Video Classification, [[Paper]](https://arxiv.org/pdf/2108.02432.pdf), [[Code]](https://github.com/VideoNetworks/TokShift-Transformer)
- (arXiv 2021.08) Mounting Video Metadata on Transformer-based Language Model for Open-ended Video Question Answering, [[Paper]](https://arxiv.org/pdf/2108.05158.pdf)
- (arXiv 2021.08) Video Relation Detection via Tracklet based Visual Transformer, [[Paper]](https://arxiv.org/pdf/2108.08669.pdf), [[Code]](https://github.com/Dawn-LX/VidVRD-tracklets)
- (arXiv 2021.08) MM-ViT: Multi-Modal Video Transformer for Compressed Video Action Recognition, [[Paper]](https://arxiv.org/pdf/2108.09322.pdf)
- (arXiv 2021.08) ZS-SLR: Zero-Shot Sign Language Recognition from RGB-D Videos, [[Paper]](https://arxiv.org/pdf/2108.10059.pdf)
- (arXiv 2021.09) FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting, [[Paper]](https://arxiv.org/pdf/2109.02974.pdf), [[Code]](https://github.com/ruiliu-ai/FuseFormer)
- (arXiv 2021.09) Hierarchical Multimodal Transformer to Summarize Videos, [[Paper]](https://arxiv.org/pdf/2109.10559.pdf)
- (arXiv 2021.10) Object-Region Video Transformers, [[Paper]](https://arxiv.org/pdf/2110.06915.pdf), [[Code]](https://roeiherz.github.io/ORViT/)
- (arXiv 2021.10) Can't Fool Me: Adversarially Robust Transformer for Video Understanding, [[Paper]](https://arxiv.org/pdf/2110.13950.pdf), [[Code]](https://roeiherz.github.io/ORViT/)
- (arXiv 2021.11) Livestock Monitoring with Transformer, [[Paper]](https://arxiv.org/pdf/2111.00801.pdf)
- (arXiv 2021.11) Sparse Adversarial Video Attacks with Spatial Transformations, [[Paper]](https://arxiv.org/pdf/2111.05468.pdf), [[Code]](https://github.com/TrustAI/DeepSAVA)
- (arXiv 2021.11) PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer, [[Paper]](https://arxiv.org/pdf/2111.12082.pdf), [[Code]](https://github.com/ZitongYu/PhysFormer)
- (arXiv 2021.11) Efficient Video Transformers with Spatial-Temporal Token Selection, [[Paper]](https://arxiv.org/pdf/2111.11591.pdf)
- (arXiv 2021.11) Video Frame Interpolation Transformer, [[Paper]](https://arxiv.org/pdf/2111.13817.pdf)
- (arXiv 2021.12) Self-supervised Video Transformer, [[Paper]](https://arxiv.org/pdf/2112.01514.pdf), [[Code]](https://git.io/J1juJ)
- (arXiv 2021.12) BEVT: BERT Pretraining of Video Transformers, [[Paper]](https://arxiv.org/pdf/2112.01529.pdf)
- (arXiv 2021.12) TBN-ViT: Temporal Bilateral Network with Vision Transformer for Video Scene Parsing, [[Paper]](https://arxiv.org/pdf/2112.01033.pdf)
- (arXiv 2021.12) Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval, [[Paper]](https://arxiv.org/pdf/2112.04446.pdf)
- (arXiv 2021.12) DualFormer: Local-Global Stratified Transformer for Efficient Video Recognition, [[Paper]](https://arxiv.org/pdf/2112.04674.pdf)
- (arXiv 2021.12) A Bilingual, OpenWorld Video Text Dataset and End-to-end Video Text Spotter with Transformer, [[Paper]](https://arxiv.org/pdf/2112.04888.pdf), [[Code]](https://github.com/weijiawu/BOVText-Benchmark)
- (arXiv 2021.12) Mask2Former for Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2112.10764.pdf), [[Code]](https://github.com/facebookresearch/Mask2Former)
- (arXiv 2021.12) LocFormer: Enabling Transformers to Perform Temporal Moment Localization on Long Untrimmed Videos With a Feature Sampling Approach, [[Paper]](https://arxiv.org/pdf/2112.10066.pdf)
- (arXiv 2021.12) Video Joint Modelling Based on Hierarchical Transformer for Co-summarization, [[Paper]](https://arxiv.org/pdf/2112.13478.pdf)
- (arXiv 2021.12) Siamese Network with Interactive Transformer for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2112.13983.pdf), [[Code]](https://github.com/LANMNG/SITVOS)
- (arXiv 2022.01) Multiview Transformers for Video Recognition,[[Paper]](https://arxiv.org/pdf/2201.04288.pdf)
- (arXiv 2022.01) TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers,[[Paper]](https://arxiv.org/pdf/2201.05047.pdf)
- (arXiv 2022.01) MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition,[[Paper]](https://arxiv.org/pdf/2201.08383.pdf)
- (arXiv 2022.01) Explore and Match: End-to-End Video Grounding with Transformer,[[Paper]](https://arxiv.org/pdf/2201.10168.pdf)
- (arXiv 2022.01) VRT: A Video Restoration Transformer,[[Paper]](https://arxiv.org/pdf/2201.12288.pdf), [[Code]](https://github.com/JingyunLiang/VRT)
- (arXiv 2022.02) Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval, [[Paper]](https://arxiv.org/pdf/2202.06014.pdf), [[Code]](https://git.openi.org.cn/zangxh/PiT.git)
- (arXiv 2022.02) Audio Visual Scene-Aware Dialog Generation with Transformer-based Video Representations, [[Paper]](https://arxiv.org/pdf/2202.09979.pdf)
- (arXiv 2022.02) Instantaneous Physiological Estimation using Video Transformers, [[Paper]](https://arxiv.org/pdf/2202.12368.pdf), [[Code]](https://github.com/revanurambareesh/instantaneous_transformer)
- (arXiv 2022.03) Spatio-temporal Vision Transformer for Super-resolution Microscopy, [[Paper]](https://arxiv.org/pdf/2203.00030.pdf), [[Code]](https://github.com/charlesnchr/vsr-sim)
- (arXiv 2022.03) ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection, [[Paper]](https://arxiv.org/pdf/2203.01562.pdf)
- (arXiv 2022.03) End-to-End Video Text Spotting with Transformer, [[Paper]](https://arxiv.org/pdf/2203.10539.pdf), [[Code]](https://github.com/weijiawu/TransDETR)
- (arXiv 2022.03) Associating Objects with Scalable Transformers for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2203.11442.pdf), [[Code]](https://github.com/z-x-yang/AOT)
- (arXiv 2022.03) Self-supervised Video-centralised Transformer for Video Face Clustering, [[Paper]](https://arxiv.org/pdf/2203.13166.pdf)
- (arXiv 2022.03) VPTR: Efficient Transformers for Video Prediction, [[Paper]](https://arxiv.org/pdf/2203.15836.pdf), [[Code]](https://github.com/XiYe20/VPTR)
- (arXiv 2022.03) Deformable Video Transformer, [[Paper]](https://arxiv.org/pdf/2203.16795.pdf)
- (arXiv 2023.03) Query-Dependent Video Representation for Moment Retrieval and Highlight Detection, [[Paper]](https://arxiv.org/pdf/2303.13874.pdf), [[Code]](http://github.com/wjun0830/QD-DETR)
- (arXiv 2022.04) Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer, [[Paper]](https://arxiv.org/pdf/2204.03638.pdf), [[Code]](https://songweige.github.io/projects/tats/index.html)
- (arXiv 2022.05) Video Frame Interpolation with Transformer, [[Paper]](https://arxiv.org/pdf/2205.07230.pdf), [[Code]](https://github.com/dvlab-research/VFIformer)
- (arXiv 2022.05) TubeFormer-DeepLab: Video Mask Transformer, [[Paper]](https://arxiv.org/pdf/2205.15361.pdf)
- (arXiv 2022.06) Recurrent Video Restoration Transformer with Guided Deformable Attention, [[Paper]](https://arxiv.org/pdf/2206.02146.pdf), [[Code]](https://github.com/JingyunLiang/RVRT)
- (arXiv 2022.06) Patch-based Object-centric Transformers for Efficient Video Generation, [[Paper]](https://arxiv.org/pdf/2206.04003.pdf), [[Code]](https://sites.google.com/view/povt-public)
- (arXiv 2022.06) Transformer-based Self-Supervised Fish Segmentation in Underwater Videos, [[Paper]](https://arxiv.org/pdf/2206.05390.pdf)
- (arXiv 2022.06) MaskViT: Masked Visual Pre-Training for Video Prediction, [[Paper]](https://arxiv.org/pdf/2206.11894.pdf), [[Code]](https://maskedvit.github.io/)
- (arXiv 2022.07) STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding, [[Paper]](https://arxiv.org/pdf/2207.02756.pdf)
- (arXiv 2022.07) Dual-Stream Transformer for Generic Event Boundary Captioning, [[Paper]](https://arxiv.org/pdf/2207.03038.pdf), [[Code]](https://github.com/GX77/Dual-Stream-Transformer-for-Generic-Event-Boundary-Captioning)
- (arXiv 2022.07) Cross-Attention Transformer for Video Interpolation, [[Paper]](https://arxiv.org/pdf/2207.04132.pdf), [[Code]](https://github.com/hannahhalin/TAIN)
- (arXiv 2022.07) Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet, [[Paper]](https://arxiv.org/pdf/2207.04320.pdf), [[Code]](https://github.com/JimmyZou/Snipper)
- (arXiv 2022.07) Time Is MattEr: Temporal Self-supervision for Video Transformers, [[Paper]](https://arxiv.org/pdf/2207.09067.pdf), [[Code]](https://github.com/alinlab/temporal-selfsupervision)
- (arXiv 2022.07) TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation, [[Paper]](https://arxiv.org/pdf/2207.09048.pdf), [[Code]](https://github.com/researchmm/TTVFI.git)
- (arXiv 2022.07) DeVIS: Making Deformable Transformers Work for Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2207.11103.pdf), [[Code]](https://github.com/acaelles97/DeVIS)
- (arXiv 2022.07) Video Swin Transformers for Egocentric Video Understanding @ Ego4D Challenges 2022, [[Paper]](https://arxiv.org/pdf/2207.11329.pdf)
- (arXiv 2022.08) BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2208.01159.pdf)
- (arXiv 2022.08) Two-Stream Transformer Architecture for Long Form Video Understanding, [[Paper]](https://arxiv.org/pdf/2208.01753.pdf)
- (arXiv 2022.08) PatchDropout: Economizing Vision Transformers Using Patch Dropout, [[Paper]](https://arxiv.org/pdf/2208.07220.pdf)
- (arXiv 2022.08) Class-attention Video Transformer for Engagement Intensity Prediction, [[Paper]](https://arxiv.org/pdf/2208.07216.pdf), [[Code]](https://github.com/mountainai/cavt)
- (arXiv 2022.08) Efficient Attention-free Video Shift Transformers, [[Paper]](https://arxiv.org/pdf/2208.11108.pdf), [[Code]](https://github.com/mountainai/cavt)
- (arXiv 2022.09) PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection, [[Paper]](https://arxiv.org/pdf/2209.02242.pdf), [[Code]](https://github.com/Hon-Wong/PTSEFormer)
- (arXiv 2022.09) Spatial-Temporal Transformer for Video Snapshot Compressive Imaging, [[Paper]](https://arxiv.org/pdf/2209.01578.pdf), [[Code]](https://github.com/ucaswangls/STFormer.git)
- (arXiv 2022.09) An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling, [[Paper]](https://arxiv.org/pdf/2209.01540.pdf)
- (arXiv 2022.09) Video Vision Transformers for Violence Detection, [[Paper]](https://arxiv.org/pdf/2209.03561.pdf)
- (arXiv 2022.09) On the Surprising Effectiveness of Transformers in Low-Labeled Video Recognition, [[Paper]](https://arxiv.org/pdf/2209.07474.pdf)
- (arXiv 2022.09) Real-time Online Video Detection with Temporal Smoothing Transformers, [[Paper]](https://arxiv.org/pdf/2209.09236.pdf), [[Code]](https://github.com/zhaoyue-zephyrus/TeSTra)
- (arXiv 2022.09) Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos, [[Paper]](https://arxiv.org/pdf/2209.09484.pdf)
- (arXiv 2022.10) Temporally Consistent Video Transformer for Long-Term Video Prediction, [[Paper]](https://arxiv.org/pdf/2210.02396.pdf), [[Project]](https://wilson1yan.github.io/teco)
- (arXiv 2022.10) Video Referring Expression Comprehension via Transformer with Content-aware Query, [[Paper]](https://arxiv.org/pdf/2210.02953.pdf), [[Project]](https://github.com/mengcaopku/ContFormer)
- (arXiv 2022.10) Turbo Training with Token Dropout, [[Paper]](https://arxiv.org/pdf/2210.04889.pdf)
- (arXiv 2022.10) Temporal and Contextual Transformer for Multi-Camera Editing of TV Shows, [[Paper]](https://arxiv.org/pdf/2210.08737.pdf)
- (arXiv 2022.10) TransVisDrone: Spatio-Temporal Transformer for Vision-based Drone-to-Drone Detection in Aerial Videos, [[Paper]](https://arxiv.org/pdf/2210.08423.pdf), [[Code]](https://github.com/tusharsangam/TransVisDrone)
- (arXiv 2022.10) Linear Video Transformer with Feature Fixation, [[Paper]](https://arxiv.org/pdf/2210.08164.pdf)
- (arXiv 2022.10) Transfer-learning for video classification: Video Swin Transformer on multiple domains, [[Paper]](https://arxiv.org/pdf/2210.09969.pdf)
- (arXiv 2022.10) ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design, [[Paper]](https://arxiv.org/pdf/2210.09573.pdf)
- (arXiv 2022.10) Foreground Guidance and Multi-Layer Feature Fusion for Unsupervised Object Discovery with Transformers, [[Paper]](https://arxiv.org/pdf/2210.13053.pdf), [[Code]](https://github.com/VDIGPKU/FORMULA)
- (arXiv 2022.10) Fully-attentive and interpretable: vision and video vision transformers for pain detection, [[Paper]](https://arxiv.org/pdf/2210.15769.pdf), [[Code]](https://github.com/IPDTFE/ViT-McMaster)
- (arXiv 2022.11) SCOTCH and SODA: A Transformer Video Shadow Detection Framework, [[Paper]](https://arxiv.org/pdf/2211.06885.pdf)
- (arXiv 2022.11) SATVSR: Scenario Adaptive Transformer for Cross Scenarios Video Super-Resolution, [[Paper]](https://arxiv.org/pdf/2211.08703.pdf)
- (arXiv 2022.11) UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer, [[Paper]](https://arxiv.org/pdf/2211.09552.pdf), [[Code]](https://github.com/OpenGVLab/UniFormerV2)
- (arXiv 2022.11) SuperTran: Reference Based Video Transformer for Enhancing Low Bitrate Streams in Real Time, [[Paper]](https://arxiv.org/pdf/2211.12604.pdf)
- (arXiv 2022.11) PatchBlender: A Motion Prior for Video Transformers, [[Paper]](https://arxiv.org/pdf/2211.14449.pdf)
- (arXiv 2022.12) Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning, [[Paper]](https://arxiv.org/pdf/2212.03229.pdf)
- (arXiv 2022.12) FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer, [[Paper]](https://arxiv.org/pdf/2212.03145.pdf)
- (arXiv 2022.12) PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data, [[Paper]](https://arxiv.org/pdf/2212.04821.pdf)
- (arXiv 2022.12) Video Prediction by Efficient Transformers, [[Paper]](https://arxiv.org/pdf/2212.06026.pdf), [[Code]](https://github.com/XiYe20/VPTR)
- (arXiv 2022.12) MAGVIT: Masked Generative Video Transformer, [[Paper]](https://arxiv.org/pdf/2212.06026.pdf), [[Code]](https://magvit.cs.cmu.edu/)
- (arXiv 2022.12) Efficient Movie Scene Detection using State-Space Transformers, [[Paper]](https://arxiv.org/pdf/2212.14427.pdf)
- (arXiv 2023.02) Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2302.11325.pdf), [[Code]](https://github.com/SimonZeng7108/Video-SwinUNet)
- (arXiv 2023.03) Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers, [[Paper]](https://arxiv.org/pdf/2303.09164.pdf)
- (arXiv 2023.03) A transformer-based approach to video frame-level prediction in Affective Behaviour Analysis In-the-wild, [[Paper]](https://arxiv.org/pdf/2303.09293.pdf)
- (arXiv 2023.03) TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction, [[Paper]](https://arxiv.org/pdf/2303.09807.pdf)
- (arXiv 2023.03) Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers, [[Paper]](https://arxiv.org/pdf/2303.11251.pdf)
- (arXiv 2023.03) MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models, [[Paper]](https://arxiv.org/pdf/2303.13009.pdf), [[Code]](https://github.com/mlvlab/MELTR)
- (arXiv 2023.03) Unmasked Teacher: Towards Training-Efficient Video Foundation Models, [[Paper]](https://arxiv.org/pdf/2303.16058.pdf), [[Code]](https://github.com/OpenGVLab/unmasked_teacher)
- (arXiv 2023.04) SVT: Supertoken Video Transformer for Efficient Video Understanding, [[Paper]](https://arxiv.org/pdf/2304.00325.pdf)
- (arXiv 2023.04) Hierarchical Vision Transformers for Cardiac Ejection Fraction Estimation, [[Paper]](https://arxiv.org/pdf/2304.00177.pdf), [[Code]](https://github.com/lhfazry/UltraSwin)
- (arXiv 2023.04) MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation, [[Paper]](https://arxiv.org/pdf/2304.05930.pdf)
- (arXiv 2023.04) SViTT: Temporal Learning of Sparse Video-Text Transformers, [[Paper]](https://arxiv.org/pdf/2304.08809.pdf), [[Code]](http://svcl.ucsd.edu/projects/svitt)
- (arXiv 2023.04) StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos, [[Paper]](https://arxiv.org/pdf/2304.13265.pdf)
- (arXiv 2023.04) Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations, [[Paper]](https://arxiv.org/pdf/2304.13089.pdf)
- (arXiv 2023.05) Transformer-based model for monocular visual odometry: a video understanding approach, [[Paper]](https://arxiv.org/pdf/2305.06121.pdf)
- (arXiv 2023.05) VDT: An Empirical Study on Video Diffusion with Transformers, [[Paper]](https://arxiv.org/pdf/2305.13311.pdf), [[Code]](https://github.com/RERV/VDT)
- (arXiv 2023.05) Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2305.16318.pdf), [[Code]](https://github.com/OpenGVLab/MUTR)
- (arXiv 2023.05) MS-DETR: Natural Language Video Localization with Sampling Moment-Moment Interaction, [[Paper]](https://arxiv.org/pdf/2305.18969.pdf), [[Code]](https://github.com/K-Nick/MS-DETR)
- (arXiv 2023.06) Backchannel Detection and Agreement Estimation from Video with Transformer Networks, [[Paper]](https://arxiv.org/pdf/2306.01656.pdf)
- (arXiv 2023.06) Unmasking Deepfakes: Masked Autoencoding Spatiotemporal Transformers for Enhanced Video Forgery Detection, [[Paper]](https://arxiv.org/pdf/2306.06881.pdf)
- (arXiv 2023.06) Unfolding Framework with Prior of Convolution-Transformer Mixture and Uncertainty Estimation for Video Snapshot Compressive Imaging, [[Paper]](https://arxiv.org/pdf/2306.11316.pdf)
- (arXiv 2023.06) TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting, [[Paper]](https://arxiv.org/pdf/2306.10940.pdf), [[Code]](https://github.com/Orion-Ai-Lab/TeleViT)
- (arXiv 2023.07) Efficient Convolution and Transformer-Based Network for Video Frame Interpolation, [[Paper]](https://arxiv.org/pdf/2307.06443.pdf)
- (arXiv 2023.07) Hierarchical Spatiotemporal Transformers for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2307.08263.pdf)
- (arXiv 2023.07) Video Frame Interpolation with Flow Transformer, [[Paper]](https://arxiv.org/pdf/2307.16144.pdf)
- (arXiv 2023.07) Temporally-Adaptive Models for Efficient Video Understanding, [[Paper]](https://arxiv.org/pdf/2308.05787.pdf), [[Code]](https://github.com/alibaba-mmai-research/TAdaConv)
- (arXiv 2023.08) EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2308.04162.pdf), [[Code]](https://github.com/lab206/EPCFormer)
- (arXiv 2023.08) Video OWL-ViT: Temporally-consistent open-world localization in video, [[Paper]](https://arxiv.org/pdf/2308.11062.pdf), [[Code]](https://github.com/google-research/scenic)
- (arXiv 2023.08) Spherical Vision Transformer for 360-degree Video Saliency Prediction, [[Paper]](https://arxiv.org/pdf/2308.13004.pdf)
- (arXiv 2023.09) Self-Supervised Video Transformers for Isolated Sign Language Recognition, [[Paper]](https://arxiv.org/pdf/2309.02450.pdf)
- (arXiv 2023.09) CATR: Combinatorial-Dependence Audio-Queried Transformer for Audio-Visual Video Segmentation, [[Paper]](https://arxiv.org/pdf/2309.09709.pdf), [[Code]](https://github.com/aspirinone/CATR.github.io)
- (arXiv 2023.09) PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation, [[Paper]](https://arxiv.org/pdf/2309.12303.pdf), [[Code]](https://github.com/shilinyan99/PanoVOS)
- (arXiv 2023.09) Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2309.11933.pdf)
- (arXiv 2023.09) Spatial-Temporal Transformer based Video Compression Framework, [[Paper]](https://arxiv.org/pdf/2309.11913.pdf)
- (arXiv 2023.10) Video Transformers under Occlusion: How Physics and Background Attributes Impact Large Models for Robotic Manipulation, [[Paper]](https://arxiv.org/pdf/2310.02044.pdf), [[Code]](https://github.com/ShutongJIN/OccluManip.git)
- (arXiv 2023.10) Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition, [[Paper]](https://arxiv.org/pdf/2310.03108.pdf)
- (arXiv 2023.10) Video Referring Expression Comprehension via Transformer with Content-conditioned Query, [[Paper]](https://arxiv.org/pdf/2310.16402.pdf)
- (arXiv 2023.11) Concatenated Masked Autoencoders as Spatial-Temporal Learner, [[Paper]](https://arxiv.org/pdf/2311.00961.pdf), [[Code]](https://github.com/minhoooo1/CatMAE)
- (arXiv 2023.11) Multi-entity Video Transformers for Fine-Grained Video Representation Learning, [[Paper]](https://arxiv.org/pdf/2311.10873.pdf), [[Code]](https://github.com/facebookresearch/video_rep_learning)
- (arXiv 2023.11) E-ViLM: Efficient Video-Language Model via Masked Video Modeling with Semantic Vector-Quantized Tokenizer, [[Paper]](https://arxiv.org/pdf/2311.17267.pdf)
- (arXiv 2023.11) Just Add 蟺! Pose Induced Video Transformers for Understanding Activities of Daily Living, [[Paper]](https://arxiv.org/pdf/2311.18840.pdf), [[Code]](https://github.com/dominickrei/pi-vit)
- (arXiv 2023.12) DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding, [[Paper]](https://arxiv.org/pdf/2312.02549.pdf)
- (arXiv 2023.12) Unleashing the Power of CNN and Transformer for Balanced RGB-Event Video Recognition, [[Paper]](https://arxiv.org/pdf/2312.11128.pdf), [[Code]](https://github.com/Event-AHU/TSCFormer)
- (arXiv 2023.12) MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers, [[Paper]](https://arxiv.org/pdf/2312.12468.pdf), [[Code]](https://maskint.github.io/)
- (arXiv 2024.01) GloTSFormer: Global Video Text Spotting Transformer, [[Paper]](https://arxiv.org/pdf/2401.03694.pdf)
- (arXiv 2024.01) Latte: Latent Diffusion Transformer for Video Generation, [[Paper]](https://arxiv.org/pdf/2401.03048.pdf), [[Code]](https://maxin-cn.github.io/latte_project)
- (arXiv 2024.01) HaltingVT: Adaptive Token Halting Transformer for Efficient Video Recognition, [[Paper]](https://arxiv.org/pdf/2401.04975.pdf), [[Code]](https://github.com/dun-research/HaltingVT)
- (arXiv 2024.01) Transformer-based Video Saliency Prediction with High Temporal Dimension Decoding, [[Paper]](https://arxiv.org/pdf/2401.07942.pdf)
- (arXiv 2024.01) Understanding Video Transformers via Universal Concept Discovery, [[Paper]](https://arxiv.org/pdf/2401.10831.pdf), [[Code]](https://yorkucvil.github.io/VTCD)
- (arXiv 2024.01) VJT: A Video Transformer on Joint Tasks of Deblurring, Low-light Enhancement and Denoising, [[Paper]](https://arxiv.org/pdf/2401.14754.pdf)
- (arXiv 2024.03) A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video, [[Paper]](https://arxiv.org/pdf/2403.03461.pdf)
- (arXiv 2024.03) OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework, [[Paper]](https://arxiv.org/pdf/2403.08682.pdf)
- (arXiv 2024.03) vid-TLDR: Training Free Token merging for Light-weight Video Transformer, [[Paper]](https://arxiv.org/pdf/2403.13347.pdf), [[Code]](https://github.com/mlvlab/vid-TLDR)

### Visual Grounding
- (arXiv 2021.04) TransVG: End-to-End Visual Grounding with Transformers, [[Paper]](https://arxiv.org/abs/2104.08541)
- (arXiv 2021.05) Visual Grounding with Transformers, [[Paper]](https://arxiv.org/pdf/2105.04281.pdf)
- (arXiv 2021.06) Referring Transformer: A One-step Approach to Multi-task Visual Grounding, [[Paper]](https://arxiv.org/pdf/2106.03089.pdf)
- (arXiv 2021.08) Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding, [[Paper]](https://arxiv.org/pdf/2108.00205.pdf)
- (arXiv 2021.08) TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding, [[Paper]](https://arxiv.org/pdf/2108.02388.pdf)
- (arXiv 2021.09) Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation, [[Paper]](https://arxiv.org/pdf/2109.08478.pdf)
- (arXiv 2022.02) ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer, [[Paper]](https://arxiv.org/pdf/2202.07305.pdf)
- (arXiv 2022.03) TubeDETR: Spatio-Temporal Video Grounding with Transformers, [[Paper]](https://arxiv.org/pdf/2203.16434.pdf), [[Code]](https://antoyang.github.io/tubedetr.html)
- (arXiv 2022.04) Multi-View Transformer for 3D Visual Grounding, [[Paper]](https://arxiv.org/pdf/2204.02174.pdf), [[Code]](https://github.com/sega-hsj/MVT-3DVG)
- (arXiv 2022.06) TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.06619v1.pdf), [[Code]](https://github.com/djiajunustc/TransVG)
- (arXiv 2022.09) Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding, [[Paper]](https://arxiv.org/pdf/2209.13959.pdf)
- (arXiv 2022.11) UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding, [[Paper]](https://arxiv.org/pdf/2212.00836.pdf)
- (arXiv 2023.03) ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance, [[Paper]](https://arxiv.org/pdf/2303.16894.pdf), [[Code]](https://github.com/ZiyuGuo99/ViewRefer3D)
- (arXiv 2023.07) Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding, [[Paper]](https://arxiv.org/pdf/2307.09267.pdf)
- (arXiv 2023.07) Advancing Visual Grounding with Scene Knowledge: Benchmark and Method, [[Paper]](https://arxiv.org/pdf/2307.11558.pdf), [[Code]](https://github.com/zhjohnchan/SK-VG)
- (arXiv 2023.07) 3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding, [[Paper]](https://arxiv.org/pdf/2307.13363.pdf)
- (arXiv 2023.08) ViGT: Proposal-free Video Grounding with Learnable Token in Transformer, [[Paper]](https://arxiv.org/pdf/2308.06009.pdf)
- (arXiv 2023.08) Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2308.03725.pdf)
- (arXiv 2023.08) Knowing Where to Focus: Event-aware Transformer for Video Grounding, [[Paper]](https://arxiv.org/pdf/2308.06947.pdf), [[Code]](https://github.com/jinhyunj/SANet)
- (arXiv 2023.08) Language-Guided Diffusion Model for Visual Grounding, [[Paper]](https://arxiv.org/pdf/2308.09599.pdf), [[Code]](https://github.com/iQua/vgbase/tree/DiffusionVG)
- (arXiv 2023.12) BAM-DETR: Boundary-Aligned Moment Detection Transformer for Temporal Sentence Grounding in Videos, [[Paper]](https://arxiv.org/pdf/2312.00083.pdf), [[Code]](https://github.com/Pilhyeon/BAM-DETR)
- (arXiv 2023.12) Grounding Everything: Emerging Localization Properties in Vision-Language Transformers, [[Paper]](https://arxiv.org/pdf/2312.00878.pdf), [[Code]](https://github.com/WalBouss/GEM)
- (arXiv 2023.12) Mono3DVG: 3D Visual Grounding in Monocular Images, [[Paper]](https://arxiv.org/pdf/2312.08022.pdf)
- (arXiv 2023.12) GroundVLP: Harnessing Zero-shot Visual Grounding from Vision-Language Pre-training and Open-Vocabulary Object Detection, [[Paper]](https://arxiv.org/pdf/2312.15043.pdf), [[Code]](https://github.com/om-ai-lab/GroundVLP)
- (arXiv 2024.01) Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding, [[Paper]](https://arxiv.org/pdf/2401.00901.pdf), [[Code]](https://github.com/TalalWasim/Video-GroundingDINO)
- (arXiv 2024.03) MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual Grounding, [[Paper]](https://arxiv.org/pdf/2403.03077.pdf), [[Code]](https://github.com/birdy666/MiKASA-3DVG)

### Visual Question Answering
- (arXiv 2021.12) LaTr: Layout-Aware Transformer for Scene-Text VQA, [[Paper]](https://arxiv.org/pdf/2112.12494.pdf)
- (arXiv 2022.01) On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering, [[Paper]](https://arxiv.org/pdf/2201.03965.pdf)
- (arXiv 2022.01) Transformer Module Networks for Systematic Generalization in Visual Question Answering, [[Paper]](https://arxiv.org/pdf/2201.11316.pdf)
- (arXiv 2022.01) Towards Efficient and Elastic Visual Question Answering with Doubly Slimmable Transformer, [[Paper]](https://arxiv.org/pdf/2203.12814.pdf)
- (arXiv 2022.04) Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering, [[Paper]](https://arxiv.org/pdf/2204.10448.pdf), [[Code]](https://github.com/yujungheo/kbvqa-public)
- (arXiv 2022.06) DisCoVQA: Temporal Distortion-Content Transformers for Video Quality Assessment, [[Paper]](https://arxiv.org/pdf/2206.09265.pdf), [[Code]](https://github.com/yujungheo/kbvqa-public)
- (arXiv 2022.06) Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer, [[Paper]](https://arxiv.org/pdf/2206.11053.pdf), [[Code]](https://github.com/lalithjets/Surgical_VQA.git)
- (arXiv 2022.07) Weakly Supervised Grounding for VQA in Vision-Language Transformers, [[Paper]](https://arxiv.org/pdf/2207.02334.pdf), [[Code]](https://github.com/aurooj/WSG-VQA-VLTransformers)
- (arXiv 2022.07) Video Graph Transformer for Video Question Answering, [[Paper]](https://arxiv.org/pdf/2207.05342.pdf), [[Code]](https://github.com/sail-sg/VGT)
- (arXiv 2022.10) Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing, [[Paper]](https://arxiv.org/pdf/2210.04510.pdf)
- (arXiv 2022.10) SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models, [[Paper]](https://arxiv.org/pdf/2210.05950.pdf), [[Project]](https://slotformer.github.io/)
- (arXiv 2022.12) MIST: Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering, [[Paper]](https://arxiv.org/pdf/2212.09522.pdf), [[Project]](https://github.com/showlab/mist)
- (arXiv 2023.02) Efficient End-to-End Video Question Answering with Pyramidal Multimodal Transformer, [[Paper]](https://arxiv.org/pdf/2302.02136.pdf), [[Project]](https://github.com/Trunpm/PMT-AAAI23)
- (arXiv 2023.03) PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers, [[Paper]](https://arxiv.org/pdf/2303.09187.pdf)
- (arXiv 2023.04) Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder, [[Paper]](https://arxiv.org/pdf/2304.01611.pdf)
- (arXiv 2023.05) Multimodal Graph Transformer for Multimodal Question Answering, [[Paper]](https://arxiv.org/pdf/2305.00581.pdf)
- (arXiv 2023.05) Is a Video worth n脳n Images? A Highly Efficient Approach to Transformer-based Video Question Answering, [[Paper]](https://arxiv.org/pdf/2305.09107.pdf)
- (arXiv 2023.06) LiT-4-RSVQA: Lightweight Transformer-based Visual Question Answering in Remote Sensing, [[Paper]](https://arxiv.org/pdf/2306.00758.pdf), [[Code]](https://git.tu-berlin.de/rsim/lit4rsvqa)
- (arXiv 2023.07) Discovering Spatio-Temporal Rationales for Video Question Answering, [[Paper]](https://arxiv.org/pdf/2307.12058.pdf), [[Code]](https://github.com/yl3800/TranSTR)
- (arXiv 2023.07) BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering, [[Paper]](https://arxiv.org/pdf/2307.15335.pdf)
- (arXiv 2023.08) Redundancy-aware Transformer for Video Question Answering, [[Paper]](https://arxiv.org/pdf/2308.03267.pdf)

### Visual Reasoning
- (arXiv 2021.11) Recurrent Vision Transformer for Solving Visual Reasoning Problems, [[Paper]](https://arxiv.org/pdf/2111.14576.pdf)
- (arXiv 2022.04) RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning, [[Paper]](https://arxiv.org/pdf/2204.11167.pdf), [[Code]](https://github.com/NVlabs/RelViT)
- (arXiv 2022.06) SAViR-T: Spatially Attentive Visual Reasoning with Transformers, [[Paper]](https://arxiv.org/pdf/2204.11167.pdf)
- (arXiv 2023.01) Pseudo 3D Perception Transformer with Multi-level Confidence Optimization for Visual Commonsense Reasoning, [[Paper]](https://arxiv.org/pdf/2301.13335.pdf)
- (arXiv 2024.03) ViTCN: Vision Transformer Contrastive Network For Reasoning, [[Paper]](https://arxiv.org/pdf/2403.09962.pdf)

### Visual Relationship Detection
- (arXiv 2021.04) RelTransformer: Balancing the Visual Relationship Detection from Local Context, Scene and Memory, [[Paper]](https://arxiv.org/pdf/2104.11934.pdf)
- (arXiv 2021.05) Visual Composite Set Detection Using Part-and-Sum Transformers, [[Paper]](https://arxiv.org/pdf/2108.00045.pdf)
- (arXiv 2021.08) Discovering Spatial Relationships by Transformers for Domain Generalization, [[Paper]](https://arxiv.org/pdf/2108.10046.pdf)
- (arXiv 2022.06) VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection, [[Paper]](https://arxiv.org/pdf/2206.09111.pdf)
- (arXiv 2023.11) Self-Supervised Learning for Visual Relationship Detection through Masked Bounding Box Reconstruction, [[Paper]](https://arxiv.org/pdf/2311.04834.pdf), [[Code]](https://github.com/deeplab-ai/SelfSupervisedVRD)
- (arXiv 2023.11) RelVAE: Generative Pretraining for few-shot Visual Relationship Detection, [[Paper]](https://arxiv.org/pdf/2311.16261.pdf), [[Code]](https://github.com/deeplab-ai/SelfSupervisedVRD)
- (arXiv 2024.03) Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection, [[Paper]](https://arxiv.org/pdf/2403.14270.pdf)

### Voxel
- (arXiv 2021.05) SVT-Net: A Super Light-Weight Network for Large Scale Place Recognition using Sparse Voxel Transformers, [[Paper]](https://arxiv.org/abs/2105.00149)
- (arXiv 2021.09) Voxel Transformer for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2109.02497.pdf)
- (arXiv 2022.06) Unifying Voxel-based Representation with Transformer for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2206.00630.pdf), [[Code]](https://github.com/dvlab-research/UVTR)
- (arXiv 2023.02) VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion, [[Paper]](https://arxiv.org/pdf/2302.12251.pdf), [[Code]](https://github.com/NVlabs/VoxFormer)
- (arXiv 2023.03) Event Voxel Set Transformer for Spatiotemporal Representation Learning on Event Streams, [[Paper]](https://arxiv.org/pdf/2303.03856.pdf)
- (arXiv 2023.03) SnakeVoxFormer: Transformer-based Single Image Voxel Reconstruction with Run Length Encoding, [[Paper]](https://arxiv.org/pdf/2303.16293.pdf)
- (arXiv 2023.05) PVT-SSD: Single-Stage 3D Object Detector with Point-Voxel Transformer, [[Paper]](https://arxiv.org/pdf/2305.06621.pdf), [[Code]](https://github.com/Nightmare-n/PVT-SSD)
- (arXiv 2023.08) Learning Bottleneck Transformer for Event Image-Voxel Feature Fusion based Classification, [[Paper]](https://arxiv.org/pdf/2308.11937.pdf), [[Code]](https://github.com/Event-AHU/EFV_event_classification)
- (arXiv 2024.01) ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention, [[Paper]](https://arxiv.org/pdf/2401.00912.pdf), [[Code]](https://github.com/skyhehe123/ScatterFormer)
- (arXiv 2024.01) MsSVT++: Mixed-scale Sparse Voxel Transformer with Center Voting for 3D Object Detection, [[Paper]](https://arxiv.org/pdf/2401.11718.pdf)

### Weakly Supervised Learning
- (arXiv 2021.12) LCTR: On Awakening the Local Continuity of Transformer for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2112.05291)
- (arXiv 2022.01) CaFT: Clustering and Filter on Tokens of Transformer for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2201.00475)
- (arXiv 2021.03) TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2103.14862)
- (arXiv 2022.04) ViTOL: Vision Transformer for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2204.06772), [[Code]](https://github.com/Saurav-31/ViTOL)
- (arXiv 2022.07) Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration, [[Paper]](https://arxiv.org/abs/2207.10447), [[Code]](https://github.com/164140757/SCM)
- (arXiv 2022.08) Re-Attention Transformer for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2208.01838), [[Code]](https://github.com/su-hui-zz/ReAttentionTransformer)
- (arXiv 2022.09) Discriminative Sampling of Proposals in Self-Supervised Transformers for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2209.09209), [[Code]](https://github.com/shakeebmurtaza/dips)
- (arXiv 2022.09) PicT: A Slim Weakly Supervised Vision Transformer for Pavement Distress Classification, [[Paper]](https://arxiv.org/abs/2209.10074), [[Code]](https://github.com/DearCaat/PicT)
- (arXiv 2023.09) Semantic-Constraint Matching Transformer for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2309.01331)
- (arXiv 2023.10) DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised Transformers for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2310.06196), [[Code]](https://github.com/shakeebmurtaza/dips)
- (arXiv 2023.12) Multiscale Vision Transformer With Deep Clustering-Guided Refinement for Weakly Supervised Object Localization, [[Paper]](https://arxiv.org/abs/2312.09584)

### Zero-Shot Learning
- (arXiv 2021.08) Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning, [[Paper]](https://arxiv.org/pdf/2108.00205.pdf)
- (arXiv 2021.12) Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks, [[Paper]](https://arxiv.org/pdf/2112.01522.pdf)
- (arXiv 2021.12) TransZero: Attribute-guided Transformer for Zero-Shot Learning, [[Paper]](https://arxiv.org/pdf/2112.01683.pdf), [[Code]](https://github.com/shiming-chen/transzero)
- (arXiv 2021.12) TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning, [[Paper]](https://arxiv.org/pdf/2112.08643.pdf), [[Code]](https://github.com/shiming-chen/TransZero_pp)
- (arXiv 2022.03) Hybrid Routing Transformer for Zero-Shot Learning, [[Paper]](https://arxiv.org/pdf/2203.15310.pdf)
- (arXiv 2022.11) Efficient Zero-shot Visual Search via Target and Context-aware Transformer, [[Paper]](https://arxiv.org/pdf/2211.13470.pdf)
- (arXiv 2023.03) Vision Transformer-based Feature Extraction for Generalized Zero-Shot Learning, [[Paper]](https://arxiv.org/pdf/2302.00875.pdf)
- (arXiv 2023.03) Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers, [[Paper]](https://arxiv.org/pdf/2305.17328.pdf)
- (arXiv 2023.08) Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2308.06693.pdf), [[Code]](https://github.com/DLUT-yyc/Isomer)
- (arXiv 2023.08) Meta-ZSDETR: Zero-shot DETR with Meta-learning, [[Paper]](https://arxiv.org/pdf/2308.09540.pdf), [[Code]](https://github.com/DLUT-yyc/Isomer)
- (arXiv 2023.08) ViT-Lens: Towards Omni-modal Representations, [[Paper]](https://arxiv.org/pdf/2308.10185.pdf)
- (arXiv 2023.08) Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding, [[Paper]](https://arxiv.org/pdf/2308.11448.pdf)
- (arXiv 2023.11) SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation, [[Paper]](https://arxiv.org/pdf/2311.17707.pdf), [[Code]](https://mutianxu.github.io/sampro3d/)

### Others
- (CVPR'21') Transformer Interpretability Beyond Attention Visualization, [[Paper]](https://arxiv.org/abs/2012.09838), [[Code]](https://github.com/hila-chefer/Transformer-Explainability)
- (CVPR'21') Pre-Trained Image Processing Transformer, [[Paper]](https://arxiv.org/abs/2012.00364)
- (ICCV'21) PlaneTR: Structure-Guided Transformers for 3D Plane Recovery, [[Paper]](https://arxiv.org/pdf/2107.13108.pdf), [[Code]](https://github.com/IceTTTb/PlaneTR3D)
- (arXiv 2021.01) Learn to Dance with AIST++: Music Conditioned 3D Dance Generation, [[Paper]](https://arxiv.org/abs/2101.08779), [[Code]](https://google.github.io/aichoreographer/)
- (arXiv 2021.01) Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry, [[Paper]](https://arxiv.org/abs/2101.02143)
- (arXiv 2021.04) Cloth Interactive Transformer for Virtual Try-On, [[Paper]](https://arxiv.org/abs/2104.05519), [[Code]](https://github.com/Amazingren/CIT)
- (arXiv 2021.04) Fourier Image Transformer, [[Paper]](https://arxiv.org/pdf/2104.02555.pdf), [[Code]](3https://github.com/juglab/FourierImageTransformer)
- (arXiv 2021.05) Attention for Image Registration (AiR): an unsupervised Transformer approach, [[Paper]](https://arxiv.org/pdf/2105.02282.pdf)
- (arXiv 2021.05) IntFormer: Predicting pedestrian intention with the aid of the Transformer architecture, [[Paper]](https://arxiv.org/pdf/2105.08647.pdf)
- (arXiv 2021.06) A Comparison for Anti-noise Robustness of Deep Learning Classification Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to Visual Transformer and Performer, [[Paper]](https://arxiv.org/pdf/2106.01927.pdf)
- (arXiv 2021.06) Predicting Vehicles Trajectories in Urban Scenarios with Transformer Networks and Augmented Information, [[Paper]](https://arxiv.org/pdf/2106.00559.pdf)
- (arXiv 2021.06) StyTr2: Unbiased Image Style Transfer with Transformers, [[Paper]](https://arxiv.org/pdf/2105.14576.pdf)
- (arXiv 2021.06) Semantic Correspondence with Transformers, [[Paper]](https://arxiv.org/pdf/2106.02520.pdf)
- (arXiv 2021.06) Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue, [[Paper]](https://arxiv.org/pdf/2106.15550.pdf)
- (arXiv 2021.07) Grid Partitioned Attention: Efficient Transformer Approximation with Inductive Bias for High Resolution Detail Generation, [[Paper]](https://arxiv.org/pdf/2107.03742.pdf), [[Code]](https://github.com/zalandoresearch/gpa)
- (arXiv 2021.07) Image Fusion Transformer, [[Paper]](https://arxiv.org/pdf/2107.09011.pdf), [[Code]](https://github.com/Vibashan/Image-FusionTransformer)
- (arXiv 2021.07) PiSLTRc: Position-informed Sign Language Transformer with Content-aware Convolution, [[Paper]](https://arxiv.org/pdf/2107.12600.pdf)
- (arXiv 2021.07) PPT Fusion: Pyramid Patch Transformer for a Case Study in Image Fusion, [[Paper]](https://arxiv.org/pdf/2107.13967.pdf)
- (arXiv 2021.08) Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer, [[Paper]](https://arxiv.org/pdf/2108.00358.pdf)
- (arXiv 2021.08) Paint Transformer: Feed Forward Neural Painting with Stroke Prediction, [[Paper]](https://arxiv.org/pdf/2108.03798.pdf), [[Code]](https://github.com/Huage001/PaintTransformer)
- (arXiv 2021.08) The Right to Talk: An Audio-Visual Transformer Approach, [[Paper]](https://arxiv.org/pdf/2108.03256.pdf), [[Code]](https://github.com/uark-cviu/Right2Talk)
- (arXiv 2021.08) Embodied BERT: A Transformer Model for Embodied, Language-guided Visual Task Completion, [[Paper]](https://arxiv.org/pdf/2108.04927.pdf), [[Code]](https://github.com/amazon-research/embert)
- (arXiv 2021.08) Investigating transformers in the decomposition of polygonal shapes as point collections, [[Paper]](https://arxiv.org/pdf/2108.07533.pdf)
- (arXiv 2021.08) Convolutional Neural Network (CNN) vs Visual Transformer (ViT) for Digital Holography, [[Paper]](https://arxiv.org/pdf/2108.09147.pdf)
- (arXiv 2021.08) Construction material classification on imbalanced datasets for construction monitoring automation using Vision Transformer (ViT) architecture, [[Paper]](https://arxiv.org/pdf/2108.09527.pdf)
- (arXiv 2021.08) Spatial Transformer Networks for Curriculum Learning, [[Paper]](https://arxiv.org/pdf/2108.09696.pdf)
- (arXiv 2021.09) TransforMesh: A Transformer Network for Longitudinal modeling of Anatomical Meshes, [[Paper]](https://arxiv.org/pdf/2109.00532.pdf)
- (arXiv 2021.09) CTRL-C: Camera calibration TRansformer with Line-Classification, [[Paper]](https://arxiv.org/pdf/2109.02259.pdf), [[Code]](https://github.com/jwlee-vcl/CTRL-C)
- (arXiv 2021.09) The Animation Transformer: Visual Correspondence via Segment Matching, [[Paper]](https://arxiv.org/pdf/2109.02614.pdf)
- (arXiv 2021.09) Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer, [[Paper]](https://arxiv.org/pdf/2109.08024.pdf)
- (arXiv 2021.09) PETA: Photo Albums Event Recognition using Transformers Attention, [[Paper]](https://arxiv.org/pdf/2109.12499.pdf), [[Code]](https://github.com/Alibaba-MIIL/PETA)
- (arXiv 2021.10) ProTo: Program-Guided Transformer for Program-Guided Tasks, [[Paper]](https://arxiv.org/pdf/2110.00804.pdf)
- (arXiv 2021.10) TranSalNet: Visual saliency prediction using transformers, [[Paper]](https://arxiv.org/pdf/2110.03593.pdf)
- (arXiv 2021.10) Development and testing of an image transformer for explainable autonomous driving systems, [[Paper]](https://arxiv.org/pdf/2110.05559.pdf)
- (arXiv 2021.10) Leveraging redundancy in attention with Reuse Transformers, [[Paper]](https://arxiv.org/pdf/2110.06821.pdf)
- (arXiv 2021.10) Vis-TOP: Visual Transformer Overlay Processor, [[Paper]](https://arxiv.org/pdf/2110.10957.pdf)
- (arXiv 2021.10) TNTC: two-stream network with transformer-based complementarity for gait-based emotion recognition, [[Paper]](https://arxiv.org/pdf/2110.13708.pdf)
- (arXiv 2021.11) The self-supervised channel-spatial attention-based transformer network for automated, accurate prediction of crop nitrogen status from UAV imagery, [[Paper]](https://arxiv.org/pdf/2111.06839.pdf)
- (arXiv 2021.11) TRIG: Transformer-Based Text Recognizer with Initial Embedding Guidance, [[Paper]](https://arxiv.org/pdf/2111.08314.pdf)
- (arXiv 2021.11) Grounded Situation Recognition with Transformers, [[Paper]](https://arxiv.org/pdf/2111.10135.pdf), [[Code]](https://github.com/jhcho99/gsrtr)
- (arXiv 2021.11) Ice hockey player identification via transformers, [[Paper]](https://arxiv.org/pdf/2111.11535.pdf)
- (arXiv 2021.11) Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes, [[Paper]](https://arxiv.org/pdf/2111.12701.pdf)
- (arXiv 2021.11) Attention-based Dual-stream Vision Transformer for Radar Gait Recognition,[[Paper]](https://arxiv.org/pdf/2111.12290.pdf)
- (arXiv 2021.11) TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions,[[Paper]](https://arxiv.org/pdf/2111.14813.pdf), [[Code]](https://github.com/jeya-maria-jose/TransWeather)
- (arXiv 2021.11) BuildFormer: Automatic building extraction with vision transformer,[[Paper]](https://arxiv.org/pdf/2111.15637.pdf)
- (arXiv 2021.12) DoodleFormer: Creative Sketch Drawing with Transformers,[[Paper]](https://arxiv.org/pdf/2112.03258.pdf)
- (arXiv 2021.12) Transformer based trajectory prediction,[[Paper]](https://arxiv.org/pdf/2112.04350.pdf)
- (arXiv 2021.12) Deep ViT Features as Dense Visual Descriptors,[[Paper]](https://arxiv.org/pdf/2112.05814.pdf), [[Project]](https://dino-vit-features.github.io)
- (arXiv 2021.12) Hformer: Hybrid CNN-Transformer for Fringe Order Prediction in Phase Unwrapping of Fringe Projection,[[Paper]](https://arxiv.org/pdf/2112.05814.pdf)
- (arXiv 2021.12) 3D Question Answering,[[Paper]](https://arxiv.org/pdf/2112.08359.pdf)
- (arXiv 2021.12) Light Field Neural Rendering,[[Paper]](https://arxiv.org/pdf/2112.09687.pdf), [[Project]](https://light-field-neural-rendering.github.io/)
- (arXiv 2021.12) Nonlinear Transform Source-Channel Coding for Semantic Communications, [[Paper]](https://arxiv.org/pdf/2112.10961.pdf)
- (arXiv 2021.12) APRIL: Finding the Achilles' Heel on Privacy for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.14087.pdf)
- (arXiv 2022.01) Splicing ViT Features for Semantic Appearance Transfer, [[Paper]](https://arxiv.org/pdf/2201.00424.pdf), [[Project]](https://splice-vit.github.io/)
- (arXiv 2022.01) Learning class prototypes from Synthetic InSAR with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2201.03016.pdf)
- (arXiv 2022.01) Swin transformers make strong contextual encoders for VHR image road extraction, [[Paper]](https://arxiv.org/pdf/2201.03178.pdf)
- (arXiv 2022.01) Technical Report for ICCV 2021 Challenge SSLAD-Track3B: Transformers Are Better Continual Learners, [[Paper]](https://arxiv.org/pdf/2201.04924.pdf)
- (arXiv 2022.01) Spectral Compressive Imaging Reconstruction Using Convolution and Spectral Contextual Transformer, [[Paper]](https://arxiv.org/pdf/2201.05768.pdf)
- (arXiv 2022.01) VAQF: Fully Automatic Software-hardware Co-design Framework for Low-bit Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.06618.pdf)
- (arXiv 2022.01) Continual Transformers: Redundancy-Free Attention for Online Inference, [[Paper]](https://arxiv.org/pdf/2201.06268.pdf)
- (arXiv 2022.01) Disentangled Latent Transformer for Interpretable Monocular Height Estimation, [[Paper]](https://arxiv.org/pdf/2201.06357.pdf), [[Code]](https://github.com/ShadowXZT/DLT-Height-Estimation.pytorch)
- (arXiv 2022.01) A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization, [[Paper]](https://arxiv.org/pdf/2201.09206.pdf), [[Code]](https://github.com/Dmmm1997/FSRA)
- (arXiv 2022.01) Transformer-based SAR Image Despeckling, [[Paper]](https://arxiv.org/pdf/2201.09355.pdf), [[Code]](https://github.com/malshaV/sar_transformer)
- (arXiv 2022.01) Pre-Trained Language Transformers are Universal Image Classifiers, [[Paper]](https://arxiv.org/pdf/2201.10182.pdf)
- (arXiv 2022.01) Dual-Tasks Siamese Transformer Framework for Building Damage Assessment, [[Paper]](https://arxiv.org/pdf/2201.10953.pdf)
- (arXiv 2022.01) DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer, [[Paper]](https://arxiv.org/pdf/2201.11438.pdf)
- (arXiv 2022.01) Generalised Image Outpainting with U-Transformer, [[Paper]](https://arxiv.org/pdf/2201.11403.pdf)
- (arXiv 2022.02) Spherical Transformer, [[Paper]](https://arxiv.org/pdf/2202.04942.pdf)
- (arXiv 2022.02) Exploiting Spatial Sparsity for Event Cameras with Visual Transformers, [[Paper]](https://arxiv.org/pdf/2202.05054.pdf)
- (arXiv 2022.02) Spatial Transformer K-Means, [[Paper]](https://arxiv.org/pdf/2202.07829.pdf)
- (arXiv 2022.02) RNGDet: Road Network Graph Detection by Transformer in Aerial Images, [[Paper]](https://arxiv.org/pdf/2202.07824.pdf)
- (arXiv 2022.02) Image-to-Graph Transformers for Chemical Structure Recognition, [[Paper]](https://arxiv.org/pdf/2202.09580.pdf)
- (arXiv 2022.02) ProFormer: Learning Data-efficient Representations of Body Movement with Prototype-based Feature Augmentation and Visual Transformers, [[Paper]](https://arxiv.org/pdf/2202.11423.pdf), [[Code]](https://github.com/KPeng9510/ProFormer)
- (arXiv 2022.03) TableFormer: Table Structure Understanding with Transformers, [[Paper]](https://arxiv.org/pdf/2203.01017.pdf)
- (arXiv 2022.03) Ensembles of Vision Transformers as a New Paradigm for Automated Classification in Ecology, [[Paper]](https://arxiv.org/pdf/2203.01726.pdf)
- (arXiv 2022.03) A Unified Transformer Framework for Group-based Segmentation: Co-Segmentation, Co-Saliency Detection and Video Salient Object Detection, [[Paper]](https://arxiv.org/pdf/2203.04708.pdf), [[Code]](https://github.com/suyukun666/UFO)
- (arXiv 2022.03) PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction Transformer, [[Paper]](https://arxiv.org/pdf/2203.09293.pdf)
- (arXiv 2022.03) Cascade Transformers for End-to-End Person Search, [[Paper]](https://arxiv.org/pdf/2203.09642.pdf), [[Code]](https://github.com/Kitware/COAT)
- (arXiv 2022.03) SepTr: Separable Transformer for Audio Spectrogram Processing, [[Paper]](https://arxiv.org/pdf/2203.09581.pdf), [[Code]](https://github.com/ristea/septr)
- (arXiv 2022.03) CNNs and Transformers Perceive Hybrid Images Similar to Humans, [[Paper]](https://arxiv.org/pdf/2203.11678.pdf), [[Code]](https://github.com/aliborji/hybrid_images.git)
- (arXiv 2022.03) Disentangling Patterns and Transformations from One Sequence of Images with Shape-invariant Lie Group Transformer, [[Paper]](https://arxiv.org/pdf/2203.11210.pdf)
- (arXiv 2022.03) ViT-FOD: A Vision Transformer based Fine-grained Object Discriminator, [[Paper]](https://arxiv.org/pdf/2203.12816.pdf)
- (arXiv 2022.03) Surface Vision Transformers: Attention-Based Modelling applied to Cortical Analysis, [[Paper]](https://arxiv.org/pdf/2203.16414.pdf)
- (arXiv 2022.04) Event Transformer. A sparse-aware solution for efficient event data processing, [[Paper]](https://arxiv.org/pdf/2204.03355.pdf)
- (arXiv 2022.04) OutfitTransformer: Learning Outfit Representations for Fashion Recommendation, [[Paper]](https://arxiv.org/pdf/2204.04812.pdf)
- (arXiv 2022.04) Fashionformer: A simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition, [[Paper]](https://arxiv.org/pdf/2204.04654.pdf)
- (arXiv 2022.04) Event Transformer, [[Paper]](https://arxiv.org/pdf/2204.05172.pdf)
- (arXiv 2022.04) Residual Swin Transformer Channel Attention Network for Image Demosaicing, [[Paper]](https://arxiv.org/pdf/2204.07098.pdf)
- (arXiv 2022.04) Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer, [[Paper]](https://arxiv.org/pdf/2204.08680.pdf)
- (arXiv 2022.04) ClothFormer:Taming Video Virtual Try-on in All Module, [[Paper]](https://arxiv.org/pdf/2204.12151.pdf), [[Code]](https://cloth-former.github.io/)
- (arXiv 2022.04) Deeper Insights into ViTs Robustness towards Common Corruptions, [[Paper]](https://arxiv.org/pdf/2204.12143.pdf)
- (arXiv 2022.04) Self-Driving Car Steering Angle Prediction: Let Transformer Be a Car Again, [[Paper]](https://arxiv.org/pdf/2204.12748.pdf)
- (arXiv 2022.05) Where in the World is this Image? Transformer-based Geo-localization in the Wild, [[Paper]](https://arxiv.org/pdf/2204.13861.pdf)
- (arXiv 2022.05) Synthesized Speech Detection Using Convolutional Transformer-Based Spectrogram Analysis, [[Paper]](https://arxiv.org/pdf/2205.01800.pdf)
- (arXiv 2022.05) SwinVRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation, [[Paper]](https://arxiv.org/pdf/2205.13158.pdf)
- (arXiv 2022.05) SymFormer: End-to-end symbolic regression using transformer-based architecture, [[Paper]](https://arxiv.org/pdf/2205.15764.pdf)
- (arXiv 2022.05) Surface Analysis with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.15836.pdf)
- (arXiv 2022.06) Draft-and-Revise: Effective Image Generation with Contextual RQ-Transformer, [[Paper]](https://arxiv.org/pdf/2206.04452.pdf)
- (arXiv 2022.06) Graph-based Spatial Transformer with Memory Replay for Multi-future Pedestrian Trajectory Prediction, [[Paper]](https://arxiv.org/pdf/2206.05712.pdf), [[Code]](https://github.com/Jacobieee/ST-MR)
- (arXiv 2022.06) Learning to Estimate Shapley Values with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.05282.pdf)
- (arXiv 2022.06) CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer, [[Paper]](https://arxiv.org/pdf/2206.08883.pdf)
- (arXiv 2022.06) SpA-Former:Transformer image shadow detection and removal via spatial attention, [[Paper]](https://arxiv.org/pdf/2206.10910.pdf)
- (arXiv 2022.06) CRFormer: A Cross-Region Transformer for Shadow Removal, [[Paper]](https://arxiv.org/pdf/2207.01600.pdf)
- (arXiv 2022.07) Interaction Transformer for Human Reaction Generation, [[Paper]](https://arxiv.org/pdf/2207.01685.pdf)
- (arXiv 2022.07) FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification with Efficacy Domain Exploration, [[Paper]](https://arxiv.org/pdf/2207.01925.pdf)
- (arXiv 2022.07) MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing, [[Paper]](https://arxiv.org/pdf/2207.05621.pdf)
- (arXiv 2022.07) Earthformer: Exploring Space-Time Transformers for Earth System Forecasting, [[Paper]](https://arxiv.org/pdf/2207.05833.pdf)
- (arXiv 2022.07) Trans4Map: Revisiting Holistic Top-down Mapping from Egocentric Images to Allocentric Semantics with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.06205.pdf), [[Code]](https://github.com/jamycheung/Trans4Map)
- (arXiv 2022.07) DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation, [[Paper]](https://arxiv.org/pdf/2207.06124.pdf), [[Code]](https://github.com/Huage001/DynaST)
- (arXiv 2022.07) Imaging through the Atmosphere using Turbulence Mitigation Transformer, [[Paper]](https://arxiv.org/pdf/2207.06465.pdf), [[Project]](https://xg416.github.io/TMT/)
- (arXiv 2022.07) iColoriT: Towards Propagating Local Hint to the Right Region in Interactive Colorization by Leveraging Vision Transformer, [[Paper]](https://arxiv.org/pdf/2207.06831.pdf)
- (arXiv 2022.07) Learning Parallax Transformer Network for Stereo Image JPEG Artifacts Removal, [[Paper]](https://arxiv.org/pdf/2207.07335.pdf)
- (arXiv 2022.07) Explainable vision transformer enabled convolutional neural network for plant disease identification: PlantXViT, [[Paper]](https://arxiv.org/pdf/2207.07919.pdf)
- (arXiv 2022.07) Self-Distilled Vision Transformer for Domain Generalization, [[Paper]](https://arxiv.org/pdf/2207.12392.pdf), [[Code]](https://github.com/maryam089/SDViT)
- (arXiv 2022.07) Online Continual Learning with Contrastive Vision Transformer, [[Paper]](https://arxiv.org/pdf/2207.13516.pdf)
- (arXiv 2022.08) A Novel Transformer Network with Shifted Window Cross-Attention for Spatiotemporal Weather Forecasting, [[Paper]](https://arxiv.org/pdf/2208.01252.pdf)
- (arXiv 2022.08) Transformers as Meta-Learners for Implicit Neural Representations, [[Paper]](https://arxiv.org/pdf/2208.02801.pdf), [[Code]](https://github.com/yinboc/trans-inr)
- (arXiv 2022.08) LaTTe: Language Trajectory TransformEr, [[Paper]](https://arxiv.org/pdf/2208.02918.pdf), [[Code]](https://github.com/arthurfenderbucker/NL_trajectory_reshaper)
- (arXiv 2022.08) DALLE-URBAN: Capturing the urban design expertise of large text to image transformers, [[Paper]](https://arxiv.org/pdf/2208.04139.pdf), [[Code]](https://github.com/sachith500/DALLEURBAN)
- (arXiv 2022.08) A Vision Transformer-Based Approach to Bearing Fault Classification via Vibration Signals, [[Paper]](https://arxiv.org/pdf/2208.07070.pdf)
- (arXiv 2022.08) SnowFormer: Scale-aware Transformer via Context Interaction for Single Image Desnowing, [[Paper]](https://arxiv.org/pdf/2208.09703.pdf), [[Code]](https://github.com/Ephemeral182/SnowFormer)
- (arXiv 2022.08) Hierarchical Local-Global Transformer for Temporal Sentence Grounding, [[Paper]](https://arxiv.org/pdf/2208.14882.pdf)
- (arXiv 2022.08) SIM-Trans: Structure Information Modeling Transformer for Fine-grained Visual Categorization, [[Paper]](https://arxiv.org/pdf/2208.14607.pdf), [[Code]](https://github.com/PKU-ICST-MIPL/SIM-Trans_ACMMM2022)
- (arXiv 2022.09) Tzransformers are Sample Efficient World Models, [[Paper]](https://arxiv.org/pdf/2209.00588.pdf), [[Code]](https://github.com/eloialonso/iris)
- (arXiv 2022.09) Fusion of Satellite Images and Weather Data with Transformer Networks for Downy Mildew Disease Detection, [[Paper]](https://arxiv.org/pdf/2209.02797.pdf)
- (arXiv 2022.09) Visual Transformer for Soil Classification, [[Paper]](https://arxiv.org/pdf/2209.02950.pdf)
- (arXiv 2022.09) Transformer based Fingerprint Feature Extraction, [[Paper]](https://arxiv.org/pdf/2209.03846.pdf), [[Project]](https://saraansh1999.github.io/global-plus-local-fp-transformer)
- (arXiv 2022.09) Transformers and CNNs both Beat Humans on SBIR, [[Paper]](https://arxiv.org/pdf/2209.06629.pdf)
- (arXiv 2022.09) CenterLineDet: Road Lane CenterLine Graph Detection With Vehicle-Mounted Sensors by Transformer for High-definition Map Creation, [[Paper]](https://arxiv.org/pdf/2209.07734.pdf), [[Project]](https://tonyxuqaq.github.io/projects/CenterLineDet/)
- (arXiv 2022.09) SQ-Swin: a Pretrained Siamese Quadratic Swin Transformer for Lettuce Browning Prediction, [[Paper]](https://arxiv.org/pdf/2209.07683.pdf)
- (arXiv 2022.09) Integrative Feature and Cost Aggregation with Transformers for Dense Correspondence, [[Paper]](https://arxiv.org/pdf/2209.08742.pdf)
- (arXiv 2022.09) Colonoscopy Landmark Detection using Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.11304.pdf)
- (arXiv 2022.09) Transformer-Based Microbubble Localization, [[Paper]](https://arxiv.org/pdf/2209.11859.pdf)
- (arXiv 2022.10) Self-Distillation for Further Pre-training of Transformers, [[Paper]](https://arxiv.org/pdf/2210.02871.pdf)
- (arXiv 2022.10) Vision Transformer Based Model for Describing a Set of Images as a Story, [[Paper]](https://arxiv.org/pdf/2210.02762.pdf)
- (arXiv 2022.10) LMQFormer: A Laplace-Prior-Guided Mask Query Transformer for Lightweight Snow Removal, [[Paper]](https://arxiv.org/pdf/2210.04787.pdf)
- (arXiv 2022.10) Robustify Transformers with Robust Kernel Density Estimation, [[Paper]](https://arxiv.org/pdf/2210.05794.pdf)
- (arXiv 2022.10) Pretrained Transformers Do not Always Improve Robustness, [[Paper]](https://arxiv.org/pdf/2210.07663.pdf)
- (arXiv 2022.10) Machine-Learning Love: classifying the equation of state of neutron stars with Transformers, [[Paper]](https://arxiv.org/pdf/2210.08382.pdf)
- (arXiv 2022.10) High-Fidelity Visual Structural Inspections through Transformers and Learnable Resizers, [[Paper]](https://arxiv.org/pdf/2210.12175.pdf)
- (arXiv 2022.10) Minutiae-Guided Fingerprint Embeddings via Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.13994.pdf)
- (arXiv 2022.11) WITT: A Wireless Image Transmission Transformer for Semantic Communications, [[Paper]](https://arxiv.org/pdf/2211.00937.pdf), [[Code]](https://github.com/KeYang8/WITT)
- (arXiv 2022.11) PolyBuilding: Polygon Transformer for End-to-End Building Extraction, [[Paper]](https://arxiv.org/pdf/2211.01589.pdf)
- (arXiv 2022.11) HYDRA-HGR: A Hybrid Transformer-based Architecture for Fusion of Macroscopic and Microscopic Neural Drive Information, [[Paper]](https://arxiv.org/pdf/2211.02619.pdf)
- (arXiv 2022.11) A Simple Transformer-Based Model for Ego4D Natural Language Queries Challenge, [[Paper]](https://arxiv.org/pdf/2211.08704.pdf)
- (arXiv 2022.11) Demystify Self-Attention in Vision Transformers from a Semantic Perspective: Analysis and Application, [[Paper]](https://arxiv.org/pdf/2211.08543.pdf)
- (arXiv 2022.11) FlowLens: Seeing Beyond the FoV via Flow-guided Clip-Recurrent Transformer, [[Paper]](https://arxiv.org/pdf/2211.11293.pdf), [[Code]](https://github.com/MasterHow/FlowLens)
- (arXiv 2022.11) Event Transformer+. A multi-purpose solution for efficient event data processing, [[Paper]](https://arxiv.org/pdf/2211.12222.pdf)
- (arXiv 2022.11) A Dual-scale Lead-seperated Transformer With Lead-orthogonal Attention And Meta-information For Ecg Classification, [[Paper]](https://arxiv.org/pdf/2211.12777.pdf)
- (arXiv 2022.11) RGB no more: Minimally-decoded JPEG Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.16421.pdf)
- (arXiv 2022.11) Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing, [[Paper]](https://arxiv.org/pdf/2211.16499.pdf), [[Code]](https://counterfactualsimulation.github.io/)
- (arXiv 2022.11) ShaDocNet: Learning Spatial-Aware Tokens in Transformer for Document Shadow Removal, [[Paper]](https://arxiv.org/pdf/2211.16675.pdf)
- (arXiv 2022.12) Solving the Weather4cast Challenge via Visual Transformers for 3D Images, [[Paper]](https://arxiv.org/pdf/2212.02456.pdf)
- (arXiv 2022.12) Enabling and Accelerating Dynamic Vision Transformer Inference for Real-Time Applications, [[Paper]](https://arxiv.org/pdf/2212.02687.pdf)
- (arXiv 2022.12) Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer, [[Paper]](https://arxiv.org/pdf/2212.03434.pdf)
- (arXiv 2022.12) Examining the Difference Among Transformers and CNNs with Explanation Methods, [[Paper]](https://arxiv.org/pdf/2212.06872.pdf)
- (arXiv 2023.01) Robust Transformer with Locality Inductive Bias and Feature Normalization, [[Paper]](https://arxiv.org/pdf/2301.11553.pdf)
- (arXiv 2023.01) Continuous Spatiotemporal Transformers, [[Paper]](https://arxiv.org/pdf/2301.13338.pdf)
- (arXiv 2023.01) ShadowFormer: Global Context Helps Image Shadow Removal, [[Paper]](https://arxiv.org/pdf/2302.01650.pdf), [[Code]](https://github.com/GuoLanqing/ShadowFormer)
- (arXiv 2023.02) V1T: large-scale mouse V1 response prediction using a Vision Transformer, [[Paper]](https://arxiv.org/pdf/2302.03023.pdf)
- (arXiv 2023.02) LipFormer: Learning to Lipread Unseen Speakers based on Visual-Landmark Transformers, [[Paper]](https://arxiv.org/pdf/2302.03023.pdf)
- (arXiv 2023.02) Mitigating Bias in Visual Transformers via Targeted Alignment, [[Paper]](https://arxiv.org/pdf/2302.04358.pdf)
- (arXiv 2023.02) Scaling Vision Transformers to 22 Billion Parameters, [[Paper]](https://arxiv.org/pdf/2302.05442.pdf)
- (arXiv 2023.02) A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity, [[Paper]](https://arxiv.org/pdf/2302.06015.pdf)
- (arXiv 2023.02) ForceFormer: Exploring Social Force and Transformer for Pedestrian Trajectory Prediction, [[Paper]](https://arxiv.org/pdf/2302.07583.pdf)
- (arXiv 2023.02) CK-Transformer: Commonsense Knowledge Enhanced Transformers for Referring Expression Comprehension, [[Paper]](https://arxiv.org/pdf/2302.09027.pdf), [[Code]](https://github.com/FightingFighting/CK-Transformer)
- (arXiv 2023.02) TBFormer: Two-Branch Transformer for Image Forgery Localization, [[Paper]](https://arxiv.org/pdf/2302.13004.pdf)
- (arXiv 2023.02) Enhancing Classification with Hierarchical Scalable Query on Fusion Transformer, [[Paper]](https://arxiv.org/pdf/2302.14487.pdf)
- (arXiv 2023.03) BEL: A Bag Embedding Loss for Transformer enhances Multiple Instance Whole Slide Image Classification, [[Paper]](https://arxiv.org/pdf/2303.01377.pdf)
- (arXiv 2023.03) Estimating Extreme 3D Image Rotation with Transformer Cross-Attention, [[Paper]](https://arxiv.org/pdf/2303.02615.pdf), [[Code]](https://anonymous.4open.science/r/AttExtremeRotation-A467/)
- (arXiv 2023.03) Dual-path Adaptation from Image to Video Transformers, [[Paper]](https://arxiv.org/pdf/2303.09857.pdf), [[Code]](https://github.com/park-jungin/DualPath)
- (arXiv 2023.03) The Multiscale Surface Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.11909.pdf), [[Code]](https://github.com/metrics-lab/surface-vision-transformers)
- (arXiv 2023.03) Multiscale Attention via Wavelet Neural Operators for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.12398.pdf)
- (arXiv 2023.03) Learning Expressive Prompting With Residuals for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.15591.pdf)
- (arXiv 2023.03) VTAE: Variational Transformer Autoencoder with Manifolds Learning, [[Paper]](https://arxiv.org/pdf/2304.00948.pdf)
- (arXiv 2023.04) Towards an Effective and Efficient Transformer for Rain-by-snow Weather Removal, [[Paper]](https://arxiv.org/pdf/2304.02860.pdf), [[Code]](https://github.com/chdwyb/RSFormer)
- (arXiv 2023.04) TopNet: Transformer-based Object Placement Network for Image Compositing, [[Paper]](https://arxiv.org/pdf/2304.03372.pdf)
- (arXiv 2023.04) Hyper-Decision Transformer for Efficient Online Policy Adaptation, [[Paper]](https://arxiv.org/pdf/2304.08487.pdf), [[Code]](https://sites.google.com/view/hdtforiclr2023/home)
- (arXiv 2023.04) DarSwin: Distortion Aware Radial Swin Transformer, [[Paper]](https://arxiv.org/pdf/2304.09691.pdf)
- (arXiv 2023.04) DropDim: A Regularization Method for Transformer Networks, [[Paper]](https://arxiv.org/pdf/2304.10321.pdf)
- (arXiv 2023.04) Multipar-T: Multiparty-Transformer for Capturing Contingent Behaviors in Group Conversations, [[Paper]](https://arxiv.org/pdf/2304.12204.pdf)
- (arXiv 2023.04) Detection of Pavement Cracks by Deep Learning Models of Transformer and UNet, [[Paper]](https://arxiv.org/pdf/2304.12596.pdf)
- (arXiv 2023.04) Recurrent Transformer Encoders for Vision-based Estimation of Fatigue and Engagement in Cognitive Training Sessions, [[Paper]](https://arxiv.org/pdf/2304.12470.pdf)
- (arXiv 2023.04) Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning, [[Paper]](https://arxiv.org/pdf/2304.12520.pdf)
- (arXiv 2023.05) Unsupervised Mutual Transformer Learning for Multi-Gigapixel Whole Slide Image Classification, [[Paper]](https://arxiv.org/pdf/2305.02032.pdf)
- (arXiv 2023.05) HIINT: Historical, Intra- and Inter- personal Dynamics Modeling with Cross-person Memory Transformer, [[Paper]](https://arxiv.org/pdf/2305.12369.pdf)
- (arXiv 2023.05) Key-Value Transformer, [[Paper]](https://arxiv.org/pdf/2305.19129.pdf), [[Code]](https://github.com/aliborji/kv-transformer)
- (arXiv 2023.05) Solar Irradiance Anticipative Transformer, [[Paper]](https://arxiv.org/pdf/2305.18487.pdf)
- (arXiv 2023.05) Contextual Vision Transformers for Robust Representation Learning, [[Paper]](https://arxiv.org/pdf/2305.19402.pdf)
- (arXiv 2023.06) Affinity-based Attention in Self-supervised Transformers Predicts Dynamics of Object Grouping in Humans, [[Paper]](https://arxiv.org/pdf/2306.00294.pdf)
- (arXiv 2023.06) DAM-Net: Global Flood Detection from SAR Imagery Using Differential Attention Metric-Based Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.00704.pdf), [[Code]](https://github.com/Tamer-Saleh/S1GFlood-Detection)
- (arXiv 2023.06) A Universal Latent Fingerprint Enhancer Using Transformers, [[Paper]](https://arxiv.org/pdf/2306.00231.pdf)
- (arXiv 2023.06) BokehOrNot: Transforming Bokeh Effect with Image Transformer and Lens Metadata Embedding, [[Paper]](https://arxiv.org/pdf/2306.04032.pdf), [[Code]](https://github.com/indicator0/bokehornot)
- (arXiv 2023.06) Multi-level Multiple Instance Learning with Transformer for Whole Slide Image Classification, [[Paper]](https://arxiv.org/pdf/2306.05029.pdf), [[Code]](https://github.com/hustvl/MMIL-Transformer)
- (arXiv 2023.06) Sequence-to-Sequence Model with Transformer-based Attention Mechanism and Temporal Pooling for Non-Intrusive Load Monitoring, [[Paper]](https://arxiv.org/pdf/2306.05012.pdf)
- (arXiv 2023.06) Transferring Knowledge for Food Image Segmentation using Transformers and Convolutions, [[Paper]](https://arxiv.org/pdf/2306.09203.pdf)
- (arXiv 2023.06) Building Blocks for a Complex-Valued Transformer Architecture, [[Paper]](https://arxiv.org/pdf/2306.09827.pdf)
- (arXiv 2023.06) B-cos Alignment for Inherently Interpretable CNNs and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.10898.pdf), [[Code]](https://github.com/B-cos/B-cos-v2)
- (arXiv 2023.06) Minimalist and High-Quality Panoramic Imaging with PSF-aware Transformers, [[Paper]](https://arxiv.org/pdf/2306.12992.pdf)
- (arXiv 2023.06) Learning from Visual Observation via Offline Pretrained State-to-Go Transformer, [[Paper]](https://arxiv.org/pdf/2306.12860.pdf), [[Code]](https://sites.google.com/view/stgtransformer)
- (arXiv 2023.07) INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.03712.pdf)
- (arXiv 2023.07) TreeFormer: a Semi-Supervised Transformer-based Framework for Tree Counting from a Single High Resolution Image, [[Paper]](https://arxiv.org/pdf/2307.06118.pdf), [[Code]](https://github.com/HAAClassic/TreeFormer)
- (arXiv 2023.07) HEAL-SWIN: A Vision Transformer On The Sphere, [[Paper]](https://arxiv.org/pdf/2307.07313.pdf), [[Code]](https://github.com/JanEGerken/HEAL-SWIN)
- (arXiv 2023.07) S2R-ViT for Multi-Agent Cooperative Perception: Bridging the Gap from Simulation to Reality, [[Paper]](https://arxiv.org/pdf/2307.07935.pdf)
- (arXiv 2023.07) Entropy Transformer Networks: A Learning Approach via Tangent Bundle Data Manifold, [[Paper]](https://arxiv.org/pdf/2307.12517.pdf)
- (arXiv 2023.07) IML-ViT: Image Manipulation Localization by Vision Transformer, [[Paper]](https://arxiv.org/pdf/2307.14863.pdf), [[Code]](https://github.com/SunnyHaze/IML-ViT)
- (arXiv 2023.07) DocDeshadower: Frequency-aware Transformer for Document Shadow Removal, [[Paper]](https://arxiv.org/pdf/2307.15318.pdf)
- (arXiv 2023.08) Experts Weights Averaging: A New General Training Scheme for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.06093.pdf)
- (arXiv 2023.08) MapTRv2: An End-to-End Framework for Online Vectorized HD Map Construction, [[Paper]](https://arxiv.org/pdf/2308.05736.pdf), [[Code]](https://github.com/hustvl/MapTR)
- (arXiv 2023.08) A Robust Approach Towards Distinguishing Natural and Computer Generated Images using Multi-Colorspace fused and Enriched Vision Transformer, [[Paper]](https://arxiv.org/pdf/2308.07279.pdf)
- (arXiv 2023.08) A Lightweight Transformer for Faster and Robust EBSD Data Collection, [[Paper]](https://arxiv.org/pdf/2308.09693.pdf)
- (arXiv 2023.08) WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning, [[Paper]](https://arxiv.org/pdf/2308.10195.pdf)
- (arXiv 2023.08) SwinLSTM: Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM, [[Paper]](https://arxiv.org/pdf/2308.09891.pdf), [[Code]](https://github.com/SongTang-x/SwinLSTM)
- (arXiv 2023.08) SketchDreamer: Interactive Text-Augmented Creative Sketch Ideation, [[Paper]](https://arxiv.org/pdf/2308.14191.pdf), [[Code]](https://github.com/WinKawaks/SketchDreamer)
- (arXiv 2023.09) Fearless Luminance Adaptation: A Macro-Micro-Hierarchical Transformer for Exposure Correction, [[Paper]](https://arxiv.org/pdf/2309.00872.pdf)
- (arXiv 2023.09) DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions, [[Paper]](https://arxiv.org/pdf/2309.03576.pdf), [[Code]](https://github.com/Haochen-Wang409/DropPos)
- (arXiv 2023.09) CNN Injected Transformer for Image Exposure Correction, [[Paper]](https://arxiv.org/pdf/2309.04366.pdf), [[Code]](https://github.com/rebeccaeexu/CIT-EC/)
- (arXiv 2023.09) Effective Image Tampering Localization via Enhanced Transformer and Co-attention Fusion, [[Paper]](https://arxiv.org/pdf/2309.09306.pdf), [[Code]](https://github.com/multimediaFor/EITLNet)
- (arXiv 2023.09) Automatic Bat Call Classification using Transformer Networks, [[Paper]](https://arxiv.org/pdf/2309.11218.pdf)
- (arXiv 2023.09) Beyond Grids: Exploring Elastic Input Sampling for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2309.13353.pdf)
- (arXiv 2023.09) Improving Facade Parsing with Vision Transformers and Line Integration, [[Paper]](https://arxiv.org/pdf/2309.15523.pdf), [[Code]](https://github.com/wbw520/RTFP)
- (arXiv 2023.10) Exploring Fairness in Pre-trained Visual Transformer based Natural and GAN Generated Image Detection Systems and Understanding the Impact of Image Compression in Fairness, [[Paper]](https://arxiv.org/pdf/2310.12076.pdf)
- (arXiv 2023.10) Transformer-based nowcasting of radar composites from satellite images for severe weather, [[Paper]](https://arxiv.org/pdf/2310.19515.pdf)
- (arXiv 2023.10) Triplet Attention Transformer for Spatiotemporal Predictive Learning, [[Paper]](https://arxiv.org/pdf/2310.18698.pdf)
- (arXiv 2023.11) FATE: Feature-Agnostic Transformer-based Encoder for learning generalized embedding spaces in flow cytometry data, [[Paper]](https://arxiv.org/pdf/2311.03314.pdf), [[Code]](https://github.com/lisaweijler/FATE)
- (arXiv 2023.11) CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer, [[Paper]](https://arxiv.org/pdf/2311.06443.pdf)
- (arXiv 2023.11) Multi-View Spectrogram Transformer for Respiratory Sound Classification, [[Paper]](https://arxiv.org/pdf/2311.09655.pdf)
- (arXiv 2023.11) Inspecting Explainability of Transformer Models with Additional Statistical Information, [[Paper]](https://arxiv.org/pdf/2311.11378.pdf)
- (arXiv 2023.11) Disentangling Structure and Appearance in ViT Feature Space, [[Paper]](https://arxiv.org/pdf/2311.12193.pdf)
- (arXiv 2023.11) EWasteNet: A Two-Stream Data Efficient Image Transformer Approach for E-Waste Classification, [[Paper]](https://arxiv.org/pdf/2311.12823.pdf)
- (arXiv 2023.11) BenthIQ: a Transformer-Based Benthic Classification Model for Coral Restoration, [[Paper]](https://arxiv.org/pdf/2311.13661.pdf)
- (arXiv 2023.11) GeoViT: A Versatile Vision Transformer Architecture for Geospatial Image Analysis, [[Paper]](https://arxiv.org/pdf/2311.14301.pdf)
- (arXiv 2023.11) ChAda-ViT : Channel Adaptive Attention for Joint Representation Learning of Heterogeneous Microscopy Images, [[Paper]](https://arxiv.org/pdf/2311.15264.pdf)
- (arXiv 2023.11) Typhoon Intensity Prediction with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2311.16450.pdf), [[Code]](https://github.com/chen-huanxin/Tint)
- (arXiv 2023.11) Dual-Stream Attention Transformers for Sewer Defect Classification, [[Paper]](https://arxiv.org/pdf/2311.16145.pdf)
- (arXiv 2023.11) Transformer-empowered Multi-modal Item Embedding for Enhanced Image Search in E-Commerce, [[Paper]](https://arxiv.org/pdf/2311.17954.pdf)
- (arXiv 2023.12) Integrating Human Vision Perception in Vision Transformers for Classifying Waste Items, [[Paper]](https://arxiv.org/pdf/2312.12143.pdf)
- (arXiv 2023.12) ROI-Aware Multiscale Cross-Attention Vision Transformer for Pest Image Identification, [[Paper]](https://arxiv.org/pdf/2312.16914.pdf)
- (arXiv 2024.01) SPFormer: Enhancing Vision Transformer with Superpixel Representation, [[Paper]](https://arxiv.org/pdf/2401.02931.pdf)
- (arXiv 2024.01) DedustNet: A Frequency-dominated Swin Transformer-based Wavelet Network for Agricultural Dust Removal, [[Paper]](https://arxiv.org/pdf/2401.04750.pdf)
- (arXiv 2024.01) Surface Normal Estimation with Transformers, [[Paper]](https://arxiv.org/pdf/2401.05745.pdf)
- (arXiv 2024.02) Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention, [[Paper]](https://arxiv.org/pdf/2402.04563.pdf), [[Code]](https://github.com/chen-huanxin/Tint)
- (arXiv 2024.02) DMAT: A Dynamic Mask-Aware Transformer for Human De-occlusion, [[Paper]](https://arxiv.org/pdf/2402.04558.pdf)
- (arXiv 2024.02) GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring, [[Paper]](https://arxiv.org/pdf/2402.07164.pdf)
- (arXiv 2024.02) Sheet Music Transformer: End-To-End Optical Music Recognition Beyond Monophonic Transcription, [[Paper]](https://arxiv.org/pdf/2402.07596.pdf)
- (arXiv 2024.02) ViTaL: An Advanced Framework for Automated Plant Disease Identification in Leaf Images Using Vision Transformers and Linear Projection For Feature Reduction, [[Paper]](https://arxiv.org/pdf/2402.17424.pdf)
- (arXiv 2024.02) SFTformer: A Spatial-Frequency-Temporal Correlation-Decoupling Transformer for Radar Echo Extrapolation, [[Paper]](https://arxiv.org/pdf/2402.18044.pdf)
- (arXiv 2024.03) Pig aggression classification using CNN, Transformers and Recurrent Networks, [[Paper]](https://arxiv.org/pdf/2403.08528.pdf)
- (arXiv 2024.03) Masked Generative Story Transformer with Character Guidance and Caption Augmentation, [[Paper]](https://arxiv.org/pdf/2403.08502.pdf)
- (arXiv 2024.03) Fusion Transformer with Object Mask Guidance for Image Forgery Analysis, [[Paper]](https://arxiv.org/pdf/2403.12229.pdf)

## Contact & Feedback

If you have any suggestions about this project, feel free to contact me.

- [e-mail: yzhangcst[at]gmail.com]
