### Multi-task/modal
- (arXiv 2021.02) Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer, [[Paper]](https://arxiv.org/abs/2102.10772), [[Code]](https://mmf.sh/)
- (arXiv 2021.04) MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding, [[Paper]](https://arxiv.org/pdf/2104.12763.pdf), [[Code]](https://github.com/ashkamath/mdetr)
- (arXiv 2021.04) Multi-Modal Fusion Transformer for End-to-End Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2104.09224.pdf)
- (arXiv 2021.04) VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text, [[Paper]](https://arxiv.org/pdf/2104.11178.pdf)
- (arXiv 2021.04) Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning, [[Paper]](https://arxiv.org/abs/2104.03135)
- (arXiv 2021.06) Scene Transformer: A Unified Multi-task Model for Behavior Prediction and Planning, [[Paper]](https://arxiv.org/pdf/2106.08417.pdf)
- (arXiv 2021.06) Spatio-Temporal Multi-Task Learning Transformer for Joint Moving Object Detection and Segmentation, [[Paper]](https://arxiv.org/pdf/2106.11401.pdf)
- (arXiv 2021.06) A Transformer-based Cross-modal Fusion Model with Adversarial Training, [[Paper]](https://arxiv.org/pdf/2106.13033.pdf)
- (arXiv 2021.07) Attention Bottlenecks for Multimodal Fusion, [[Paper]](https://arxiv.org/pdf/2107.00135.pdf)
- (arXiv 2021.07) Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots, [[Paper]](https://arxiv.org/pdf/2107.00811.pdf)
- (arXiv 2021.07) Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions, [[Paper]](https://arxiv.org/pdf/2107.00789.pdf)
- (arXiv 2021.07) Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers, [[Paper]](https://arxiv.org/pdf/2107.03996.pdf), [[Code]](https://RchalYang.github.io/LocoTransformer)
- (arXiv 2021.08) StrucTexT: Structured Text Understanding with Multi-Modal Transformers, [[Paper]](https://arxiv.org/pdf/2108.02923.pdf)
- (arXiv 2021.08) Billion-Scale Pretraining with Vision Transformers for Multi-Task Visual Representations, [[Paper]](https://arxiv.org/pdf/2108.05887.pdf)
- (arXiv 2021.09) TxT: Crossmodal End-to-End Learning with Transformers, [[Paper]](https://arxiv.org/pdf/2109.04422.pdf)
- (arXiv 2021.09) Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2109.04448.pdf)
- (arXiv 2021.09) Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering, [[Paper]](https://arxiv.org/pdf/2109.04735.pdf)
- (arXiv 2021.09) On Pursuit of Designing Multi-modal Transformer for Video Grounding, [[Paper]](https://arxiv.org/pdf/2109.06085.pdf), [[Code]](https://sites.google.com/view/mengcao/publication/gtr)
- (arXiv 2021.09) Dyadformer: A Multi-modal Transformer for Long-Range Modeling of Dyadic Interactions, [[Paper]](https://arxiv.org/pdf/2109.09487.pdf)
- (arXiv 2021.09) KD-VLP: Improving End-to-End Vision-and-Language Pretraining with Object Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2109.10504.pdf)
- (arXiv 2021.10) Unifying Multimodal Transformer for Bi-directional Image and Text Generation, [[Paper]](https://arxiv.org/pdf/2110.09753.pdf), [[Code]](https://github.com/researchmm/generate-it)
- (arXiv 2021.10) VLDeformer: Learning Visual-Semantic Embeddings by Vision-Language Transformer Decomposing, [[Paper]](https://arxiv.org/pdf/2110.11338.pdf)
- (arXiv 2021.10) Detecting Dementia from Speech and Transcripts using Transformers, [[Paper]](https://arxiv.org/pdf/2110.14769.pdf)
- (arXiv 2021.11) MEmoBERT: Pre-training Model with Prompt-based Learning for Multimodal Emotion Recognition, [[Paper]](https://arxiv.org/pdf/2111.00865.pdf)
- (arXiv 2021.11) VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts, [[Paper]](https://arxiv.org/pdf/2111.02358.pdf), [[Code]](https://github.com/microsoft/unilm/tree/master/vlmo)
- (arXiv 2021.11) An Empirical Study of Training End-to-End Vision-and-Language Transformers, [[Paper]](https://arxiv.org/pdf/2111.02387.pdf), [[Code]](https://github.com/zdou0830/METER)
- (arXiv 2021.11) CLIP2TV: An Empirical Study on Transformer-based Methods for Video-Text Retrieval, [[Paper]](https://arxiv.org/pdf/2111.05610.pdf)
- (arXiv 2021.11) Graph Relation Transformer: Incorporating pairwise object features into the Transformer architecture, [[Paper]](https://arxiv.org/pdf/2111.06075.pdf), [[Code1]](https://github.com/michaelzyang/graph-relation-m4c), [[Code2]](https://github.com/derikclive/transformers)
- (arXiv 2021.11) UFO: A UniFied TransfOrmer for Vision-Language Representation Learning, [[Paper]](https://arxiv.org/pdf/2111.10023.pdf)
- (arXiv 2021.11) Multi-modal Transformers Excel at Class-agnostic Object Detection, [[Paper]](https://arxiv.org/pdf/2111.11430.pdf), [[Code]](https://git.io/J1HPY)
- (arXiv 2021.11) Sparse Fusion for Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2111.11992.pdf)
- (arXiv 2021.11) VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling, [[Paper]](https://arxiv.org/pdf/2111.12681.pdf), [[Code]](https://github.com/tsujuifu/pytorch_violet)
- (arXiv 2021.11) Cerberus Transformer: Joint Semantic, Affordance and Attribute Parsing, [[Paper]](https://arxiv.org/pdf/2111.12608.pdf), [[Code]](https://github.com/OPEN-AIR-SUN/Cerberus)
- (arXiv 2021.11) PolyViT: Co-training Vision Transformers on Images, Videos and Audio, [[Paper]](https://arxiv.org/pdf/2111.12993.pdf)
- (arXiv 2021.11) End-to-End Referring Video Object Segmentation with Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2111.14821.pdf), [[Code]](https://github.com/mttr2021/MTTR)
- (arXiv 2021.12) TransMEF: A Transformer-Based Multi-Exposure Image Fusion Framework using Self-Supervised Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2112.01030.pdf), [[Code]](https://github.com/miccaiif/TransMEF)
- (arXiv 2021.12) LMR-CBT: Learning Modality-fused Representations with CB-Transformer for Multimodal Emotion Recognition from Unaligned Multimodal Sequences, [[Paper]](https://arxiv.org/pdf/2112.01697.pdf)
- (arXiv 2021.12) Unified Multimodal Pre-training and Prompt-based Tuning for Vision-Language Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2112.05587.pdf)
- (arXiv 2021.12) VUT: Versatile UI Transformer for Multi-Modal Multi-Task User Interface Modeling, [[Paper]](https://arxiv.org/pdf/2112.05692.pdf)
- (arXiv 2021.12) VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks, [[Paper]](https://arxiv.org/pdf/2112.06825.pdf),[[Code]](https://github.com/ylsung/VL_adapter)
- (arXiv 2021.12) Towards a Unified Foundation Model: Jointly Pre-Training Transformers on Unpaired Images and Text, [[Paper]](https://arxiv.org/pdf/2112.07074.pdf)
- (arXiv 2021.12) Distilled Dual-Encoder Model for Vision-Language Understanding, [[Paper]](https://arxiv.org/pdf/2112.08723.pdf),[[Code]](https://github.com/kugwzk/Distilled-DualEncoder)
- (arXiv 2021.12) Multimodal Personality Recognition using Cross-Attention Transformer and Behaviour Encoding, [[Paper]](https://arxiv.org/pdf/2112.12180.pdf)
- (arXiv 2021.12) SLIP: Self-supervision meets Language-Image Pre-training, [[Paper]](https://arxiv.org/pdf/2112.12750.pdf),[[Code]](https://github.com/facebookresearch/SLIP)
- (arXiv 2021.12) Synchronized Audio-Visual Frames with Fractional Positional Encoding for Transformers in Video-to-Text Translation, [[Paper]](https://arxiv.org/pdf/2112.14088.pdf),[[Code]](https://github.com/fpe-vtt/ftt-vpe)
- (arXiv 2022.01) Robust Self-Supervised Audio-Visual Speech Recognition, [[Paper]](https://arxiv.org/pdf/2201.01763.pdf),[[Code]](https://github.com/facebookresearch/av_hubert)
- (arXiv 2022.01) Self-Training Vision Language BERTs with a Unified Conditional Model, [[Paper]](https://arxiv.org/pdf/2201.02010.pdf)
- (arXiv 2022.01) Uniformer: Unified Transformer for Efficient Spatiotemporal Representation Learning, [[Paper]](https://arxiv.org/pdf/2201.04676.pdf),[[Code]](https://github.com/Sense-X/UniFormer)
- (arXiv 2022.01) BridgeFormer: Bridging Video-text Retrieval with Multiple Choice Questions, [[Paper]](https://arxiv.org/pdf/2201.04850.pdf),[[Code]](https://geyuying.github.io/MCQ.html)
- (arXiv 2022.01) OMNIVORE: A Single Model for Many Visual Modalities, [[Paper]](https://arxiv.org/pdf/2201.08377.pdf),[[Code]](https://facebookresearch.github.io/omnivore/)
- (arXiv 2022.01) A Pre-trained Audio-Visual Transformer for Emotion Recognition, [[Paper]](https://arxiv.org/pdf/2201.09165.pdf)
- (arXiv 2022.01) Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition, [[Paper]](https://arxiv.org/pdf/2201.10439.pdf)
- (arXiv 2022.02) Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer, [[Paper]](https://arxiv.org/pdf/2202.05508.pdf)
- (arXiv 2022.03) DXM-TransFuse U-net: Dual Cross-Modal Transformer Fusion U-net for Automated Nerve Identification, [[Paper]](https://arxiv.org/pdf/2202.13304.pdf)
- (arXiv 2022.03) LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives, [[Paper]](https://arxiv.org/pdf/2203.01445.pdf)
- (arXiv 2022.03) VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer, [[Paper]](https://arxiv.org/pdf/2203.04099.pdf),[[Project]](https://ipcv.github.io/VoViT/)
- (arXiv 2022.03) MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization, [[Paper]](https://arxiv.org/pdf/2203.07086.pdf),[[Project]](https://ipcv.github.io/VoViT/)
- (arXiv 2022.03) Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation, [[Paper]](https://arxiv.org/pdf/2203.06386.pdf)
- (arXiv 2022.03) Inverted Pyramid Multi-task Transformer for Dense Scene Understanding, [[Paper]](https://arxiv.org/pdf/2203.07997.pdf)
- (arXiv 2022.03) UNIMO-2: End-to-End Unified Vision-Language Grounded Learning, [[Paper]](https://arxiv.org/pdf/2203.09067.pdf),[[Project]](https://unimo-ptm.github.io/)
- (arXiv 2022.03) Multi-Modal Learning for AU Detection Based on Multi-Head Fused Transformers, [[Paper]](https://arxiv.org/pdf/2203.11441.pdf)
- (arXiv 2022.03) UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection, [[Paper]](https://arxiv.org/pdf/2203.12745.pdf),[[Project]](https://github.com/TencentARC/UMT)
- (arXiv 2022.03) Multi-modal Multi-label Facial Action Unit Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2203.13301.pdf),[[Project]](https://github.com/TencentARC/UMT)
- (arXiv 2022.03) Multimodal Fusion Transformer for Remote Sensing Image Classification, [[Paper]](https://arxiv.org/pdf/2203.16952.pdf)
- (arXiv 2022.03) VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers, [[Paper]](https://arxiv.org/pdf/2203.17247.pdf)
- (arXiv 2022.04) MultiMAE: Multi-modal Multi-task Masked Autoencoders, [[Paper]](https://arxiv.org/pdf/2204.01678.pdf),[[Project]](https://multimae.epfl.ch/)
- (arXiv 2022.04) Multi-Task Distributed Learning using Vision Transformer with Random Patch Permutation, [[Paper]](https://arxiv.org/pdf/2204.03500.pdf)
- (arXiv 2022.04) MHMS: Multimodal Hierarchical Multimedia Summarization, [[Paper]](https://arxiv.org/pdf/2204.03734.pdf)
- (arXiv 2022.04) Multimodal Transformer for Nursing Activity Recognition, [[Paper]](https://arxiv.org/pdf/2204.04564.pdf)
- (arXiv 2022.04) Are Multimodal Transformers Robust to Missing Modality?, [[Paper]](https://arxiv.org/pdf/2204.05454.pdf)
- (arXiv 2022.04) X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks, [[Paper]](https://arxiv.org/pdf/2204.05626.pdf)
- (arXiv 2022.04) Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks, [[Paper]](https://arxiv.org/pdf/2204.07780.pdf)
- (arXiv 2022.04) Multimodal Token Fusion for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.08721.pdf)
- (arXiv 2022.04) Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval, [[Paper]](https://arxiv.org/pdf/2204.09730.pdf), [[Code]](https://github.com/mshukor/TFood)
- (arXiv 2022.04) ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer, [[Paper]](https://arxiv.org/pdf/2204.10777.pdf)
- (arXiv 2022.05) Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training, [[Paper]](https://arxiv.org/pdf/2205.04948.pdf)
- (arXiv 2022.05) MulT: An End-to-End Multitask Learning Transformer, [[Paper]](https://arxiv.org/pdf/2205.08303.pdf)), [[Project]](https://ivrl.github.io/MulT/)
- (arXiv 2022.05) Training Vision-Language Transformers from Captions Alone, [[Paper]](https://arxiv.org/pdf/2205.09256.pdf), [[Code]](https://github.com/guilk/VLC)
- (arXiv 2022.05) GIT: A Generative Image-to-text Transformer for Vision and Language, [[Paper]](https://arxiv.org/pdf/2205.14100.pdf)
- (arXiv 2022.05) Multi-Task Learning with Multi-query Transformer for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2205.14354.pdf)
- (arXiv 2022.06) VL-BEIT: Generative Vision-Language Pretraining, [[Paper]](https://arxiv.org/pdf/2206.01127.pdf), [[Code]](https://github.com/microsoft/unilm)
- (arXiv 2022.06) AntPivot: Livestream Highlight Detection via Hierarchical Attention Mechanism, [[Paper]](https://arxiv.org/pdf/2206.04888.pdf)
- (arXiv 2022.06) A Unified Sequence Interface for Vision Tasks, [[Paper]](https://arxiv.org/pdf/2206.07669.pdf)
- (arXiv 2022.06) Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos, [[Paper]](https://arxiv.org/pdf/2206.07981.pdf)
- (arXiv 2022.06) Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks, [[Paper]](https://arxiv.org/pdf/2206.08916.pdf)
- (arXiv 2022.06) M&M Mix: A Multimodal Multiview Transformer Ensemble, [[Paper]](https://arxiv.org/pdf/2206.09852.pdf)
- (arXiv 2022.06) RoME: Role-aware Mixture-of-Expert Transformer for Text-to-Video Retrieval, [[Paper]](https://arxiv.org/pdf/2206.12845.pdf)
- (arXiv 2022.07) You Only Need One Detector: Unified Object Detector for Different Modalities based on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.01071.pdf), [[Code]](https://github.com/liketheflower/YONOD.git)
- (arXiv 2022.07) Open-Vocabulary Multi-Label Classification via Multi-modal Knowledge Transfer, [[Paper]](https://arxiv.org/pdf/2207.01887.pdf), [[Code]](https://github.com/seanhe97/MKT)
- (arXiv 2022.07) Audio鈭扸isual Segmentation, [[Paper]](https://arxiv.org/pdf/2207.05042.pdf), [[Code]](https://github.com/OpenNLPLab/AVSBench)
- (arXiv 2022.07) FashionViL: Fashion-Focused Vision-and-Language Representation Learning, [[Paper]](https://arxiv.org/pdf/2207.08150.pdf), [[Code]](https://github.com/BrandonHanx/mmf)
- (arXiv 2022.07) Multimodal Transformer for Automatic 3D Annotation and Object Detection, [[Paper]](https://arxiv.org/pdf/2207.09805.pdf), [[Code]](https://github.com/Cliu2/MTrans)
- (arXiv 2022.07) UFO: Unified Feature Optimization, [[Paper]](https://arxiv.org/pdf/2207.10341.pdf), [[Code]](https://github.com/PaddlePaddle/VIMER/tree/main/UFO)
- (arXiv 2022.07) An Ensemble Approach for Multiple Emotion Descriptors Estimation Using Multi-task Learning, [[Paper]](https://arxiv.org/pdf/2207.10878.pdf)
- (arXiv 2022.07) STrajNet: Occupancy Flow Prediction via Multi-modal Swin Transformer, [[Paper]](https://arxiv.org/pdf/2208.00394.pdf)
- (arXiv 2022.08) Multi-Task Transformer with uncertainty modelling for Face Based Affective Computing, [[Paper]](https://arxiv.org/pdf/2208.03506.pdf)
- (arXiv 2022.08) Multi-modal Transformer Path Prediction for Autonomous Vehicle, [[Paper]](https://arxiv.org/pdf/2208.07256.pdf)
- (arXiv 2022.08) Efficient Multimodal Transformer with Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis, [[Paper]](https://arxiv.org/pdf/2208.07589.pdf)
- (arXiv 2022.08) VAuLT: Augmenting the Vision-and-Language Transformer with the Propagation of Deep Language Representations, [[Paper]](https://arxiv.org/pdf/2208.09021.pdf), [[Code]](https://github.com/gchochla/VAuLT)
- (arXiv 2022.08) Flat Multi-modal Interaction Transformer for Named Entity Recognition, [[Paper]](https://arxiv.org/pdf/2208.11039.pdf), [[Code]](https://github.com/gchochla/VAuLT)
- (arXiv 2022.08) TFusion: Transformer based N-to-One Multimodal Fusion Block, [[Paper]](https://arxiv.org/pdf/2208.12776.pdf)
- (arXiv 2022.09) Multi-task Swin Transformer for Motion Artifacts Classification and Cardiac Magnetic Resonance Image Segmentation, [[Paper]](https://arxiv.org/pdf/2209.02470.pdf)
- (arXiv 2022.09) TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction, [[Paper]](https://arxiv.org/pdf/2209.05036.pdf), [[Code]](https://t.ly/V-_W)
- (arXiv 2022.09) Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer, [[Paper]](https://arxiv.org/pdf/2209.07026.pdf), [[Code]](https://github.com/Reimilia/Simple3D-Former)
- (arXiv 2022.09) UniColor: A Unified Framework for Multi-Modal Colorization with Transformer, [[Paper]](https://arxiv.org/pdf/2209.11223.pdf), [[Code]](https://luckyhzt.github.io/unicolor)
- (arXiv 2022.09) TVLT: Textless Vision-Language Transformer, [[Paper]](https://arxiv.org/pdf/2209.14156.pdf), [[Code]](https://github.com/zinengtang/TVLT)
- (arXiv 2022.10) A Strong Transfer Baseline for RGB-D Fusion in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.00843.pdf)
- (arXiv 2022.10) Cascaded Multi-Modal Mixing Transformers for Alzheimer's Disease Classification with Incomplete Data, [[Paper]](https://arxiv.org/pdf/2210.00255.pdf)
- (arXiv 2022.10) VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment, [[Paper]](https://arxiv.org/pdf/2210.04135.pdf)
- (arXiv 2022.10) Transformer-based Localization from Embodied Dialog with Large-scale Pre-training, [[Paper]](https://arxiv.org/pdf/2210.04864.pdf)
- (arXiv 2022.10) AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization, [[Paper]](https://arxiv.org/pdf/2210.05060.pdf)
- (arXiv 2022.10) Understanding Embodied Reference with Touch-Line Transformer, [[Paper]](https://arxiv.org/pdf/2210.05668.pdf)
- (arXiv 2022.10) Foundation Transformers, [[Paper]](https://arxiv.org/pdf/2210.06423.pdf)
- (arXiv 2022.10) PedFormer: Pedestrian Behavior Prediction via Cross-Modal Attention Modulation and Gated Multitask Learning, [[Paper]](https://arxiv.org/pdf/2210.07886.pdf)
- (arXiv 2022.10) Multimodal Image Fusion based on Hybrid CNN-Transformer and Non-local Cross-modal Attention, [[Paper]](https://arxiv.org/pdf/2210.09847.pdf), [[Code]](https://github.com/pandayuanyu/HCFusion)
- (arXiv 2022.10) Multi-Source Transformer Architectures for Audiovisual Scene Classification, [[Paper]](https://arxiv.org/pdf/2210.10212.pdf)
- (arXiv 2022.10) Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies, [[Paper]](https://arxiv.org/pdf/2210.12079.pdf)
- (arXiv 2022.10) M3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design, [[Paper]](https://arxiv.org/pdf/2210.14793.pdf), [[Code]](https://github.com/VITA-Group/M3ViT)
- (arXiv 2022.10) TAMFormer: Multi-Modal Transformer with Learned Attention Mask for Early Intent Prediction, [[Paper]](https://arxiv.org/pdf/2210.14714.pdf), [[Code]](https://github.com/VITA-Group/M3ViT)
- (arXiv 2022.10) Multimodal Transformer Distillation for Audio-Visual Synchronization, [[Paper]](https://arxiv.org/pdf/2210.15563.pdf)
- (arXiv 2022.10) Masked Vision-Language Transformer in Fashion, [[Paper]](https://arxiv.org/pdf/2210.15110.pdf), [[Code]](https://github.com/GewelsJI/MVLT)
- (arXiv 2022.10) Multimodal Transformer for Parallel Concatenated Variational Autoencoders, [[Paper]](https://arxiv.org/pdf/2210.16174.pdf)
- (arXiv 2022.10) RCDPT: Radar-Camera fusion Dense Prediction Transformer, [[Paper]](https://arxiv.org/pdf/2211.02432.pdf)
- (arXiv 2022.11) Efficient Joint Detection and Multiple Object Tracking with Spatially Aware Transformer, [[Paper]](https://arxiv.org/pdf/2211.05654.pdf)
- (arXiv 2022.11) OneFormer: One Transformer to Rule Universal Image Segmentation, [[Paper]](https://arxiv.org/pdf/2211.06220.pdf), [[Code]](https://github.com/SHI-Labs/OneFormer)
- (arXiv 2022.11) TransCC: Transformer-based Multiple Illuminant Color Constancy Using Multitask Learning, [[Paper]](https://arxiv.org/pdf/2211.08772.pdf)
- (arXiv 2022.11) Unifying Vision-Language Representation Space with Single-tower Transformer, [[Paper]](https://arxiv.org/pdf/2211.11153.pdf)
- (arXiv 2022.11) Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion, [[Paper]](https://arxiv.org/pdf/2205.02357.pdf), [[Code]](https://github.com/zjunlp/MKGformer)
- (arXiv 2022.12) Multimodal Vision Transformers with Forced Attention for Behavior Analysis, [[Paper]](https://arxiv.org/pdf/2212.03968.pdf)
- (arXiv 2022.12) Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers, [[Paper]](https://arxiv.org/pdf/2212.04970.pdf), [[Code]](https://hangz-nju-cuhk.github.io/projects/AV-CAT)
- (arXiv 2022.12) Hierarchical multimodal transformers for Multi-Page DocVQA, [[Paper]](https://arxiv.org/pdf/2212.05935.pdf)
- (arXiv 2022.12) Vision Transformers are Parameter-Efficient Audio-Visual Learners, [[Paper]](https://arxiv.org/pdf/2212.07983.pdf), [[Code]](https://genjib.github.io/project_page/LAVISH/)
- (arXiv 2022.12) Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program, [[Paper]](https://arxiv.org/pdf/2212.12952.pdf)
- (arXiv 2023.01) Cross Modal Transformer via Coordinates Encoding for 3D Object Dectection, [[Paper]](https://arxiv.org/pdf/2301.01283.pdf), [[Code]](https://github.com/junjie18/CMT)
- (arXiv 2023.01) DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction, [[Paper]](https://arxiv.org/pdf/2301.03461.pdf), [[Code]](https://github.com/yangyangxu0/DeMT)
- (arXiv 2023.01) Multi-scale multi-modal micro-expression recognition algorithm based on transformer, [[Paper]](https://arxiv.org/pdf/2301.02969.pdf)
- (arXiv 2023.01) Logically at Factify 2023: A Multi-Modal Fact Checking System Based on Evidence Retrieval techniques and Transformer Encoder Architecture, [[Paper]](https://arxiv.org/pdf/2301.03127.pdf)
- (arXiv 2023.01) ViTs for SITS: Vision Transformers for Satellite Image ries, [[Paper]](https://arxiv.org/pdf/2301.04944.pdf)
- (arXiv 2023.01) Zorro: the masked multimodal transformer, [[Paper]](https://arxiv.org/pdf/2301.09595.pdf)
- (arXiv 2023.01) Multimodal Event Transformer for Image-guided Story Ending Generation, [[Paper]](https://arxiv.org/pdf/2301.11357.pdf)
- (arXiv 2023.01) UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers, [[Paper]](https://arxiv.org/pdf/2301.13741.pdf)
- (arXiv 2023.02) Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing, [[Paper]](https://arxiv.org/pdf/2302.05744.pdf)
- (arXiv 2023.02) ViM: Vision Middleware for Unified Downstream Transferring, [[Paper]](https://arxiv.org/pdf/2303.06911.pdf)
- (arXiv 2023.03) One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale, [[Paper]](https://arxiv.org/pdf/2303.06555.pdf), [[Code]](https://github.com/thu-ml/unidiffuser)
- (arXiv 2023.03) MAGVLT: Masked Generative Vision-and-Language Transformer, [[Paper]](https://arxiv.org/pdf/2303.12208.pdf)
- (arXiv 2023.03) LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR Perception, [[Paper]](https://arxiv.org/pdf/2303.12194.pdf)
- (arXiv 2023.03) MMFormer: Multimodal Transformer Using Multiscale Self-Attention for Remote Sensing Image Classification, [[Paper]](https://arxiv.org/pdf/2303.13101.pdf)
- (arXiv 2023.04) Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification, [[Paper]](https://arxiv.org/pdf/2304.02836.pdf)
- (arXiv 2023.04) PARFormer: Transformer-based Multi-Task Network for Pedestrian Attribute Recognition, [[Paper]](https://arxiv.org/pdf/2304.07230.pdf), [[Code]](https://github.com/xwf199/PARFormer)
- (arXiv 2023.04) AutoTaskFormer: Searching Vision Transformers for Multi-task Learning, [[Paper]](https://arxiv.org/pdf/2304.08756.pdf)
- (arXiv 2023.05) MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer, [[Paper]](https://arxiv.org/pdf/2305.00355.pdf), [[Code]](https://github.com/YoucanBaby/MH-DETR)
- (arXiv 2023.05) MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis, [[Paper]](https://arxiv.org/pdf/2305.05992.pdf), [[Project]](https://jabir-zheng.github.io/MMoT)
- (arXiv 2023.05) JOINEDTrans: Prior Guided Multi-task Transformer for Joint Optic Disc/Cup Segmentation and Fovea Detection, [[Paper]](https://arxiv.org/pdf/2305.11504.pdf), [[Project]](https://github.com/HuaqingHe/JOINEDTrans)
- (arXiv 2023.05) Brain encoding models based on multimodal transformers can transfer across language and vision, [[Paper]](https://arxiv.org/pdf/2305.12248.pdf)
- (arXiv 2023.05) CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers, [[Paper]](https://arxiv.org/pdf/2305.17455.pdf), [[Code]](https://github.com/sdc17/CrossGET)
- (arXiv 2023.05) Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts, [[Paper]](https://arxiv.org/pdf/2305.18691.pdf), [[Code]](https://github.com/sharc-lab/Edge-MoE)
- (arXiv 2023.06) Transformer-based Multi-Modal Learning for Multi Label Remote Sensing Image Classification, [[Paper]](https://arxiv.org/pdf/2306.01523.pdf), [[Code]](https://git.tu-berlin.de/rsim/sct-fusion)
- (arXiv 2023.06) Energy-Based Models for Cross-Modal Localization using Convolutional Transformers, [[Paper]](https://arxiv.org/pdf/2306.04021.pdf)
- (arXiv 2023.06) Efficient Multi-Task Scene Analysis with RGB-D Transformers, [[Paper]](https://arxiv.org/pdf/2306.05242.pdf), [[Code]](https://github.com/TUI-NICR/EMSAFormer)
- (arXiv 2023.06) ContentCTR: Frame-level Live Streaming Click-Through Rate Prediction with Multimodal Transformer, [[Paper]](https://arxiv.org/pdf/2306.14392.pdf)
- (arXiv 2023.07) End-To-End Prediction of Knee Osteoarthritis Progression With Multi-Modal Transformers, [[Paper]](https://arxiv.org/pdf/2307.00873.pdf)
- (arXiv 2023.07) Interactive Image Segmentation with Cross-Modality Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.02280.pdf), [[Code]](https://github.com/lik1996/iCMFormer)
- (arXiv 2023.07) TransNuSeg: A Lightweight Multi-Task Transformer for Nuclei Segmentation, [[Paper]](https://arxiv.org/pdf/2307.08051.pdf), [[Code]](https://github.com/zhenqi-he/transnuseg)
- (arXiv 2023.07) Meta-Transformer: A Unified Framework for Multimodal Learning, [[Paper]](https://arxiv.org/pdf/2307.10802.pdf), [[Project]](https://kxgong.github.io/meta_transformer/)
- (arXiv 2023.07) ComPtr: Towards Diverse Bi-source Dense Prediction Tasks via A Simple yet General Complementary Transformer, [[Paper]](https://arxiv.org/pdf/2307.12349.pdf), [[Code]](https://github.com/lartpang/ComPtr)
- (arXiv 2023.07) Audio-aware Query-enhanced Transformer for Audio-Visual Segmentation, [[Paper]](https://arxiv.org/pdf/2307.13236.pdf)
- (arXiv 2023.07) Prompt Guided Transformer for Multi-Task Dense Prediction, [[Paper]](https://arxiv.org/pdf/2307.15362.pdf)
- (arXiv 2023.07) Audio-Visual Segmentation by Exploring Cross-Modal Mutual Semantics, [[Paper]](https://arxiv.org/pdf/2307.16005)
- (arXiv 2023.08) FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2308.01006.pdf)
- (arXiv 2023.08) Multimodal Neurons in Pretrained Text-Only Transformers, [[Paper]](https://arxiv.org/pdf/2308.01544.pdf), [[Project]](https://mmns.csail.mit.edu/)
- (arXiv 2023.08) A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models, [[Paper]](https://arxiv.org/pdf/2308.01328.pdf)
- (arXiv 2023.08) 3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment, [[Paper]](https://arxiv.org/pdf/2308.04352.pdf), [[Project]](https://3d-vista.github.io/)
- (arXiv 2023.08) Vision Transformer Adapters for Generalizable Multitask Learning, [[Paper]](https://arxiv.org/pdf/2308.12372.pdf), [[Code]](https://ivrl.github.io/VTAGML)
- (arXiv 2023.08) UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization, [[Paper]](https://arxiv.org/pdf/2308.14395.pdf), [[Code]](https://github.com/ymhzyj/UMMAFormer/)
- (arXiv 2023.09) Exchanging-based Multimodal Fusion with Transformer, [[Paper]](https://arxiv.org/pdf/2309.02190.pdf), [[Code]](https://github.com/RecklessRonan/MuSE)
- (arXiv 2023.09) Multimodal Transformer for Material Segmentation, [[Paper]](https://arxiv.org/pdf/2309.04001.pdf), [[Code]](https://github.com/csiplab/MMSFormer)
- (arXiv 2023.09) Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens, [[Paper]](https://arxiv.org/pdf/2309.08531.pdf)
- (arXiv 2023.09) MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer, [[Paper]](https://arxiv.org/pdf/2309.09067.pdf), [[Code]](https://github.com/fudong03/MMST-ViT)
- (arXiv 2023.09) Unified Frequency-Assisted Transformer Framework for Detecting and Grounding Multi-Modal Manipulation, [[Paper]](https://arxiv.org/pdf/2309.09667.pdf)
- (arXiv 2023.09) Tile Classification Based Viewport Prediction with Multi-modal Fusion Transformer, [[Paper]](https://arxiv.org/pdf/2309.14704.pdf)
- (arXiv 2023.10) LeTFuser: Light-weight End-to-end Transformer-Based Sensor Fusion for Autonomous Driving with Multi-Task Learning, [[Paper]](https://arxiv.org/pdf/2310.13135.pdf), [[Code]](https://github.com/pagand/e2etransfuser/tree/cvpr-w)
- (arXiv 2023.10) 3M-TRANSFORMER: A Multi-Stage Multi-Stream Multimodal Transformer for Embodied Turn-Taking Prediction, [[Paper]](https://arxiv.org/pdf/2310.14859.pdf)
- (arXiv 2023.10) MMTF-DES: A Fusion of Multimodal Transformer Models for Desire, Emotion, and Sentiment Analysis of Social Media Data, [[Paper]](https://arxiv.org/pdf/2310.14143.pdf)
- (arXiv 2023.11) Learning A Multi-Task Transformer Via Unified And Customized Instruction Tuning For Chest Radiograph Interpretation, [[Paper]](https://arxiv.org/pdf/2311.01092.pdf)
- (arXiv 2023.11) Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization, [[Paper]](https://arxiv.org/pdf/2311.03785.pdf)
- (arXiv 2023.11) PolyMaX: General Dense Prediction with Mask Transformer, [[Paper]](https://arxiv.org/abs/2311.05770)
- (arXiv 2023.11) Vision-Language Integration in Multimodal Video Transformers (Partially) Aligns with the Brain, [[Paper]](https://arxiv.org/pdf/2311.07766.pdf)
- (arXiv 2023.11) Language Grounded QFormer for Efficient Vision Language Understanding, [[Paper]](https://arxiv.org/pdf/2311.07449.pdf)
- (arXiv 2023.11) DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models,  [[Paper]](https://arxiv.org/pdf/2311.08623.pdf)
- (arXiv 2023.11) VIT-LENS-2: Gateway to Omni-modal Intelligence,  [[Paper]](https://arxiv.org/pdf/2311.16081.pdf), [[Code]](https://github.com/TencentARC/ViT-Lens锛?
- (arXiv 2023.11) You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception,  [[Paper]](https://arxiv.org/pdf/2312.05525.pdf)
- (arXiv 2023.12) VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation,  [[Paper]](https://arxiv.org/pdf/2312.09251.pdf)
- (arXiv 2024.01) Multimodal Informative ViT: Information Aggregation and Distribution for Hyperspectral and LiDAR Classification,  [[Paper]](https://arxiv.org/pdf/2401.03179.pdf), [[Code]](https://github.com/icey-zhang/MIViT)
- (arXiv 2024.01) SeTformer is What You Need for Vision and Language,  [[Paper]](https://arxiv.org/pdf/2401.03540.pdf)
- (arXiv 2024.01) Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities,  [[Paper]](https://arxiv.org/pdf/2401.14405.pdf), [[Code]](https://github.com/AILab-CVC/M2PT)
- (arXiv 2024.01) Computation and Parameter Efficient Multi-Modal Fusion Transformer for Cued Speech Recognition,  [[Paper]](https://arxiv.org/pdf/2401.17604.pdf)
- (arXiv 2024.02) Question Aware Vision Transformer for Multimodal Reasoning,  [[Paper]](https://arxiv.org/pdf/2402.05472.pdf)
- (arXiv 2024.02) A Touch, Vision, and Language Dataset for Multimodal Alignment,  [[Paper]](https://arxiv.org/pdf/2402.13232.pdf), [[Code]](https://tactile-vlm.github.io/)
- (arXiv 2024.02) Multimodal Transformer With a Low-Computational-Cost Guarantee,  [[Paper]](https://arxiv.org/pdf/2402.15096.pdf)
- (arXiv 2024.03) Task Indicating Transformer for Task-conditional Dense Predictions,  [[Paper]](https://arxiv.org/pdf/2403.00327.pdf)
- (arXiv 2024.03) Multimodal Transformer for Comics Text-Cloze, [[Paper]](https://arxiv.org/pdf/2403.03719.pdf)
- (arXiv 2024.03) MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer, [[Paper]](https://arxiv.org/pdf/2403.02991.pdf), [[Code]](https://github.com/double125/MADTP)
- (arXiv 2024.03) GiT: Towards Generalist Vision Transformer through Universal Language Interface, [[Paper]](https://arxiv.org/pdf/2403.09394.pdf), [[Code]](https://github.com/Haiyang-W/GiT)
- (arXiv 2024.03) Uni-SMART: Universal Science Multimodal Analysis and Research Transformer, [[Paper]](https://arxiv.org/pdf/2403.10301.pdf)
- (arXiv 2024.03) Affective Behaviour Analysis via Integrating Multi-Modal Knowledge, [[Paper]](https://arxiv.org/pdf/2403.10825.pdf)
- (arXiv 2024.03) M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2403.12552.pdf), [[Code]](https://anonymous.4open.science/r/M2DA-4772/)
- (arXiv 2024.04) ViTamin: Designing Scalable Vision Models in the Vision-Language Era, [[Paper]](https://arxiv.org/pdf/2404.02132.pdf), [[Code]](https://github.com/Beckschen/ViTamin)
- (arXiv 2024.04) GLID: Pre-training a Generalist Encoder-Decoder Vision Model, [[Paper]](https://arxiv.org/pdf/2404.07603.pdf)
- (arXiv 2024.04) The revenge of BiSeNet: Efficient Multi-Task Image Segmentation, [[Paper]](https://arxiv.org/pdf/2404.09570.pdf)
- (arXiv 2024.04) HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild, [[Paper]](https://arxiv.org/pdf/2404.13819.pdf), [[Code]](https://supreethn.github.io/research/hoistformer/index.html)
- (arXiv 2024.04) UniRGB-IR: A Unified Framework for Visible-Infrared Downstream Tasks via Adapter Tuning, [[Paper]](https://arxiv.org/pdf/2404.17360.pdf), [[Code]](https://github.com/PoTsui99/UniRGB-IR.git)
- (arXiv 2024.05) Similarity Guided Multimodal Fusion Transformer for Semantic Location Prediction in Social Media, [[Paper]](https://arxiv.org/pdf/2405.05760.pdf)
- (arXiv 2024.05) CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers, [[Paper]](https://arxiv.org/pdf/2405.13195.pdf)
- (arXiv 2024.06) GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2406.01210.pdf), [[Code]](https://github.com/JiaDingCN/GeminiFusion)
- (arXiv 2024.07) DaBiT: Depth and Blur informed Transformer for Joint Refocusing and Super-Resolution, [[Paper]](https://arxiv.org/pdf/2407.01230.pdf), [[Code]](https://github.com/crispianm/DaBiT)
- (arXiv 2024.07) Instruct-IPT: All-in-One Image Processing Transformer via Weight Modulation, [[Paper]](https://arxiv.org/pdf/2407.00676.pdf), [[Code]](https://github.com/huawei-noah/Pretrained-IPT)
- (arXiv 2024.07) CorMulT: A Semi-supervised Modality Correlation-aware Multimodal Transformer for Sentiment Analysis, [[Paper]](https://arxiv.org/pdf/2407.07046.pdf)
- (arXiv 2024.07) A Multimodal Transformer for Live Streaming Highlight Prediction, [[Paper]](https://arxiv.org/pdf/2407.12002.pdf)
- (arXiv 2024.07) Hierarchical Multi-modal Transformer for Cross-modal Long Document Classification, [[Paper]](https://arxiv.org/pdf/2407.10105.pdf)
- (arXiv 2024.08) Algorithm Research of ELMo Word Embedding and Deep Learning Multimodal Transformer in Image Description, [[Paper]](https://arxiv.org/pdf/2408.06357)
- (arXiv 2024.08) Show-o: One Single Transformer to Unify Multimodal Understanding and Generation, [[Paper]](https://arxiv.org/pdf/2408.12528), [[Code]](https://github.com/showlab/Show-o)
- (arXiv 2024.08) Enhanced Parking Perception by Multi-Task Fisheye Cross-view Transformers, [[Paper]](https://arxiv.org/pdf/2408.12575)
- (arXiv 2024.08) SITransformer: Shared Information-Guided Transformer for Extreme Multimodal Summarization, [[Paper]](https://arxiv.org/pdf/2408.15829), [[Code]](https://github.com/SichengLeoLiu/MMAsia24-XMSMO)
- (arXiv 2024.09) Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy, [[Paper]](https://arxiv.org/pdf/2409.09221)
- (arXiv 2024.09) Genetic Information Analysis of Age-Related Macular Degeneration Fellow Eye Using Multi-Modal Selective ViT, [[Paper]](https://arxiv.org/pdf/2409.11128)
- (arXiv 2024.11) UIFormer: A Unified Transformer-based Framework for Incremental Few-Shot Object Detection and Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2411.08569)
- (arXiv 2024.11) CMAViT: Integrating Climate, Managment, and Remote Sensing Data for Crop Yield Estimation with Multimodel Vision Transformers, [[Paper]](https://arxiv.org/pdf/2411.16989), [[Code]](https://github.com/plant-ai-biophysics-lab/CMAViT)
