### Video
- (ECCV'20) Multi-modal Transformer for Video Retrieval, [[Paper]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490205.pdf)
- (ICLR'21) Support-set bottlenecks for video-text representation learning, [[Paper]](https://arxiv.org/pdf/2010.02824.pdf)
- (arXiv 2021.01) SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2101.08833.pdf)
- (arXiv 2021.02) Video Transformer Network, [[Paper]](https://arxiv.org/pdf/2102.00719.pdf)
- (arXiv 2021.02) Is Space-Time Attention All You Need for Video Understanding? [[Paper]](https://arxiv.org/pdf/2102.05095.pdf), [[Code]](https://github.com/lucidrains/TimeSformer-pytorch)
- (arXiv.2021.02) A Straightforward Framework For Video Retrieval Using CLIP, [[Paper]](https://arxiv.org/pdf/2102.12443.pdf), [[Code]](https://github.com/Deferf/CLIP_Video_Representation)
- (arXiv 2021.03) Space-Time Crop & Attend: Improving Cross-modal Video Representation Learning, [[Paper]](https://arxiv.org/pdf/2103.10211.pdf)
- (arXiv 2021.03) Enhancing Transformer for Video Understanding Using Gated Multi-Level Attention and Temporal Adversarial Training, [[Paper]](https://arxiv.org/pdf/2103.10043.pdf)
- (arXiv 2021.03) MDMMT: Multidomain Multimodal Transformer for Video Retrieval, [[Paper]](https://arxiv.org/pdf/2103.10699.pdf)
- (arXiv 2021.03) An Image is Worth 16x16 Words, What is a Video Worth? [[Paper]](https://arxiv.org/pdf/2103.13915.pdf)
- (arXiv 2021.03) ViViT: A Video Vision Transformer, [[paper]](https://arxiv.org/abs/2103.15691)
- (arXiv 2021.04) Composable Augmentation Encoding for Video Representation Learning, [[Paper]](https://arxiv.org/pdf/2104.00616.pdf)
- (arXiv 2021.04) Temporal Query Networks for Fine-grained Video Understanding, [[Paper]](https://arxiv.org/pdf/2104.09496.pdf), [[Project]](https://www.robots.ox.ac.uk/~vgg/research/tqn/)
- (arXiv 2021.04) Higher Order Recurrent Space-Time Transformer, [[Paper]](https://arxiv.org/pdf/2104.08665.pdf), [[Code]](https://github.com/CorcovadoMing/HORST)
- (arXiv 2021.04) VideoGPT: Video Generation using VQ-VAE and Transformers, [[Paper]](https://arxiv.org/pdf/2104.10157.pdf), [[Code]](https://wilson1yan.github.io/videogpt/index.html)
- (arXiv 2021.04) VidTr: Video Transformer Without Convolutions, [[Paper]](https://arxiv.org/pdf/2104.11746.pdf)
- (arXiv 2021.05) Local Frequency Domain Transformer Networks for Video Prediction, [[Paper]](https://arxiv.org/ftp/arxiv/papers/2105/2105.04637.pdf)
- (arXiv 2021.05) End-to-End Video Object Detection with Spatial-Temporal Transformers, [[Paper]](https://arxiv.org/pdf/2105.10920.pdf), [[Code]](https://github.com/SJTU-LuHe/TransVOD)
- (arXiv 2021.06) Anticipative Video Transformer, [[Paper]](https://arxiv.org/pdf/2106.02036.pdf), [[Project]](https://facebookresearch.github.io/AVT/)
- (arXiv 2021.06) TransVOS: Video Object Segmentation with Transformers, [[Paper]](https://arxiv.org/pdf/2106.00588.pdf)
- (arXiv 2021.06) Associating Objects with Transformers for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2106.02638.pdf)
- (arXiv 2021.06) Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers, [[Paper]](https://arxiv.org/pdf/2106.05392.pdf)
- (arXiv 2021.06) Space-time Mixing Attention for Video Transformer, [[Paper]](https://arxiv.org/pdf/2106.05968.pdf)
- (arXiv 2021.06) Video Instance Segmentation using Inter-Frame Communication Transformers, [[Paper]](https://arxiv.org/pdf/2106.03299.pdf)
- (arXiv 2021.06) Long-Short Temporal Contrastive Learning of Video Transformers, [[Paper]](https://arxiv.org/pdf/2106.09212.pdf)
- (arXiv 2021.06) Video Swin Transformer, [[Paper]](https://arxiv.org/pdf/2106.13230.pdf), [[Code]](https://github.com/SwinTransformer/Video-Swin-Transformer)
- (arXiv 2021.06) Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection, [[Paper]](https://arxiv.org/pdf/2106.14447.pdf)
- (arXiv 2021.07) Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation, [[Paper]](https://arxiv.org/pdf/2107.00977.pdf), [[Code]](https://github.com/HReynaud/UVT)
- (arXiv 2021.07) Generative Video Transformer: Can Objects be the Words, [[Paper]](https://arxiv.org/pdf/2107.09240.pdf)
- (arXiv 2021.07) Convolutional Transformer based Dual Discriminator Generative Adversarial Networks for Video Anomaly Detection, [[Paper]](https://arxiv.org/pdf/2107.13720.pdf)
- (arXiv 2021.08) Token Shift Transformer for Video Classification, [[Paper]](https://arxiv.org/pdf/2108.02432.pdf), [[Code]](https://github.com/VideoNetworks/TokShift-Transformer)
- (arXiv 2021.08) Mounting Video Metadata on Transformer-based Language Model for Open-ended Video Question Answering, [[Paper]](https://arxiv.org/pdf/2108.05158.pdf)
- (arXiv 2021.08) Video Relation Detection via Tracklet based Visual Transformer, [[Paper]](https://arxiv.org/pdf/2108.08669.pdf), [[Code]](https://github.com/Dawn-LX/VidVRD-tracklets)
- (arXiv 2021.08) MM-ViT: Multi-Modal Video Transformer for Compressed Video Action Recognition, [[Paper]](https://arxiv.org/pdf/2108.09322.pdf)
- (arXiv 2021.08) ZS-SLR: Zero-Shot Sign Language Recognition from RGB-D Videos, [[Paper]](https://arxiv.org/pdf/2108.10059.pdf)
- (arXiv 2021.09) FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting, [[Paper]](https://arxiv.org/pdf/2109.02974.pdf), [[Code]](https://github.com/ruiliu-ai/FuseFormer)
- (arXiv 2021.09) Hierarchical Multimodal Transformer to Summarize Videos, [[Paper]](https://arxiv.org/pdf/2109.10559.pdf)
- (arXiv 2021.10) Object-Region Video Transformers, [[Paper]](https://arxiv.org/pdf/2110.06915.pdf), [[Code]](https://roeiherz.github.io/ORViT/)
- (arXiv 2021.10) Can't Fool Me: Adversarially Robust Transformer for Video Understanding, [[Paper]](https://arxiv.org/pdf/2110.13950.pdf), [[Code]](https://roeiherz.github.io/ORViT/)
- (arXiv 2021.11) Livestock Monitoring with Transformer, [[Paper]](https://arxiv.org/pdf/2111.00801.pdf)
- (arXiv 2021.11) Sparse Adversarial Video Attacks with Spatial Transformations, [[Paper]](https://arxiv.org/pdf/2111.05468.pdf), [[Code]](https://github.com/TrustAI/DeepSAVA)
- (arXiv 2021.11) PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer, [[Paper]](https://arxiv.org/pdf/2111.12082.pdf), [[Code]](https://github.com/ZitongYu/PhysFormer)
- (arXiv 2021.11) Efficient Video Transformers with Spatial-Temporal Token Selection, [[Paper]](https://arxiv.org/pdf/2111.11591.pdf)
- (arXiv 2021.11) Video Frame Interpolation Transformer, [[Paper]](https://arxiv.org/pdf/2111.13817.pdf)
- (arXiv 2021.12) Self-supervised Video Transformer, [[Paper]](https://arxiv.org/pdf/2112.01514.pdf), [[Code]](https://git.io/J1juJ)
- (arXiv 2021.12) BEVT: BERT Pretraining of Video Transformers, [[Paper]](https://arxiv.org/pdf/2112.01529.pdf)
- (arXiv 2021.12) TBN-ViT: Temporal Bilateral Network with Vision Transformer for Video Scene Parsing, [[Paper]](https://arxiv.org/pdf/2112.01033.pdf)
- (arXiv 2021.12) Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval, [[Paper]](https://arxiv.org/pdf/2112.04446.pdf)
- (arXiv 2021.12) DualFormer: Local-Global Stratified Transformer for Efficient Video Recognition, [[Paper]](https://arxiv.org/pdf/2112.04674.pdf)
- (arXiv 2021.12) A Bilingual, OpenWorld Video Text Dataset and End-to-end Video Text Spotter with Transformer, [[Paper]](https://arxiv.org/pdf/2112.04888.pdf), [[Code]](https://github.com/weijiawu/BOVText-Benchmark)
- (arXiv 2021.12) Mask2Former for Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2112.10764.pdf), [[Code]](https://github.com/facebookresearch/Mask2Former)
- (arXiv 2021.12) LocFormer: Enabling Transformers to Perform Temporal Moment Localization on Long Untrimmed Videos With a Feature Sampling Approach, [[Paper]](https://arxiv.org/pdf/2112.10066.pdf)
- (arXiv 2021.12) Video Joint Modelling Based on Hierarchical Transformer for Co-summarization, [[Paper]](https://arxiv.org/pdf/2112.13478.pdf)
- (arXiv 2021.12) Siamese Network with Interactive Transformer for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2112.13983.pdf), [[Code]](https://github.com/LANMNG/SITVOS)
- (arXiv 2022.01) Multiview Transformers for Video Recognition,[[Paper]](https://arxiv.org/pdf/2201.04288.pdf)
- (arXiv 2022.01) TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers,[[Paper]](https://arxiv.org/pdf/2201.05047.pdf)
- (arXiv 2022.01) MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition,[[Paper]](https://arxiv.org/pdf/2201.08383.pdf)
- (arXiv 2022.01) Explore and Match: End-to-End Video Grounding with Transformer,[[Paper]](https://arxiv.org/pdf/2201.10168.pdf)
- (arXiv 2022.01) VRT: A Video Restoration Transformer,[[Paper]](https://arxiv.org/pdf/2201.12288.pdf), [[Code]](https://github.com/JingyunLiang/VRT)
- (arXiv 2022.02) Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval, [[Paper]](https://arxiv.org/pdf/2202.06014.pdf), [[Code]](https://git.openi.org.cn/zangxh/PiT.git)
- (arXiv 2022.02) Audio Visual Scene-Aware Dialog Generation with Transformer-based Video Representations, [[Paper]](https://arxiv.org/pdf/2202.09979.pdf)
- (arXiv 2022.02) Instantaneous Physiological Estimation using Video Transformers, [[Paper]](https://arxiv.org/pdf/2202.12368.pdf), [[Code]](https://github.com/revanurambareesh/instantaneous_transformer)
- (arXiv 2022.03) Spatio-temporal Vision Transformer for Super-resolution Microscopy, [[Paper]](https://arxiv.org/pdf/2203.00030.pdf), [[Code]](https://github.com/charlesnchr/vsr-sim)
- (arXiv 2022.03) ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection, [[Paper]](https://arxiv.org/pdf/2203.01562.pdf)
- (arXiv 2022.03) End-to-End Video Text Spotting with Transformer, [[Paper]](https://arxiv.org/pdf/2203.10539.pdf), [[Code]](https://github.com/weijiawu/TransDETR)
- (arXiv 2022.03) Associating Objects with Scalable Transformers for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2203.11442.pdf), [[Code]](https://github.com/z-x-yang/AOT)
- (arXiv 2022.03) Self-supervised Video-centralised Transformer for Video Face Clustering, [[Paper]](https://arxiv.org/pdf/2203.13166.pdf)
- (arXiv 2022.03) VPTR: Efficient Transformers for Video Prediction, [[Paper]](https://arxiv.org/pdf/2203.15836.pdf), [[Code]](https://github.com/XiYe20/VPTR)
- (arXiv 2022.03) Deformable Video Transformer, [[Paper]](https://arxiv.org/pdf/2203.16795.pdf)
- (arXiv 2023.03) Query-Dependent Video Representation for Moment Retrieval and Highlight Detection, [[Paper]](https://arxiv.org/pdf/2303.13874.pdf), [[Code]](http://github.com/wjun0830/QD-DETR)
- (arXiv 2022.04) Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer, [[Paper]](https://arxiv.org/pdf/2204.03638.pdf), [[Code]](https://songweige.github.io/projects/tats/index.html)
- (arXiv 2022.05) Video Frame Interpolation with Transformer, [[Paper]](https://arxiv.org/pdf/2205.07230.pdf), [[Code]](https://github.com/dvlab-research/VFIformer)
- (arXiv 2022.05) TubeFormer-DeepLab: Video Mask Transformer, [[Paper]](https://arxiv.org/pdf/2205.15361.pdf)
- (arXiv 2022.06) Recurrent Video Restoration Transformer with Guided Deformable Attention, [[Paper]](https://arxiv.org/pdf/2206.02146.pdf), [[Code]](https://github.com/JingyunLiang/RVRT)
- (arXiv 2022.06) Patch-based Object-centric Transformers for Efficient Video Generation, [[Paper]](https://arxiv.org/pdf/2206.04003.pdf), [[Code]](https://sites.google.com/view/povt-public)
- (arXiv 2022.06) Transformer-based Self-Supervised Fish Segmentation in Underwater Videos, [[Paper]](https://arxiv.org/pdf/2206.05390.pdf)
- (arXiv 2022.06) MaskViT: Masked Visual Pre-Training for Video Prediction, [[Paper]](https://arxiv.org/pdf/2206.11894.pdf), [[Code]](https://maskedvit.github.io/)
- (arXiv 2022.07) STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding, [[Paper]](https://arxiv.org/pdf/2207.02756.pdf)
- (arXiv 2022.07) Dual-Stream Transformer for Generic Event Boundary Captioning, [[Paper]](https://arxiv.org/pdf/2207.03038.pdf), [[Code]](https://github.com/GX77/Dual-Stream-Transformer-for-Generic-Event-Boundary-Captioning)
- (arXiv 2022.07) Cross-Attention Transformer for Video Interpolation, [[Paper]](https://arxiv.org/pdf/2207.04132.pdf), [[Code]](https://github.com/hannahhalin/TAIN)
- (arXiv 2022.07) Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet, [[Paper]](https://arxiv.org/pdf/2207.04320.pdf), [[Code]](https://github.com/JimmyZou/Snipper)
- (arXiv 2022.07) Time Is MattEr: Temporal Self-supervision for Video Transformers, [[Paper]](https://arxiv.org/pdf/2207.09067.pdf), [[Code]](https://github.com/alinlab/temporal-selfsupervision)
- (arXiv 2022.07) TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation, [[Paper]](https://arxiv.org/pdf/2207.09048.pdf), [[Code]](https://github.com/researchmm/TTVFI.git)
- (arXiv 2022.07) DeVIS: Making Deformable Transformers Work for Video Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2207.11103.pdf), [[Code]](https://github.com/acaelles97/DeVIS)
- (arXiv 2022.07) Video Swin Transformers for Egocentric Video Understanding @ Ego4D Challenges 2022, [[Paper]](https://arxiv.org/pdf/2207.11329.pdf)
- (arXiv 2022.08) BATMAN: Bilateral Attention Transformer in Motion-Appearance Neighboring Space for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2208.01159.pdf)
- (arXiv 2022.08) Two-Stream Transformer Architecture for Long Form Video Understanding, [[Paper]](https://arxiv.org/pdf/2208.01753.pdf)
- (arXiv 2022.08) PatchDropout: Economizing Vision Transformers Using Patch Dropout, [[Paper]](https://arxiv.org/pdf/2208.07220.pdf)
- (arXiv 2022.08) Class-attention Video Transformer for Engagement Intensity Prediction, [[Paper]](https://arxiv.org/pdf/2208.07216.pdf), [[Code]](https://github.com/mountainai/cavt)
- (arXiv 2022.08) Efficient Attention-free Video Shift Transformers, [[Paper]](https://arxiv.org/pdf/2208.11108.pdf), [[Code]](https://github.com/mountainai/cavt)
- (arXiv 2022.09) PTSEFormer: Progressive Temporal-Spatial Enhanced TransFormer Towards Video Object Detection, [[Paper]](https://arxiv.org/pdf/2209.02242.pdf), [[Code]](https://github.com/Hon-Wong/PTSEFormer)
- (arXiv 2022.09) Spatial-Temporal Transformer for Video Snapshot Compressive Imaging, [[Paper]](https://arxiv.org/pdf/2209.01578.pdf), [[Code]](https://github.com/ucaswangls/STFormer.git)
- (arXiv 2022.09) An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling, [[Paper]](https://arxiv.org/pdf/2209.01540.pdf)
- (arXiv 2022.09) Video Vision Transformers for Violence Detection, [[Paper]](https://arxiv.org/pdf/2209.03561.pdf)
- (arXiv 2022.09) On the Surprising Effectiveness of Transformers in Low-Labeled Video Recognition, [[Paper]](https://arxiv.org/pdf/2209.07474.pdf)
- (arXiv 2022.09) Real-time Online Video Detection with Temporal Smoothing Transformers, [[Paper]](https://arxiv.org/pdf/2209.09236.pdf), [[Code]](https://github.com/zhaoyue-zephyrus/TeSTra)
- (arXiv 2022.09) Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos, [[Paper]](https://arxiv.org/pdf/2209.09484.pdf)
- (arXiv 2022.10) Temporally Consistent Video Transformer for Long-Term Video Prediction, [[Paper]](https://arxiv.org/pdf/2210.02396.pdf), [[Project]](https://wilson1yan.github.io/teco)
- (arXiv 2022.10) Video Referring Expression Comprehension via Transformer with Content-aware Query, [[Paper]](https://arxiv.org/pdf/2210.02953.pdf), [[Project]](https://github.com/mengcaopku/ContFormer)
- (arXiv 2022.10) Turbo Training with Token Dropout, [[Paper]](https://arxiv.org/pdf/2210.04889.pdf)
- (arXiv 2022.10) Temporal and Contextual Transformer for Multi-Camera Editing of TV Shows, [[Paper]](https://arxiv.org/pdf/2210.08737.pdf)
- (arXiv 2022.10) TransVisDrone: Spatio-Temporal Transformer for Vision-based Drone-to-Drone Detection in Aerial Videos, [[Paper]](https://arxiv.org/pdf/2210.08423.pdf), [[Code]](https://github.com/tusharsangam/TransVisDrone)
- (arXiv 2022.10) Linear Video Transformer with Feature Fixation, [[Paper]](https://arxiv.org/pdf/2210.08164.pdf)
- (arXiv 2022.10) Transfer-learning for video classification: Video Swin Transformer on multiple domains, [[Paper]](https://arxiv.org/pdf/2210.09969.pdf)
- (arXiv 2022.10) ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design, [[Paper]](https://arxiv.org/pdf/2210.09573.pdf)
- (arXiv 2022.10) Foreground Guidance and Multi-Layer Feature Fusion for Unsupervised Object Discovery with Transformers, [[Paper]](https://arxiv.org/pdf/2210.13053.pdf), [[Code]](https://github.com/VDIGPKU/FORMULA)
- (arXiv 2022.10) Fully-attentive and interpretable: vision and video vision transformers for pain detection, [[Paper]](https://arxiv.org/pdf/2210.15769.pdf), [[Code]](https://github.com/IPDTFE/ViT-McMaster)
- (arXiv 2022.11) SCOTCH and SODA: A Transformer Video Shadow Detection Framework, [[Paper]](https://arxiv.org/pdf/2211.06885.pdf)
- (arXiv 2022.11) SATVSR: Scenario Adaptive Transformer for Cross Scenarios Video Super-Resolution, [[Paper]](https://arxiv.org/pdf/2211.08703.pdf)
- (arXiv 2022.11) UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer, [[Paper]](https://arxiv.org/pdf/2211.09552.pdf), [[Code]](https://github.com/OpenGVLab/UniFormerV2)
- (arXiv 2022.11) SuperTran: Reference Based Video Transformer for Enhancing Low Bitrate Streams in Real Time, [[Paper]](https://arxiv.org/pdf/2211.12604.pdf)
- (arXiv 2022.11) PatchBlender: A Motion Prior for Video Transformers, [[Paper]](https://arxiv.org/pdf/2211.14449.pdf)
- (arXiv 2022.12) Rethinking Video ViTs: Sparse Video Tubes for Joint Image and Video Learning, [[Paper]](https://arxiv.org/pdf/2212.03229.pdf)
- (arXiv 2022.12) FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer, [[Paper]](https://arxiv.org/pdf/2212.03145.pdf)
- (arXiv 2022.12) PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data, [[Paper]](https://arxiv.org/pdf/2212.04821.pdf)
- (arXiv 2022.12) Video Prediction by Efficient Transformers, [[Paper]](https://arxiv.org/pdf/2212.06026.pdf), [[Code]](https://github.com/XiYe20/VPTR)
- (arXiv 2022.12) MAGVIT: Masked Generative Video Transformer, [[Paper]](https://arxiv.org/pdf/2212.06026.pdf), [[Code]](https://magvit.cs.cmu.edu/)
- (arXiv 2022.12) Efficient Movie Scene Detection using State-Space Transformers, [[Paper]](https://arxiv.org/pdf/2212.14427.pdf)
- (arXiv 2023.02) Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation, [[Paper]](https://arxiv.org/pdf/2302.11325.pdf), [[Code]](https://github.com/SimonZeng7108/Video-SwinUNet)
- (arXiv 2023.03) Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos with Transformers, [[Paper]](https://arxiv.org/pdf/2303.09164.pdf)
- (arXiv 2023.03) A transformer-based approach to video frame-level prediction in Affective Behaviour Analysis In-the-wild, [[Paper]](https://arxiv.org/pdf/2303.09293.pdf)
- (arXiv 2023.03) TKN: Transformer-based Keypoint Prediction Network For Real-time Video Prediction, [[Paper]](https://arxiv.org/pdf/2303.09807.pdf)
- (arXiv 2023.03) Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers, [[Paper]](https://arxiv.org/pdf/2303.11251.pdf)
- (arXiv 2023.03) MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models, [[Paper]](https://arxiv.org/pdf/2303.13009.pdf), [[Code]](https://github.com/mlvlab/MELTR)
- (arXiv 2023.03) Unmasked Teacher: Towards Training-Efficient Video Foundation Models, [[Paper]](https://arxiv.org/pdf/2303.16058.pdf), [[Code]](https://github.com/OpenGVLab/unmasked_teacher)
- (arXiv 2023.04) SVT: Supertoken Video Transformer for Efficient Video Understanding, [[Paper]](https://arxiv.org/pdf/2304.00325.pdf)
- (arXiv 2023.04) Hierarchical Vision Transformers for Cardiac Ejection Fraction Estimation, [[Paper]](https://arxiv.org/pdf/2304.00177.pdf), [[Code]](https://github.com/lhfazry/UltraSwin)
- (arXiv 2023.04) MED-VT: Multiscale Encoder-Decoder Video Transformer with Application to Object Segmentation, [[Paper]](https://arxiv.org/pdf/2304.05930.pdf)
- (arXiv 2023.04) SViTT: Temporal Learning of Sparse Video-Text Transformers, [[Paper]](https://arxiv.org/pdf/2304.08809.pdf), [[Code]](http://svcl.ucsd.edu/projects/svitt)
- (arXiv 2023.04) StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos, [[Paper]](https://arxiv.org/pdf/2304.13265.pdf)
- (arXiv 2023.04) Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations, [[Paper]](https://arxiv.org/pdf/2304.13089.pdf)
- (arXiv 2023.05) Transformer-based model for monocular visual odometry: a video understanding approach, [[Paper]](https://arxiv.org/pdf/2305.06121.pdf)
- (arXiv 2023.05) VDT: An Empirical Study on Video Diffusion with Transformers, [[Paper]](https://arxiv.org/pdf/2305.13311.pdf), [[Code]](https://github.com/RERV/VDT)
- (arXiv 2023.05) Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2305.16318.pdf), [[Code]](https://github.com/OpenGVLab/MUTR)
- (arXiv 2023.05) MS-DETR: Natural Language Video Localization with Sampling Moment-Moment Interaction, [[Paper]](https://arxiv.org/pdf/2305.18969.pdf), [[Code]](https://github.com/K-Nick/MS-DETR)
- (arXiv 2023.06) Backchannel Detection and Agreement Estimation from Video with Transformer Networks, [[Paper]](https://arxiv.org/pdf/2306.01656.pdf)
- (arXiv 2023.06) Unmasking Deepfakes: Masked Autoencoding Spatiotemporal Transformers for Enhanced Video Forgery Detection, [[Paper]](https://arxiv.org/pdf/2306.06881.pdf)
- (arXiv 2023.06) Unfolding Framework with Prior of Convolution-Transformer Mixture and Uncertainty Estimation for Video Snapshot Compressive Imaging, [[Paper]](https://arxiv.org/pdf/2306.11316.pdf)
- (arXiv 2023.06) TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting, [[Paper]](https://arxiv.org/pdf/2306.10940.pdf), [[Code]](https://github.com/Orion-Ai-Lab/TeleViT)
- (arXiv 2023.07) Efficient Convolution and Transformer-Based Network for Video Frame Interpolation, [[Paper]](https://arxiv.org/pdf/2307.06443.pdf)
- (arXiv 2023.07) Hierarchical Spatiotemporal Transformers for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2307.08263.pdf)
- (arXiv 2023.07) Video Frame Interpolation with Flow Transformer, [[Paper]](https://arxiv.org/pdf/2307.16144.pdf)
- (arXiv 2023.07) Temporally-Adaptive Models for Efficient Video Understanding, [[Paper]](https://arxiv.org/pdf/2308.05787.pdf), [[Code]](https://github.com/alibaba-mmai-research/TAdaConv)
- (arXiv 2023.08) EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2308.04162.pdf), [[Code]](https://github.com/lab206/EPCFormer)
- (arXiv 2023.08) Video OWL-ViT: Temporally-consistent open-world localization in video, [[Paper]](https://arxiv.org/pdf/2308.11062.pdf), [[Code]](https://github.com/google-research/scenic)
- (arXiv 2023.08) Spherical Vision Transformer for 360-degree Video Saliency Prediction, [[Paper]](https://arxiv.org/pdf/2308.13004.pdf)
- (arXiv 2023.09) Self-Supervised Video Transformers for Isolated Sign Language Recognition, [[Paper]](https://arxiv.org/pdf/2309.02450.pdf)
- (arXiv 2023.09) CATR: Combinatorial-Dependence Audio-Queried Transformer for Audio-Visual Video Segmentation, [[Paper]](https://arxiv.org/pdf/2309.09709.pdf), [[Code]](https://github.com/aspirinone/CATR.github.io)
- (arXiv 2023.09) PanoVOS:Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation, [[Paper]](https://arxiv.org/pdf/2309.12303.pdf), [[Code]](https://github.com/shilinyan99/PanoVOS)
- (arXiv 2023.09) Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2309.11933.pdf)
- (arXiv 2023.09) Spatial-Temporal Transformer based Video Compression Framework, [[Paper]](https://arxiv.org/pdf/2309.11913.pdf)
- (arXiv 2023.10) Video Transformers under Occlusion: How Physics and Background Attributes Impact Large Models for Robotic Manipulation, [[Paper]](https://arxiv.org/pdf/2310.02044.pdf), [[Code]](https://github.com/ShutongJIN/OccluManip.git)
- (arXiv 2023.10) Reinforcement Learning-based Mixture of Vision Transformers for Video Violence Recognition, [[Paper]](https://arxiv.org/pdf/2310.03108.pdf)
- (arXiv 2023.10) Video Referring Expression Comprehension via Transformer with Content-conditioned Query, [[Paper]](https://arxiv.org/pdf/2310.16402.pdf)
- (arXiv 2023.11) Concatenated Masked Autoencoders as Spatial-Temporal Learner, [[Paper]](https://arxiv.org/pdf/2311.00961.pdf), [[Code]](https://github.com/minhoooo1/CatMAE)
- (arXiv 2023.11) Multi-entity Video Transformers for Fine-Grained Video Representation Learning, [[Paper]](https://arxiv.org/pdf/2311.10873.pdf), [[Code]](https://github.com/facebookresearch/video_rep_learning)
- (arXiv 2023.11) E-ViLM: Efficient Video-Language Model via Masked Video Modeling with Semantic Vector-Quantized Tokenizer, [[Paper]](https://arxiv.org/pdf/2311.17267.pdf)
- (arXiv 2023.11) Just Add 蟺! Pose Induced Video Transformers for Understanding Activities of Daily Living, [[Paper]](https://arxiv.org/pdf/2311.18840.pdf), [[Code]](https://github.com/dominickrei/pi-vit)
- (arXiv 2023.12) DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding, [[Paper]](https://arxiv.org/pdf/2312.02549.pdf)
- (arXiv 2023.12) Unleashing the Power of CNN and Transformer for Balanced RGB-Event Video Recognition, [[Paper]](https://arxiv.org/pdf/2312.11128.pdf), [[Code]](https://github.com/Event-AHU/TSCFormer)
- (arXiv 2023.12) MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers, [[Paper]](https://arxiv.org/pdf/2312.12468.pdf), [[Code]](https://maskint.github.io/)
- (arXiv 2024.01) GloTSFormer: Global Video Text Spotting Transformer, [[Paper]](https://arxiv.org/pdf/2401.03694.pdf)
- (arXiv 2024.01) Latte: Latent Diffusion Transformer for Video Generation, [[Paper]](https://arxiv.org/pdf/2401.03048.pdf), [[Code]](https://maxin-cn.github.io/latte_project)
- (arXiv 2024.01) HaltingVT: Adaptive Token Halting Transformer for Efficient Video Recognition, [[Paper]](https://arxiv.org/pdf/2401.04975.pdf), [[Code]](https://github.com/dun-research/HaltingVT)
- (arXiv 2024.01) Transformer-based Video Saliency Prediction with High Temporal Dimension Decoding, [[Paper]](https://arxiv.org/pdf/2401.07942.pdf)
- (arXiv 2024.01) Understanding Video Transformers via Universal Concept Discovery, [[Paper]](https://arxiv.org/pdf/2401.10831.pdf), [[Code]](https://yorkucvil.github.io/VTCD)
- (arXiv 2024.01) VJT: A Video Transformer on Joint Tasks of Deblurring, Low-light Enhancement and Denoising, [[Paper]](https://arxiv.org/pdf/2401.14754.pdf)
- (arXiv 2024.03) A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video, [[Paper]](https://arxiv.org/pdf/2403.03461.pdf)
- (arXiv 2024.03) OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework, [[Paper]](https://arxiv.org/pdf/2403.08682.pdf)
- (arXiv 2024.03) vid-TLDR: Training Free Token merging for Light-weight Video Transformer, [[Paper]](https://arxiv.org/pdf/2403.13347.pdf), [[Code]](https://github.com/mlvlab/vid-TLDR)
- (arXiv 2024.04) SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers, [[Paper]](https://arxiv.org/pdf/2404.03831.pdf)
- (arXiv 2024.04) PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos, [[Paper]](https://arxiv.org/pdf/2404.04430.pdf)
- (arXiv 2024.04) A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos, [[Paper]](https://arxiv.org/pdf/2404.07351.pdf)
- (arXiv 2024.04) Arena: A Patch-of-Interest ViT Inference Acceleration System for Edge-Assisted Video Analytics, [[Paper]](https://arxiv.org/pdf/2404.09245.pdf)
- (arXiv 2024.04) Multilateral Temporal-view Pyramid Transformer for Video Inpainting Detection, [[Paper]](https://arxiv.org/pdf/2404.11054.pdf)
- (arXiv 2024.04) Sentiment-oriented Transformer-based Variational Autoencoder Network for Live Video Commenting, [[Paper]](https://arxiv.org/pdf/2404.12782.pdf), [[Code]](https://github.com/fufy1024/So-TVAE)
- (arXiv 2024.05) DeVOS: Flow-Guided Deformable Transformer for Video Object Segmentation, [[Paper]](https://arxiv.org/pdf/2405.08715.pdf)
- (arXiv 2024.06) SViTT-Ego: A Sparse Video-Text Transformer for Egocentric Video,  [[Paper]](https://arxiv.org/pdf/2406.09462.pdf)
- (arXiv 2024.06) Video Frame Interpolation for Polarization via Swin-Transformer,  [[Paper]](https://arxiv.org/pdf/2406.11371.pdf)
- (arXiv 2024.07) Leveraging Transformers for Weakly Supervised Object Localization in Unconstrained Videos,  [[Paper]](https://arxiv.org/pdf/2407.06018.pdf), [[Code]](https://github.com/shakeebmurtaza/TrCAM/)
- (arXiv 2024.07) Pose-guided multi-task video transformer for driver action recognition,  [[Paper]](https://arxiv.org/pdf/2407.13750.pdf)
- (arXiv 2024.07) Hierarchical Separable Video Transformer for Snapshot Compressive Imaging,  [[Paper]](https://arxiv.org/pdf/2407.11946.pdf), [[Code]](https://github.com/pwangcs/HiSViT)
- (arXiv 2024.08) GAReT: Cross-view Video Geolocalization with Adapters and Auto-Regressive Transformers,  [[Paper]](https://arxiv.org/pdf/2408.02840.pdf), [[Code]](https://github.com/manupillai308/GAReT)
- (arXiv 2024.08) Dynamic and Compressive Adaptation of Transformers From Images to Videos,  [[Paper]](https://arxiv.org/pdf/2408.06840.pdf)
- (arXiv 2024.09) Transformer with Leveraged Masked Autoencoder for video-based Pain Assessment,  [[Paper]](https://arxiv.org/pdf/2409.05088.pdf)
- (arXiv 2024.10) Saliency-Guided DETR for Moment Retrieval and Highlight Detection,  [[Paper]](https://arxiv.org/pdf/2410.01615.pdf)
- (arXiv 2024.11) SSTAA: Spatio-Temporal Attention Attribution for Real-Time Interpreting Transformer-based Video Models,  [[Paper]](https://arxiv.org/pdf/2411.00630.pdf)
- (arXiv 2024.11) Don't Look Twice: Faster Video Transformers with Run-Length Tokenization,  [[Paper]](https://arxiv.org/pdf/2411.05222.pdf), [[Code]](https://rccchoudhury.github.io/rlt/)
- (arXiv 2024.11) DT-JRD: Deep Transformer based Just Recognizable Difference Prediction Model for Video Coding for Machines,  [[Paper]](https://arxiv.org/pdf/2411.09308)
- (arXiv 2024.12) TRecViT: A Recurrent Video Transformer,  [[Paper]](https://arxiv.org/pdf/2412.14294), [[Code]](https://github.com/google-deepmind/trecvit)
- (arXiv 2025.01) FullTransNet: Full Transformer with Local-Global Attention for Video Summarization,  [[Paper]](https://arxiv.org/pdf/2501.00882), [[Code]](https://github.com/Chianglu/FullTranNet)
- (arXiv 2025.01) LongViTU: Instruction Tuning for Long-Form Video Understanding,  [[Paper]](https://arxiv.org/pdf/2501.05037)
- (arXiv 2025.01) TranStable: Towards Robust Pixel-level Online Video Stabilization by Jointing Transformer and CNN,  [[Paper]](https://arxiv.org/pdf/2501.15138)
- (arXiv 2025.02) MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation,  [[Paper]](https://arxiv.org/pdf/2502.12632)
- (arXiv 2025.03) CFSum: A Transformer-Based Multi-Modal Video Summarization Framework With Coarse-Fine Fusion,  [[Paper]](https://arxiv.org/pdf/2503.00364)
- (arXiv 2025.03) HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding,  [[Paper]](https://arxiv.org/pdf/2503.08585)
- (arXiv 2025.03) TransiT: Transient Transformer for Non-line-of-sight Videography,  [[Paper]](https://arxiv.org/pdf/2503.11328)
- (arXiv 2025.03) Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers,  [[Paper]](https://arxiv.org/pdf/2503.11579), [[Code]](https://github.com/TIGER-AI-Lab/Vamba)
- (arXiv 2025.03) TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection,  [[Paper]](https://arxiv.org/pdf/2503.13903)
- (arXiv 2025.03) RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance,  [[Paper]](https://arxiv.org/pdf/2503.12242)
- (arXiv 2025.04) Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition,  [[Paper]](https://arxiv.org/pdf/2504.07792)
- (arXiv 2025.04) Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation,  [[Paper]](https://arxiv.org/pdf/2504.16788)
- (arXiv 2025.04) Learning Streaming Video Representation via Multitask Training,  [[Paper]](https://arxiv.org/pdf/2504.20041), [[Code]](https://go2heart.github.io/streamformer/)
- (arXiv 2025.06) HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation,  [[Paper]](https://arxiv.org/pdf/2506.02975), [[Code]](https://github.com/Tencent/HaploVLM)
- (arXiv 2025.06) Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A Comparative Analysis for Classification Tasks,  [[Paper]](https://arxiv.org/pdf/2506.04367)
- (arXiv 2025.07) ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models,  [[Paper]](https://arxiv.org/pdf/2507.09876)
- (arXiv 2025.07) VideoITG: Improving Multimodal Video Understanding with Instructed Temporal Grounding,  [[Paper]](https://arxiv.org/pdf/2507.13353), [[Code]](https://nvlabs.github.io/VideoITG/)
- (arXiv 2025.08) A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition,  [[Paper]](https://arxiv.org/pdf/2508.06528)
- (arXiv 2025.08) RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization,  [[Paper]](https://arxiv.org/pdf/2508.09459), [[Code]](https://github.com/WenOOI/RelayFormer)
- (arXiv 2025.08) MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing,  [[Paper]](https://arxiv.org/pdf/2508.14423)
- (arXiv 2025.09) Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing,  [[Paper]](https://arxiv.org/pdf/2509.08228), [[Code]](https://github.com/mcao92/BSTFormer)
