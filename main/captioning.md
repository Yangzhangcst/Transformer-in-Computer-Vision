### Captioning
- (arXiv 2021.01)  CPTR: Full Transformer Network for Image Captioning, [[Paper]](https://arxiv.org/pdf/2101.10804.pdf)
- (arXiv 2021.01) Dual-Level Collaborative Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2101.06462.pdf)
- (arXiv.2021.02) VisualGPT: Data-efficient Image Captioning by Balancing Visual Input and Linguistic Knowledge from Pretraining, [[Paper]](https://arxiv.org/pdf/2102.10407.pdf), [[Code]](https://github.com/Vision-CAIR/VisualGPT)
- (arXiv 2021.06) Semi-Autoregressive Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2106.09436.pdf), [[Code]](https://github.com/YuanEZhou/satic)
- (arXiv 2021.08) Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers, [[Paper]](https://arxiv.org/pdf/2108.02147.pdf)
- (arXiv 2021.08) Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning, [[Paper]](https://arxiv.org/pdf/2108.02366.pdf), [[Code]](https://github.com/Unbear430/DGCN-for-image-captioning)
- (arXiv 2021.09) Bornon: Bengali Image Captioning with Transformer-based Deep learning approach, [[Paper]](https://arxiv.org/pdf/2109.05218.pdf)
- (arXiv 2021.09) Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning, [[Paper]](https://arxiv.org/pdf/2109.07799.pdf), [[Code]](https://github.com/shikha-gist/Image-Captioning/) 
- (arXiv 2021.09) Geometry-Entangled Visual Semantic Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2109.14137.pdf)
- (arXiv 2021.10) Geometry Attention Transformer with Position-aware LSTMs for Image Captioning, [[Paper]](https://arxiv.org/pdf/2110.00335.pdf)
- (arXiv 2021.10) Bangla Image Caption Generation through CNN-Transformer based Encoder-Decoder Network, [[Paper]](https://arxiv.org/pdf/2110.12442.pdf)
- (arXiv 2021.11) SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning, [[Paper]](https://arxiv.org/pdf/2111.13196.pdf)
- (arXiv 2021.12) Injecting Semantic Concepts into End-to-End Image Captioning, [[Paper]](https://arxiv.org/pdf/2112.05230.pdf)
- (arXiv 2022.01) Compact Bidirectional Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2201.01984.pdf), [[Code]](https://github.com/YuanEZhou/CBTrans) 
- (arXiv 2022.02) ACORT: A Compact Object Relation Transformer for Parameter Efficient Image Captioning, [[Paper]](https://arxiv.org/pdf/2202.05451.pdf), [[Code]](https://github.com/jiahuei/sparse-image-captioning) 
- (arXiv 2022.02) Deep soccer captioning with transformer: dataset, semantics-related losses, and multi-level evaluation, [[Paper]](https://arxiv.org/pdf/2202.05728.pdf), [[Code]](https://sites.google.com/view/soccercaptioning) 
- (arXiv 2022.03) X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning, [[Paper]](https://arxiv.org/pdf/2203.00843.pdf)
- (arXiv 2022.03) End-to-End Transformer Based Model for Image Captioning, [[Paper]](https://arxiv.org/pdf/2203.15350.pdf)
- (arXiv 2022.03) Quantifying Societal Bias Amplification in Image Captioning, [[Paper]](https://arxiv.org/pdf/2203.15395.pdf)
- (arXiv 2022.04) Image Captioning In the Transformer Age, [[Paper]](https://arxiv.org/pdf/2204.07374.pdf)
- (arXiv 2022.05) Dual-Level Decoupled Transformer for Video Captioning, [[Paper]](https://arxiv.org/pdf/2205.03039.pdf)
- (arXiv 2022.05) Variational Transformer: A Framework Beyond the Trade-off between Accuracy and Diversity for Image Captioning, [[Paper]](https://arxiv.org/pdf/2205.14458.pdf), [[Code]](https://github.com/kaelsunkiller/VaT)
- (arXiv 2022.06) Transformer-Based Multi-modal Proposal and Re-Rank for Wikipedia Image-Caption Matching, [[Paper]](https://arxiv.org/pdf/2206.10436.pdf), [[Code]](https://github.com/mesnico/Wiki-Image-Caption-Matching)
- (arXiv 2022.07) ExpansionNet: exploring the sequence length bottleneck in the Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2207.03327.pdf), [[Code]](https://github.com/jchenghu/ExpansionNet)
- (arXiv 2022.07) GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features, [[Paper]](https://arxiv.org/pdf/2207.09666.pdf), [[Code]](https://github.com/davidnvq/grit)
- (arXiv 2022.07) Retrieval-Augmented Transformer for Image Captioning, [[Paper]](https://arxiv.org/pdf/2207.13162.pdf)
- (arXiv 2022.09) vieCap4H-VLSP 2021: Vietnamese Image Captioning for Healthcare Domain using Swin Transformer and Attention-based LSTM, [[Paper]](https://arxiv.org/pdf/2209.01304.pdf), [[Code]](https://git.io/JDdJm)
- (arXiv 2022.11) VieCap4H - VLSP 2021: ObjectAoA -- Enhancing performance of Object Relation Transformer with Attention on Attention for Vietnamese image captioning, [[Paper]](https://arxiv.org/pdf/2211.05405.pdf)
- (arXiv 2022.11) VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning, [[Paper]](https://arxiv.org/pdf/2211.15103.pdf), [[Code]](https://github.com/UARK-AICV/VLTinT)
- (arXiv 2022.11) GRiT: A Generative Region-to-text Transformer for Object Understanding, [[Paper]](https://arxiv.org/pdf/2212.00280.pdf), [[Code]](https://github.com/JialianW/GRiT)
- (arXiv 2023.01) End-to-End 3D Dense Captioning with Vote2Cap-DETR, [[Paper]](https://arxiv.org/pdf/2301.02508.pdf), [[Code]](https://github.com/ch3cook-fdu/Vote2Cap-DETR)
- (arXiv 2023.02) ADAPT: Action-aware Driving Caption Transformer, [[Paper]](https://arxiv.org/pdf/2302.00673.pdf), [[Code]](https://github.com/jxbbb/ADAPT)
- (arXiv 2023.02) DEVICE: DEpth and VIsual ConcEpts Aware Transformer for TextCaps, [[Paper]](https://arxiv.org/pdf/2302.01540.pdf)
- (arXiv 2023.03) Neighborhood Contrastive Transformer for Change Captioning, [[Paper]](https://arxiv.org/pdf/2303.03171.pdf), [[Code]](https://github.com/tuyunbin/NCT)
- (arXiv 2023.03) Comparative study of Transformer and LSTM Network with attention mechanism on Image Captioning, [[Paper]](https://arxiv.org/pdf/2303.02648.pdf)
- (arXiv 2023.03) Text with Knowledge Graph Augmented Transformer for Video Captioning, [[Paper]](https://arxiv.org/pdf/2303.12423.pdf)
- (arXiv 2023.05) Transforming Visual Scene Graphs to Image Captions, [[Paper]](https://arxiv.org/pdf/2305.02177.pdf)
- (arXiv 2023.07) Embedded Heterogeneous Attention Transformer for Cross-lingual Image Captioning, [[Paper]](https://arxiv.org/pdf/2307.09915.pdf)
- (arXiv 2023.08) RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension, [[Paper]](https://arxiv.org/pdf/2308.02299.pdf), [[Code]](https://github.com/mightyzau/RegionBLIP)
- (arXiv 2023.08) Enhancing image captioning with depth information using a Transformer-based framework, [[Paper]](https://arxiv.org/pdf/2308.03767.pdf)
- (arXiv 2023.09) Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning, [[Paper]](https://arxiv.org/pdf/2309.02999.pdf), [[Code]](https://github.com/ch3cook-fdu/Vote2Cap-DETR)
- (arXiv 2023.09) Collaborative Three-Stream Transformers for Video Captioning, [[Paper]](https://arxiv.org/pdf/2309.09611.pdf), [[Code]](https://github.com/ch3cook-fdu/Vote2Cap-DETR)
- (arXiv 2023.09) Accurate and Fast Compressed Video Captioning, [[Paper]](https://arxiv.org/pdf/2309.12867.pdf), [[Code]](https://github.com/acherstyx/CoCap)
- (arXiv 2024.11) ViTOC: Vision Transformer and Object-aware Captioner, [[Paper]](https://arxiv.org/pdf/2411.07265.pdf)
- (arXiv 2024.12) UnMA-CapSumT: Unified and Multi-Head Attention-driven Caption Summarization Transformer, [[Paper]](https://arxiv.org/pdf/2412.11836.pdf), [[Code]](https://github.com/dhruvsharma09/UnMA-CapSumT)
- (arXiv 2024.12) Automated Image Captioning with CNNs and Transformers, [[Paper]](https://arxiv.org/pdf/2412.10511.pdf), [[Code]](https://github.com/JeremyNathanJusuf/image-captioning)
- (arXiv 2025.02) Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware Graph Transformer for Video Captioning, [[Paper]](https://arxiv.org/pdf/2502.13754.pdf)
- (arXiv 2025.03) SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion, [[Paper]](https://arxiv.org/pdf/2503.16825.pdf), [[Code]](https://github.com/gxytcrc/SGFormer)
- (arXiv 2025.06) Analyzing Transformer Models and Knowledge Distillation Approaches for Image Captioning on Edge AI, [[Paper]](https://arxiv.org/pdf/2506.03607)
- (arXiv 2025.06) Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation, [[Paper]](https://arxiv.org/pdf/2506.05399)
- (arXiv 2025.09) Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model, [[Paper]](https://arxiv.org/pdf/2509.17365), [[Code]](https://github.com/amani-td/Caption-Generation)
