### Diffusion
- (arXiv 2022.12) Scalable Diffusion Models with Transformers, [[Paper]](https://arxiv.org/pdf/2212.09748.pdf), [[Code]](https://www.wpeebles.com/DiT)
- (arXiv 2023.03) Masked Diffusion Transformer is a Strong Image Synthesizer, [[Paper]](https://arxiv.org/pdf/2303.14389.pdf), [[Code]](https://github.com/sail-sg/MDT)
- (arXiv 2023.04) ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis, [[Paper]](https://arxiv.org/pdf/2304.01053.pdf)
- (arXiv 2023.06) DFormer: Diffusion-guided Transformer for Universal Image Segmentation, [[Paper]](https://arxiv.org/pdf/2306.03437.pdf), [[Code]](https://github.com/cp3wan/DFormer)
- (arXiv 2023.08) Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers, [[Paper]](https://arxiv.org/pdf/2308.14152.pdf)
- (arXiv 2023.09) Large-Vocabulary 3D Diffusion Model with Transformer, [[Paper]](https://arxiv.org/pdf/2309.07920.pdf), [[Project]](https://ziangcao0312.github.io/difftf_pages/)
- (arXiv 2023.09) Cartoondiff: Training-free Cartoon Image Generation with Diffusion Transformer Models, [[Paper]](https://arxiv.org/pdf/2309.08251.pdf), [[Project]](https://cartoondiff.github.io/)
- (arXiv 2023.12) DiffiT: Diffusion Vision Transformers for Image Generation, [[Paper]](https://arxiv.org/pdf/2312.02139.pdf)
- (arXiv 2023.12) DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2312.06400.pdf)
- (arXiv 2024.01) SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers, [[Paper]](https://arxiv.org/pdf/2401.08740.pdf), [[Code]](https://github.com/willisma/SiT)
- (arXiv 2024.01) Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2401.11605.pdf), [[Code]](https://crowsonkb.github.io/hourglass-diffusion-transformers)
- (arXiv 2024.02) Cross-view Masked Diffusion Transformers for Person Image Synthesis, [[Paper]](https://arxiv.org/pdf/2402.01516.pdf)
- (arXiv 2024.02) FiT: Flexible Vision Transformer for Diffusion Model, [[Paper]](https://arxiv.org/pdf/2402.12376.pdf), [[Code]](https://github.com/whlzy/FiT)
- (arXiv 2024.03) Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts, [[Paper]](https://arxiv.org/pdf/2403.09176.pdf), [[Code]](https://byeongjun-park.github.io/Switch-DiT/)
- (arXiv 2024.03) SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2403.17004.pdf)
- (arXiv 2024.04) WcDT: World-centric Diffusion Transformer for Traffic Scene Generation, [[Paper]](https://arxiv.org/pdf/2404.02082.pdf)
- (arXiv 2024.04) Solving Masked Jigsaw Puzzles with Diffusion Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.07292.pdf)
- (arXiv 2024.04) Diffscaler: Enhancing the Generative Prowess of Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2404.09976.pdf)
- (arXiv 2024.04) Lazy Diffusion Transformer for Interactive Image Editing, [[Paper]](https://arxiv.org/pdf/2404.12382.pdf), [[Project]](https://lazydiffusion.github.io/)
- (arXiv 2024.05) U-DiTs: Downsample Tokens in U-Shaped Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2405.02730.pdf), [[Code]](https://github.com/YuchuanTian/U-DiT)
- (arXiv 2024.05) Inf-DiT: Upsampling Any-Resolution Image with Memory-Efficient Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2405.04312.pdf), [[Code]](https://github.com/THUDM/Inf-DiT)
- (arXiv 2024.05) Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2405.05945.pdf), [[Code]](https://github.com/THUDM/Inf-DiT)
- (arXiv 2024.05) DiffTF++: 3D-aware Diffusion Transformer for Large-Vocabulary 3D Generation, [[Paper]](https://arxiv.org/pdf/2405.08055.pdf)
- (arXiv 2024.05) Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding, [[Paper]](https://arxiv.org/pdf/2405.08748.pdf),[[Code]](http://github.com/Tencent/HunyuanDiT)
- (arXiv 2024.05) Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2405.14832.pdf),[[Project]](https://nju-3dv.github.io/projects/Direct3D/)
- (arXiv 2024.05) PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models, [[Paper]](https://arxiv.org/pdf/2405.14430.pdf),[[Code]](https://github.com/PipeFusion/PipeFusion)
- (arXiv 2024.05) Human4DiT: Free-view Human Video Generation with 4D Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2405.17405.pdf),[[Code]](https://human4dit.github.io/)
- (arXiv 2024.05) PTQ4DiT: Post-training Quantization for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2405.16005.pdf)
- (arXiv 2024.05) VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2405.18326.pdf),[[Code]](https://zhengjun-ai.github.io/viton-dit-page/)
- (arXiv 2024.05) DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention, [[Paper]](https://arxiv.org/pdf/2405.18428.pdf),[[Code]](https://github.com/hustvl/DiG)
- (arXiv 2024.06) Δ-DiT: A Training-Free Acceleration Method Tailored for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2406.01125)
- (arXiv 2024.06) Dimba: Transformer-Mamba Diffusion Models, [[Paper]](https://arxiv.org/pdf/2406.01159),[[Code]](https://dimba-project.github.io/)
- (arXiv 2024.06) AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation, [[Paper]](https://arxiv.org/pdf/2406.07686)
- (arXiv 2024.06) DiTFastAttn: Attention Compression for Diffusion Transformer Models, [[Paper]](https://arxiv.org/pdf/2406.08552),[[Code]](https://github.com/thu-nics/DiTFastAttn)
- (arXiv 2024.06) Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT, [[Paper]](https://arxiv.org/pdf/2406.18583),[[Code]](https://github.com/Alpha-VLLM/Lumina-T2X)
- (arXiv 2024.07) FORA: Fast-Forward Caching in Diffusion Transformer Acceleration, [[Paper]](https://arxiv.org/pdf/2407.01425),[[Code]](https://github.com/prathebaselva/FORA)
- (arXiv 2024.07) VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control, [[Paper]](https://arxiv.org/pdf/2407.12781)
- (arXiv 2024.07) Scaling Diffusion Transformers to 16 Billion Parameters, [[Paper]](https://arxiv.org/pdf/2407.11633),[[Code]](https://github.com/feizc/DiT-MoE)
- (arXiv 2024.07) DriveDiTFit: Fine-tuning Diffusion Transformers for Autonomous Driving, [[Paper]](https://arxiv.org/pdf/2407.15661),[[Code]](https://github.com/TtuHamg/DriveDiTFit)
- (arXiv 2024.07) Diffusion Feedback Helps CLIP See Better, [[Paper]](https://arxiv.org/pdf/2407.20171),[[Code]](https://github.com/baaivision/DIVA)
- (arXiv 2024.08) Tora: Trajectory-oriented Diffusion Transformer for Video Generation, [[Paper]](https://arxiv.org/pdf/2407.21705),[[Code]](https://github.com/ali-videoai/Tora)
- (arXiv 2024.08) Latent Space Disentanglement in Diffusion Transformers Enables Zero-shot Fine-grained Semantic Editing, [[Paper]](https://arxiv.org/pdf/2408.13335)
- (arXiv 2024.08) DiffSurf: A Transformer-based Diffusion Model for Generating and Reconstructing 3D Surfaces in Pose, [[Paper]](https://arxiv.org/pdf/2408.14860)
- (arXiv 2024.08) MegActor-Σ: Unlocking Flexible Mixed-Modal Control in Portrait Animation with Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2408.14975)
- (arXiv 2024.09) Qihoo-T2X: An Efficiency-Focused Diffusion Transformer via Proxy Tokens for Text-to-Any-Task, [[Paper]](https://arxiv.org/pdf/2409.04005),[[Code]](https://github.com/360CVGroup/Qihoo-T2X)
- (arXiv 2024.09) DiTAS: Quantizing Diffusion Transformers via Enhanced Activation Smoothing, [[Paper]](https://arxiv.org/pdf/2409.07756)
- (arXiv 2024.09) Token Caching for Diffusion Transformer Acceleration, [[Paper]](https://arxiv.org/pdf/2409.18523)
- (arXiv 2024.10) ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2410.00086),[[Code]](https://github.com/modelscope/scepter)
- (arXiv 2024.10) HarmoniCa: Harmonizing Training and Inference for Better Feature Cache in Diffusion Transformer Acceleration, [[Paper]](https://arxiv.org/pdf/2410.01723)
- (arXiv 2024.10) EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice Routing, [[Paper]](https://arxiv.org/pdf/2410.02098)
- (arXiv 2024.10) MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers for Open-Domain Sound Generation, [[Paper]](https://arxiv.org/pdf/2410.02130)
- (arXiv 2024.10) Dynamic Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2410.03456),[[Code]](https://github.com/NUS-HPC-AI-Lab/Dynamic-Diffusion-Transformer)
- (arXiv 2024.10) SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2410.10629)
- (arXiv 2024.10) Boosting Camera Motion Control for Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2410.10802)
- (arXiv 2024.10) The Ingredients for Robotic Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2410.10088),[[Code]](https://github.com/sudeepdasari/dit-policy)
- (arXiv 2024.10) FasterDiT: Towards Faster Diffusion Transformers Training without Architecture Modification, [[Paper]](https://arxiv.org/pdf/2410.10356)
- (arXiv 2024.10) Precipitation Nowcasting Using Diffusion Transformer with Causal Attention, [[Paper]](https://arxiv.org/pdf/2410.13314)
- (arXiv 2024.10) Group Diffusion Transformers are Unsupervised Multitask Learners, [[Paper]](https://arxiv.org/pdf/2410.15027)
- (arXiv 2024.10) Diffusion Transformer Policy, [[Paper]](https://arxiv.org/pdf/2410.15959)
- (arXiv 2024.10) On Inductive Biases That Enable Generalization of Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2410.21273)
- (arXiv 2024.10) GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation, [[Paper]](https://arxiv.org/pdf/2410.20474),[[Code]](https://github.com/KAIST-Visual-AI-Group/GrounDiT/)
- (arXiv 2024.10) EDT: An Efficient Diffusion Transformer Framework Inspired by Human-like Sketching, [[Paper]](https://arxiv.org/pdf/2410.23788),[[Code]](https://github.com/xinwangChen/EDT)
- (arXiv 2024.10) In-Context LoRA for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2410.23775),[[Code]](https://github.com/ali-vilab/In-Context-LoRA)
- (arXiv 2024.11) Learning Where to Edit Vision Transformers, [[Paper]](https://arxiv.org/pdf/2411.01948),[[Code]](https://github.com/hustyyq/Where-to-Edit)
- (arXiv 2024.11) Adaptive Caching for Faster Video Generation with Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2411.02397),[[Code]](https://github.com/AdaCache-DiT/AdaCache)
- (arXiv 2024.11) DiT4Edit: Diffusion Transformer for Image Editing, [[Paper]](https://arxiv.org/pdf/2411.03286)
- (arXiv 2024.11) DanceFusion: A Spatio-Temporal Skeleton Diffusion Transformer for Audio-Driven Dance Motion Reconstruction, [[Paper]](https://arxiv.org/pdf/2411.04646),[[Code]](https://th-mlab.github.io/DanceFusion/)
- (arXiv 2024.11) DiT4Edit: Diffusion Transformer for Image Editing, [[Paper]](https://arxiv.org/pdf/2411.03286),[[Code]](https://github.com/fkyyyy/DiT4Edit)
- (arXiv 2024.11) Latent Space Disentanglement in Diffusion Transformers Enables Precise Zero-shot Semantic Editing, [[Paper]](https://arxiv.org/pdf/2411.08196)
- (arXiv 2024.11) LaVin-DiT: Large Vision Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2411.11505)
- (arXiv 2024.11) FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on, [[Paper]](https://arxiv.org/pdf/2411.10499),[[Code]](https://github.com/BoyuanJiang/FitDiT)
- (arXiv 2024.11) Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic Study, [[Paper]](https://arxiv.org/pdf/2411.13588),[[Code]](https://github.com/xdit-project/DiTCacheAnalysis)
- (arXiv 2024.11) Accelerating Vision Diffusion Transformers with Skip Branches, [[Paper]](https://arxiv.org/pdf/2411.17616),[[Code]](https://github.com/OpenSparseLLMs/Skip-DiT)
- (arXiv 2024.11) Towards Precise Scaling Laws for Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2411.17470)
- (arXiv 2024.11) On Statistical Rates of Conditional Diffusion Transformers: Approximation, Estimation and Minimax Optimality, [[Paper]](https://arxiv.org/pdf/2411.17522)
- (arXiv 2024.11) LetsTalk: Latent Diffusion Transformer for Talking Video Synthesis, [[Paper]](https://arxiv.org/pdf/2411.16748),[[Code]](https://github.com/zhang-haojie/letstalk)
- (arXiv 2024.12) TinyFusion: Diffusion Transformers Learned Shallow, [[Paper]](https://arxiv.org/pdf/2412.01199),[[Code]](https://github.com/VainF/TinyFusion)
- (arXiv 2024.12) CPA: Camera-pose-awareness Diffusion Transformer for Video Generation, [[Paper]](https://arxiv.org/pdf/2412.01429)
- (arXiv 2024.12) Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks, [[Paper]](https://arxiv.org/pdf/2412.00733),[[Code]](https://github.com/fudan-generative-vision/hallo3)
- (arXiv 2024.12) OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows, [[Paper]](https://arxiv.org/pdf/2412.16112),[[Code]](https://github.com/Huage001/CLEAR)
- (arXiv 2024.12) ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2412.07720)
- (arXiv 2024.12) UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics, [[Paper]](https://arxiv.org/pdf/2412.07774),[[Code]](https://xavierchen34.github.io/UniReal-Page/)
- (arXiv 2024.12) Video Motion Transfer with Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2412.07776),[[Code]](https://github.com/ditflow/ditflow)
- (arXiv 2024.12) MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation, [[Paper]](https://arxiv.org/pdf/2412.05848)
- (arXiv 2024.12) FlexDiT: Dynamic Token Density Control for Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2412.06028),[[Code]](https://github.com/changsn/FlexDiT)
- (arXiv 2024.12) Causal Diffusion Transformers for Generative Modeling, [[Paper]](https://arxiv.org/pdf/2412.12095),[[Code]](https://github.com/causalfusion/causalfusion)
- (arXiv 2024.12) AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration, [[Paper]](https://arxiv.org/pdf/2412.11706)
- (arXiv 2024.12) ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2412.12571),[[Code]](https://github.com/ali-vilab/ChatDiT)
- (arXiv 2024.12) StyleDiT: A Unified Framework for Diverse Child and Partner Faces Synthesis with Style Latent Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2412.10785)
- (arXiv 2024.12) Video Diffusion Transformers are In-Context Learners, [[Paper]](https://arxiv.org/pdf/2412.10783),[[Code]](https://github.com/feizc/Video-In-Context)
- (arXiv 2024.12) Efficient Scaling of Diffusion Transformers for Text-to-Image Generation, [[Paper]](https://arxiv.org/pdf/2412.12391)
- (arXiv 2024.12) CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up, [[Paper]](https://arxiv.org/pdf/2412.16112)
- (arXiv 2024.12) Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2412.16822),[[Code]](https://github.com/GATECH-EIC/DiffRatio-MoD)
- (arXiv 2024.12) Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2412.16670)
- (arXiv 2024.12) DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation, [[Paper]](https://arxiv.org/pdf/2412.18597),[[Code]](https://github.com/TencentARC/DiTCtrl)
- (arXiv 2024.12) Accelerating Diffusion Transformers with Dual Feature Caching, [[Paper]](https://arxiv.org/pdf/2412.18911),[[Code]](https://github.com/Shenyi-Z/DuCa)
- (arXiv 2025.01) SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration, [[Paper]](https://arxiv.org/pdf/2501.01320),[[Code]](https://iceclear.github.io/projects/seedvr/)
- (arXiv 2025.01) Ingredients: Blending Custom Photos with Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2501.01790),[[Code]](https://github.com/feizc/Ingredients)
- (arXiv 2025.01) GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking, [[Paper]](https://arxiv.org/pdf/2501.02690),[[Code]](https://wkbian.github.io/Projects/GS-DiT/)
- (arXiv 2025.01) Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2501.03931),[[Code]](https://github.com/dvlab-research/MagicMirror/)
- (arXiv 2025.01) MC-VTON: Minimal Control Virtual Try-On Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2501.03630)
- (arXiv 2025.01) ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning, [[Paper]](https://arxiv.org/pdf/2501.04698)
- (arXiv 2025.01) 3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering, [[Paper]](https://arxiv.org/pdf/2501.05131),[[Code]](hhttps://github.com/limuloo/3DIS)
- (arXiv 2025.01) LiT: Delving into a Simplified Linear Diffusion Transformer for Image Generation, [[Paper]](https://arxiv.org/pdf/2501.12976),[[Code]](https://techmonsterwang.github.io/LiT/)
- (arXiv 2025.01) CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation, [[Paper]](https://arxiv.org/pdf/2501.11325),[[Code]](https://github.com/Zheng-Chong/CatV2TON)
- (arXiv 2025.01) PackDiT: Joint Human Motion and Text Generation via Mutual Prompting, [[Paper]](https://arxiv.org/pdf/2501.16551)
- (arXiv 2025.01) ITVTON: Virtual Try-On Diffusion Transformer Model Based on Integrated Image and Text, [[Paper]](https://arxiv.org/pdf/2501.16757)
- (arXiv 2025.01) SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2501.18427),[[Code]](https://github.com/NVlabs/Sana)
- (arXiv 2025.02) Accelerating Diffusion Transformer via Error-Optimized Cache, [[Paper]](https://arxiv.org/pdf/2501.19243)
- (arXiv 2025.02) LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2502.01105),[[Code]](https://github.com/showlab/LayerTracer)
- (arXiv 2025.02) ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features, [[Paper]](https://arxiv.org/pdf/2502.04320)
- (arXiv 2025.02) UniForm: A Unified Diffusion Transformer for Audio-Video Generation, [[Paper]](https://arxiv.org/pdf/2502.03897)
- (arXiv 2025.02) HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation, [[Paper]](https://arxiv.org/pdf/2502.04847)
- (arXiv 2025.02) VFX Creator: Animated Visual Effect Generation with Controllable Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2502.05979),[[Code]](https://vfx-creator0.github.io/)
- (arXiv 2025.02) Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile, [[Paper]](https://arxiv.org/pdf/2502.06155)
- (arXiv 2025.02) CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2502.06527)
- (arXiv 2025.02) DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training, [[Paper]](https://arxiv.org/pdf/2502.07590)
- (arXiv 2025.02) E-MD3C: Taming Masked Diffusion Transformers for Efficient Zero-Shot Object Customization, [[Paper]](https://arxiv.org/pdf/2502.09164)
- (arXiv 2025.02) SkyReels-A1: Expressive Portrait Animation in Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2502.10841)
- (arXiv 2025.02) Designing Parameter and Compute Efficient Diffusion Transformers using Distillation, [[Paper]](https://arxiv.org/pdf/2502.14226)
- (arXiv 2025.02) RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2502.14377),[[Code]](https://github.com/360CVGroup/RelaCtrl)
- (arXiv 2025.02) RIFLEx: A Free Lunch for Length Extrapolation in Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2502.15894),[[Code]](https://github.com/thu-ml/RIFLEx)
- (arXiv 2025.02) VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2502.20108),[[Code]](https://github.com/ZionGo6/VDT-Auto)
- (arXiv 2025.02) FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute, [[Paper]](https://arxiv.org/pdf/2502.20126)
- (arXiv 2025.03) LEDiT: Your Length-Extrapolatable Diffusion Transformer without Positional Encoding, [[Paper]](https://arxiv.org/pdf/2503.04344),[[Code]](https://github.com/ShenZhang-Shin/LEDiT)
- (arXiv 2025.03) Accelerating Diffusion Transformer via Gradient-Optimized Cache, [[Paper]](https://arxiv.org/pdf/2503.05156) 
- (arXiv 2025.03) Exposure Bias Reduction for Enhancing Diffusion Transformer Feature Caching, [[Paper]](https://arxiv.org/pdf/2503.07120),[[Code]](https://github.com/aSleepyTree/EB-Cache)
- (arXiv 2025.03) EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2503.07027)
- (arXiv 2025.03) TIDE : Temporal-Aware Sparse Autoencoders for Interpretable Diffusion Transformers in Image Generation, [[Paper]](https://arxiv.org/pdf/2503.07050)
- (arXiv 2025.03) Post-Training Quantization for Diffusion Transformer via Hierarchical Timestep Grouping, [[Paper]](https://arxiv.org/pdf/2503.06930)
- (arXiv 2025.03) X2I: Seamless Integration of Multimodal Understanding into Diffusion Transformer via Attention Distillation, [[Paper]](https://arxiv.org/pdf/2503.06134),[[Code]](https://github.com/OPPO-Mente-Lab/X2I)
- (arXiv 2025.03) U-StyDiT: Ultra-high Quality Artistic Style Transfer Using Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2503.08157)
- (arXiv 2025.03) OminiControl2: Efficient Conditioning for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2503.08280),[[Code]](https://github.com/Yuanshi9815/OminiControl)
- (arXiv 2025.03) Diffusion Transformer Meets Random Masks: An Advanced PET Reconstruction Framework, [[Paper]](https://arxiv.org/pdf/2503.08339),[[Code]](https://github.com/yqx7150/DREAM)
- (arXiv 2025.03) UniCombine: Unified Multi-Conditional Combination with Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2503.09277),[[Code]](https://github.com/Xuan-World/UniCombine)
- (arXiv 2025.03) AudioX: Diffusion Transformer for Anything-to-Audio Generation, [[Paper]](https://arxiv.org/pdf/2503.10522),[[Code]](https://github.com/ZeyueT/AudioX)
- (arXiv 2025.03) Cosh-DiT: Co-Speech Gesture Video Synthesis via Hybrid Audio-Visual Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2503.09942)
- (arXiv 2025.03) NAMI: Efficient Image Generation via Progressive Rectified Flow Transformers, [[Paper]](https://arxiv.org/pdf/2503.09242)
- (arXiv 2025.03) DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2503.14487),[[Code]](https://github.com/KwaiVGI/DiffMoE)
- (arXiv 2025.03) Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection, [[Paper]](https://arxiv.org/pdf/2503.12271),[[Code]](https://github.com/jacklishufan/Reflect-DiT)
- (arXiv 2025.03) Personalize Anything for Free with Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2503.12590),[[Code]](https://github.com/fenghora/personalize-anything)
- (arXiv 2025.03) FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing, [[Paper]](https://arxiv.org/pdf/2503.16153),[[Code]](https://github.com/wtybest/FreeFlux)
- (arXiv 2025.03) Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts, [[Paper]](https://arxiv.org/pdf/2503.16057)
- (arXiv 2025.03) BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2503.15927)
- (arXiv 2025.03) U-REPA: Aligning Diffusion U-Nets to ViTs, [[Paper]](https://arxiv.org/pdf/2503.18414),[[Code]](https://github.com/YuchuanTian/U-REPA)
- (arXiv 2025.03) Boosting Resolution Generalization of Diffusion Transformers with Randomized Positional Encodings, [[Paper]](https://arxiv.org/pdf/2503.18719)
- (arXiv 2025.03) Decouple and Track: Benchmarking and Improving Video Diffusion Transformers for Motion Transfer, [[Paper]](https://arxiv.org/pdf/2503.17350),[[Code]](https://github.com/Shi-qingyu/DeT)
- (arXiv 2025.03) EDiT: Efficient Diffusion Transformers with Linear Compressed Attention, [[Paper]](https://arxiv.org/pdf/2503.16726)
- (arXiv 2025.03) Towards Transformer-Based Aligned Generation with Self-Coherence Guidance, [[Paper]](https://arxiv.org/pdf/2503.17675),[[Code]](https://github.com/wang-shulei/SCG-diffusion-code)
- (arXiv 2025.03) AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2503.19824)
- (arXiv 2025.03) DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation, [[Paper]](https://arxiv.org/pdf/2503.19881),[[Code]](https://github.com/Tianhao-Qi/Mask2DiT)  
- (arXiv 2025.03) FullDiT: Multi-Task Video Generative Foundation Model with Full Attention, [[Paper]](https://arxiv.org/pdf/2503.19907)  
- (arXiv 2025.03) ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On, [[Paper]](https://arxiv.org/pdf/2503.20418),[[Code]](https://github.com/jiwoohong93/ita-mdt_code)  
- (arXiv 2025.03) Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy, [[Paper]](https://arxiv.org/pdf/2503.19757),[[Code]](https://github.com/RoboDita/Dita)  
- (arXiv 2025.03) JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization, [[Paper]](https://arxiv.org/pdf/2503.23377),[[Code]](https://javisdit.github.io/)  
- (arXiv 2025.03) DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2503.22796)
- (arXiv 2025.03) DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model, [[Paper]](https://arxiv.org/pdf/2503.23993)
- (arXiv 2025.04) SkyReels-A2: Compose Anything in Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2504.02436),[[Code]](https://github.com/SkyworkAI/SkyReels-A2)  
- (arXiv 2025.04) DDT: Decoupled Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2504.05741),[[Code]](https://github.com/MCG-NJU/DDT)
- (arXiv 2025.04) Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing, [[Paper]](https://arxiv.org/pdf/2504.04784)
- (arXiv 2025.04) DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation, [[Paper]](https://arxiv.org/pdf/2504.06803),[[Code]](https://github.com/alibaba-damo-academy/DyDiT)  
- (arXiv 2025.04) DreamFuse: Adaptive Image Fusion with Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2504.08291),[[Code]](https://ll3rd.github.io/DreamFuse/) 
- (arXiv 2025.04) Insert Anything: Image Insertion via In-Context Editing in DiT, [[Paper]](https://arxiv.org/pdf/2504.15009),[[Code]](https://github.com/song-wensong/insert-anything) 
- (arXiv 2025.04) RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild, [[Paper]](https://arxiv.org/pdf/2504.14977),[[Code]](https://github.com/damo-cv/RealisDance) 
- (arXiv 2025.04) DiTPainter: Efficient Video Inpainting with Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2504.15661)
- (arXiv 2025.04) RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild, [[Paper]](https://arxiv.org/pdf/2504.14977),[[Code]](https://github.com/damo-cv/RealisDance)
- (arXiv 2025.04) Insert Anything: Image Insertion via In-Context Editing in DiT, [[Paper]](https://arxiv.org/pdf/2504.15009),[[Code]](https://github.com/song-wensong/insert-anything)
- (arXiv 2025.04) DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2504.19614)
- (arXiv 2025.04) AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation, [[Paper]](https://arxiv.org/pdf/2504.20629),[[Code]](https://mm.kaist.ac.kr/projects/AlignDiT/)
- (arXiv 2025.04) GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2504.21476),[[Code]](https://github.com/Shenfu-Research/Garment-Diffusion)
- (arXiv 2025.04) In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer, [[Paper]](https://arxiv.org/pdf/2504.20690),[[Code]](https://github.com/River-Zhang/ICEdit)
- (arXiv 2025.05) JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2505.00482),[[Code]](https://byungki-k.github.io/JointDiT/)
- (arXiv 2025.05) FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing,  [[Paper]](https://arxiv.org/pdf/2505.03329)
- (arXiv 2025.05) Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition,  [[Paper]](https://arxiv.org/pdf/2505.05829),[[Code]](https://github.com/ccccczzy/icc)
- (arXiv 2025.05) Generative Pre-trained Autoregressive Diffusion Transformer,  [[Paper]](https://arxiv.org/pdf/2505.07344)
- (arXiv 2025.05) TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation,  [[Paper]](https://arxiv.org/pdf/2505.09140),[[Code]](https://github.com/Zechao-Guan/TopoDiT-3D)
- (arXiv 2025.05) Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis,  [[Paper]](https://arxiv.org/pdf/2505.10046),[[Code]](https://github.com/tang-bd/fuse-dit)
- (arXiv 2025.05) No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves,  [[Paper]](https://arxiv.org/pdf/2505.02831),[[Code]](https://github.com/vvvvvjdy/SRA)
- (arXiv 2025.05) FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation,  [[Paper]](https://arxiv.org/pdf/2505.20353),[[Code]](https://github.com/NoakLiu/FastCache-xDiT)
- (arXiv 2025.06) HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer,  [[Paper]](https://arxiv.org/pdf/2505.22705),[[Code]](https://github.com/HiDream-ai/HiDream-I1)
- (arXiv 2025.06) DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2505.21541),[[Code]](https://github.com/Wangzt1121/DiffDecompose)
- (arXiv 2025.06) Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas,  [[Paper]](https://arxiv.org/pdf/2506.03275)
- (arXiv 2025.06) Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2506.03065),[[Code]](https://github.com/Peyton-Chen/Sparse-vDiT)
- (arXiv 2025.06) Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2506.05096),[[Code]](https://astraea-project.github.io/ASTRAEA/)
- (arXiv 2025.06) FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2506.04213),[[Code]](https://fulldit2.github.io/)
- (arXiv 2025.06) Playing with Transformer at 30+ FPS via Next-Frame Diffusion,  [[Paper]](https://arxiv.org/pdf/2506.01380)
- (arXiv 2025.06) SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2506.00830)
- (arXiv 2025.06) EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering,  [[Paper]](https://arxiv.org/pdf/2505.24417),[[Code]](https://github.com/songyiren725/EasyText)
- (arXiv 2025.06) MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation,  [[Paper]](https://arxiv.org/pdf/2506.07999)
- (arXiv 2025.06) Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2506.07986),[[Code]](https://github.com/Vchitect/TACA)
- (arXiv 2025.06) Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces,  [[Paper]](https://arxiv.org/pdf/2506.07903)
- (arXiv 2025.06) Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression,  [[Paper]](https://arxiv.org/pdf/2506.09482),[[Code]](https://github.com/TransDiff/TransDiff)
- (arXiv 2025.06) EraserDiT: Fast Video Inpainting with Diffusion Transformer Model,  [[Paper]](https://arxiv.org/pdf/2506.12853),[[Code]](https://jieliu95.github.io/EraserDiT_demo/)
- (arXiv 2025.06) Emergent Temporal Correspondences from Video Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2506.17220),[[Code]](https://github.com/cvlab-kaist/DiffTrack)
- (arXiv 2025.06) Video Virtual Try-on with Conditional Diffusion Transformer Inpainter,  [[Paper]](https://arxiv.org/pdf/2506.21270)
- (arXiv 2025.06) XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation,  [[Paper]](https://arxiv.org/pdf/2506.21416),[[Code]](https://github.com/bytedance/XVerse)
- (arXiv 2025.06) TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation,  [[Paper]](https://arxiv.org/pdf/2506.21681)
- (arXiv 2025.06) OutDreamer: Video Outpainting with a Diffusion Transformer,  [[Paper]](https://arxiv.org/pdf/2506.22298)
- (arXiv 2025.07) Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2507.08422)
- (arXiv 2025.07) FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2507.12956),[[Code]](https://github.com/Fantasy-AMAP/fantasy-portrait)
- (arXiv 2025.07) Taming Diffusion Transformer for Real-Time Mobile Video Generation,  [[Paper]](https://arxiv.org/pdf/2507.13343),[[Code]](https://snap-research.github.io/mobile_video_dit/)
- (arXiv 2025.07) SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging,  [[Paper]](https://arxiv.org/pdf/2507.15595),[[Code]](https://github.com/Bekhouche/SegDT)
- (arXiv 2025.07) AnimeColor: Reference-based Animation Colorization with Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2507.20158),[[Code]](https://github.com/IamCreateAI/AnimeColor)
- (arXiv 2025.08) LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer,  [[Paper]](https://arxiv.org/pdf/2508.00477),[[Code]](https://github.com/Suchenl/LAMIC)
- (arXiv 2025.08) DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework,  [[Paper]](https://arxiv.org/pdf/2508.02807),[[Code]](https://github.com/Virtu-Lab/DreamVVT)
- (arXiv 2025.08) Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation,  [[Paper]](https://arxiv.org/pdf/2508.04016),[[Code]](https://github.com/wlfeng0509/s2q-vdit)
- (arXiv 2025.08) FLUX-Makeup: High-Fidelity, Identity-Consistent, and Robust Makeup Transfer via Diffusion Transformer,  [[Paper]](https://arxiv.org/pdf/2508.05069)
- (arXiv 2025.08) Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off,  [[Paper]](https://arxiv.org/pdf/2508.04825),[[Code]](https://github.com/nxnai/Voost)
- (arXiv 2025.08) RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer,  [[Paper]](https://arxiv.org/pdf/2508.05115)
- (arXiv 2025.08) UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation,  [[Paper]](https://arxiv.org/pdf/2508.05399),[[Code]](https://github.com/furiosa-ai/uncage)
- (arXiv 2025.08) KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training,  [[Paper]](https://arxiv.org/pdf/2508.06001),[[Code]](https://github.com/Kai-46/KnapFormer/)
- (arXiv 2025.08) MuGa-VTON: Multi-Garment Virtual Try-On via Diffusion Transformers with Prompt Customization,  [[Paper]](https://arxiv.org/pdf/2508.08488)
- (arXiv 2025.08) Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer,  [[Paper]](https://arxiv.org/pdf/2508.09131),[[Code]](https://zxyin.github.io/ColorCtrl/)
- (arXiv 2025.08) Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing,  [[Paper]](https://arxiv.org/pdf/2508.07519),[[Code]](https://github.com/SNU-VGILab/exploring-mmdit)
- (arXiv 2025.08) LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation,  [[Paper]](https://arxiv.org/pdf/2508.07603),[[Code]](https://github.com/ssugarwh/LaVieID)
- (arXiv 2025.08) Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers,  [[Paper]](https://arxiv.org/pdf/2508.07246),[[Code]](https://maxin-cn.github.io/miramo)
- (arXiv 2025.08) HiMat: DiT-based Ultra-High Resolution SVBRDF Generation,  [[Paper]](https://arxiv.org/pdf/2508.07011)
- (arXiv 2025.08) DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation, [[Paper]](https://arxiv.org/pdf/2508.06511),[[Code]](https://thenameishope.github.io/DiTalker/)
- (arXiv 2025.08) MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2508.09709)
- (arXiv 2025.08) MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration, [[Paper]](https://arxiv.org/pdf/2508.12691)
- (arXiv 2025.09) Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders, [[Paper]](https://arxiv.org/pdf/2509.09547)
- (arXiv 2025.09) SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching, [[Paper]](https://arxiv.org/pdf/2509.11628),[[Code]](https://github.com/Shenyi-Z/Cache4Diffusion)
- (arXiv 2025.09) BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching, [[Paper]](https://arxiv.org/pdf/2509.13789)
- (arXiv 2025.09) LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence, [[Paper]](https://arxiv.org/pdf/2509.12203)
- (arXiv 2025.09) FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2509.16518),[[Code]](https://github.com/sankeerth95/FG-Attn)
- (arXiv 2025.09) Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2509.18096),[[Code]](https://github.com/cvlab-kaist/Seg4Diff)
- (arXiv 2025.09) OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models, [[Paper]](https://arxiv.org/pdf/2509.17627),[[Code]](https://github.com/Phantom-video/OmniInsert)
- (arXiv 2025.09) DiffInk: Glyph- and Style-Aware Latent Diffusion Transformer for Text to Online Handwriting Generation, [[Paper]](https://arxiv.org/pdf/2509.23624)
- (arXiv 2025.09) QuantSparse: Comprehensively Compressing Video Diffusion Transformer with Model Quantization and Attention Sparsification, [[Paper]](https://arxiv.org/pdf/2509.23681),[[Code]](https://github.com/wlfeng0509/QuantSparse)
- (arXiv 2025.09) Stitch: Training-Free Position Control in Multimodal Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2509.26644),[[Code]](https://github.com/ExplainableML/Stitch)
- (arXiv 2025.09) LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing, [[Paper]](https://arxiv.org/pdf/2509.25731)
