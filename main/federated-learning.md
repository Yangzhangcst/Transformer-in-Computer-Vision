### Federated Learning 
- (arXiv 2022.11) FedTune: A Deep Dive into Efficient Federated Fine-Tuning with Pre-trained Transformers, [[Paper]](https://arxiv.org/pdf/2211.08025.pdf)
- (arXiv 2023.06) FeSViBS: Federated Split Learning of Vision Transformer with Block Sampling, [[Paper]](https://arxiv.org/pdf/2306.14638.pdf),[[Code]](https://github.com/faresmalik/FeSViBS)
- (arXiv 2023.08) Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning, [[Paper]](https://arxiv.org/pdf/2308.04373.pdf)
- (arXiv 2023.08) FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning, [[Paper]](https://arxiv.org/pdf/2308.09160.pdf), [[Code]](https://github.com/imguangyu/FedPerfix)
- (arXiv 2024.01) OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated learning, [[Paper]](https://arxiv.org/pdf/2401.11652.pdf)
- (arXiv 2024.03) A General and Efficient Federated Split Learning with Pre-trained Image Transformers for Heterogeneous Data, [[Paper]](https://arxiv.org/pdf/2403.16050.pdf)
- (arXiv 2024.04) Towards Multi-modal Transformers in Federated Learning, [[Paper]](https://arxiv.org/pdf/2404.12467.pdf)
- (arXiv 2024.12) EFTViT: Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Edge Devices, [[Paper]](https://arxiv.org/pdf/2412.00334.pdf)
