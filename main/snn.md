### SNN
- (arXiv 2022.10) Spikformer: When Spiking Neural Network Meets Transformer,  [[Paper]](https://arxiv.org/abs/2209.15425)
- (arxiv 2023.07) Spike-driven Transformer,  [[Paper]](https://arxiv.org/abs/2307.01694), [[Code]](https://github.com/BICLab/Spike-Driven-Transformer)
- (arxiv 2023.08) SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition,  [[Paper]](https://arxiv.org/pdf/2308.04369.pdf), [[Code]](https://github.com/Event-AHU/SSTFormer)
- (arxiv 2023.08) Attention-free Spikformer: Mixing Spike Sequences with Simple Linear Transforms,  [[Paper]](https://arxiv.org/pdf/2308.02557.pdf)
- (arxiv 2023.11) SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer,  [[Paper]](https://arxiv.org/pdf/2311.08806.pdf)
- (arxiv 2023.11) Spiking Neural Networks with Dynamic Time Steps for Vision Transformers,  [[Paper]](https://arxiv.org/pdf/2311.16456.pdf)
- (arxiv 2024.01) Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN Ticket,  [[Paper]](https://arxiv.org/pdf/2401.02020.pdf)
- (arxiv 2024.01) TIM: An Efficient Temporal Interaction Module for Spiking Transformer,  [[Paper]](https://arxiv.org/pdf/2401.11687.pdf)
- (arxiv 2024.02) Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with Parallel Spike-driven Transformer,  [[Paper]](https://arxiv.org/pdf/2402.04798.pdf)
- (arxiv 2024.02) SDiT: Spiking Diffusion Model with Transformer,  [[Paper]](https://arxiv.org/pdf/2402.11588.pdf)
- (arxiv 2024.03) SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks,  [[Paper]](https://arxiv.org/pdf/2403.14302.pdf), [[Code]](https://github.com/xyshi2000/SpikingResformer)
- (arxiv 2024.03) QKFormer: Hierarchical Spiking Transformer using Q-K Attention,  [[Paper]](https://arxiv.org/pdf/2403.16552.pdf), [[Code]](https://github.com/zhouchenlin2096/QKFormer)
- (arxiv 2024.03) Fourier or Wavelet bases as counterpart self-attention in spikformer for efficient visual classification,  [[Paper]](https://arxiv.org/pdf/2403.18228.pdf)
- (arxiv 2024.04) Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips,  [[Paper]](https://arxiv.org/pdf/2404.03663.pdf), [[Code]](https://github.com/BICLab/Spike-Driven-Transformer-V2)
- (arxiv 2024.04) A Novel Spike Transformer Network for Depth Estimation from Event Cameras via Cross-modality Knowledge Distillation,  [[Paper]](https://arxiv.org/pdf/2404.17335.pdf)
- (arxiv 2024.06) SVFormer: A Direct Training Spiking Transformer for Efficient Video Action Recognition, [[Paper]](https://arxiv.org/pdf/2406.15034.pdf)
- (arxiv 2024.07) Spiking Tucker Fusion Transformer for Audio-Visual Zero-Shot Learning, [[Paper]](https://arxiv.org/pdf/2407.08130.pdf)
- (arxiv 2024.08) AT-SNN: Adaptive Tokens for Vision Transformer on Spiking Neural Network, [[Paper]](https://arxiv.org/pdf/2408.12293.pdf)
- (arxiv 2024.09) DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention, [[Paper]](https://arxiv.org/pdf/2409.15375.pdf)
- (arxiv 2024.11) Scaling Spike-driven Transformer with Efficient Spike Firing Approximation Training, [[Paper]](https://arxiv.org/pdf/2411.16061.pdf), [[Code]](https://github.com/BICLab/Spike-Driven-Transformer-V3)
- (arxiv 2024.12) Hybrid Spiking Neural Network -- Transformer Video Classification Model, [[Paper]](https://arxiv.org/pdf/2412.00237.pdf), [[Code]](https://github.com/TheRNB/HyTSSN/tree/main)
- (arxiv 2024.12) Efficient Event-based Semantic Segmentation with Spike-driven Lightweight Transformer-based Networks, [[Paper]](https://arxiv.org/pdf/2412.12843.pdf)
- (arxiv 2024.12) Spike2Former: Efficient Spiking Transformer for High-performance Image Segmentation, [[Paper]](https://arxiv.org/pdf/2412.14587.pdf), [[Code]](https://github.com/BICLab/Spike2Former)
- (arxiv 2025.01) Binary Event-Driven Spiking Transformer, [[Paper]](https://arxiv.org/pdf/2501.05904)
- (arxiv 2025.01) Quantized Spike-driven Transformer, [[Paper]](https://arxiv.org/pdf/2501.13492), [[Code]](https://github.com/bollossom/QSD-Transformer/tree/main)
- (arxiv 2025.02) Spiking Vision Transformer with Saccadic Attention, [[Paper]](https://arxiv.org/pdf/2502.12677)
- (arxiv 2025.02) Towards High-performance Spiking Transformers from ANN to SNN Conversion, [[Paper]](https://arxiv.org/pdf/2502.21193), [[Code]](https://github.com/h-z-h-cell/Transformer-to-SNN-ECMT)
- (arxiv 2025.03) Spiking Transformer:Introducing Accurate Addition-Only Spiking Self-Attention for Transformer, [[Paper]](https://arxiv.org/pdf/2503.00226)
- (arxiv 2025.03) SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition, [[Paper]](https://arxiv.org/pdf/2503.15986)
- (arxiv 2025.05) Hybrid Spiking Vision Transformer for Object Detection with Event Cameras, [[Paper]](https://arxiv.org/pdf/2505.07715)
- (arxiv 2025.05) SpikeVideoFormer: An Efficient Spike-Driven Video Transformer with Hamming Attention and  Complexity, [[Paper]](https://arxiv.org/pdf/2505.10352), [[Code]](https://github.com/JimmyZou/SpikeVideoFormer)
- (arxiv 2025.08) STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers, [[Paper]](https://arxiv.org/pdf/2508.00387)
- (arxiv 2025.08) STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers, [[Paper]](https://arxiv.org/pdf/2508.14138)
- (arxiv 2025.09) CSDformer: A Conversion Method for Fully Spike-Driven Transformer, [[Paper]](https://arxiv.org/pdf/2509.17461)
