### Gaze
- (arXiv 2021.06) Gaze Estimation using Transformer, [[Paper]](https://arxiv.org/pdf/2105.14424.pdf), [[Code]](https://github.com/yihuacheng/GazeTR)
- (arXiv 2022.03) End-to-End Human-Gaze-Target Detection with Transformers, [[Paper]](https://arxiv.org/pdf/2203.10433.pdf)
- (arXiv 2022.05) Eye-gaze-guided Vision Transformer for Rectifying Shortcut Learning, [[Paper]](https://arxiv.org/pdf/2205.12466.pdf)
- (arXiv 2022.08) In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation, [[Paper]](https://arxiv.org/pdf/2208.04464.pdf), [[Code]](https://bolinlai.github.io/GLC-EgoGazeEst)
- (arXiv 2022.09) MGTR: End-to-End Mutual Gaze Detection with Transformer, [[Paper]](https://arxiv.org/pdf/2209.10930.pdf), [[Code]](https://github.com/Gmbition/MGTR)
- (arXiv 2023.08) Interaction-aware Joint Attention Estimation Using People Attributes, [[Paper]](https://arxiv.org/pdf/2308.05382.pdf), [[Code]](https://github.com/chihina/PJAE)
- (arXiv 2023.08) DVGaze: Dual-View Gaze Estimation, [[Paper]](https://arxiv.org/pdf/2308.10310.pdf), [[Code]](https://github.com/yihuacheng/DVGaze)
- (arXiv 2023.10) Sharingan: A Transformer-based Architecture for Gaze Following, [[Paper]](https://arxiv.org/pdf/2310.00816.pdf)
- (arXiv 2023.11) Dual input stream transformer for eye-tracking line assignment, [[Paper]](https://arxiv.org/pdf/2311.06095.pdf)
- (arXiv 2024.01) GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance, [[Paper]](https://arxiv.org/pdf/2401.00260.pdf)
- (arXiv 2024.01) EmMixformer: Mix transformer for eye movement recognition, [[Paper]](https://arxiv.org/pdf/2401.04956.pdf)
- (arXiv 2024.02) TransGOP: Transformer-Based Gaze Object Prediction, [[Paper]](https://arxiv.org/pdf/2402.13578.pdf), [[Code]](https://github.com/chenxi-Guo/TransGOP)
- (arXiv 2024.03) ViTGaze: Gaze Following with Interaction Features in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.12778.pdf), [[Code]](https://github.com/hustvl/ViTGaze)
- (arXiv 2024.04) Denoising Distillation Makes Event-Frame Transformers as Accurate Gaze Trackers, [[Paper]](https://arxiv.org/pdf/2404.00548.pdf), [[Code]](https://github.com/jdjdli/Denoise_distill_EF_gazetracker)
- (arXiv 2024.05) Gaze-DETR: Using Expert Gaze to Reduce False Positives in Vulvovaginal Candidiasis Screening, [[Paper]](https://arxiv.org/pdf/2405.09463.pdf), [[Code]](https://github.com/YanKong0408/Gaze-DETR)
- (arXiv 2024.07) OAT: Object-Level Attention Transformer for Gaze Scanpath Prediction, [[Paper]](https://arxiv.org/pdf/2407.13335.pdf), [[Code]](https://github.com/HKUST-NISL/oat_eccv24)
- (arXiv 2025.07) Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers, [[Paper]](https://arxiv.org/pdf/2507.15833.pdf), [[Code]](https://ian-chuang.github.io/gaze-av-aloha/)
- (arXiv 2025.08) GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations, [[Paper]](https://arxiv.org/pdf/2508.12966.pdf)
