
### Pruning & Quantization
- (arXiv 2021.04) Visual Transformer Pruning, [[Paper]](https://arxiv.org/pdf/2104.08500.pdf)
- (arXiv 2021.06) Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.14156.pdf)
- (arXiv 2021.11) PTQ4ViT: Post-Training Quantization Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12293.pdf), [[Code]](https://github.com/hahnyuan/PTQ4ViT)
- (arXiv 2021.11) FQ-ViT: Fully Quantized Vision Transformer without Retraining, [[Paper]](https://arxiv.org/pdf/2111.15127.pdf)
- (arXiv 2022.01) Q-ViT: Fully Differentiable Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.07703.pdf)
- (arXiv 2022.01) VAQF: Fully Automatic Software-hardware Co-design Framework for Low-bit Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.06618.pdf)
- (arXiv 2022.03) Patch Similarity Aware Data-Free Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.02250.pdf)
- (arXiv 2022.03) CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction, [[Paper]](https://arxiv.org/pdf/2203.04570.pdf)
- (arXiv 2022.07) I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference, [[Paper]](https://arxiv.org/pdf/2207.01405.pdf)
- (arXiv 2022.08) Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization, [[Paper]](https://arxiv.org/pdf/2208.05163.pdf)
- (arXiv 2022.09) PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.05687.pdf), [[Code]](https://github.com/zkkli/PSAQ-ViT)
- (arXiv 2022.10) EAPruning: Evolutionary Pruning for Vision Transformers and CNNs, [[Paper]](https://arxiv.org/pdf/2210.00181.pdf)
- (arXiv 2022.10) SaiT: Sparse Vision Transformers through Adaptive Token Pruning, [[Paper]](https://arxiv.org/pdf/2210.05832.pdf)
- (arXiv 2022.10) Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.06707.pdf), [[Code]](https://github.com/YanjingLi0202/Q-ViT)
- (arXiv 2022.10) oViT: An Accurate Second-Order Pruning Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.09223.pdf)
- (arXiv 2022.11) CPT-V: A Contrastive Approach to Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.09643.pdf)
- (arXiv 2022.11) NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.16056.pdf)
- (arXiv 2022.12) Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis, [[Paper]](https://arxiv.org/pdf/2212.03185.pdf), [[Code]](https://github.com/TencentARC/BasicVQ-GEN)
- (arXiv 2022.12) RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.08254.pdf)
- (arXiv 2023.02) Oscillation-free Quantization for Low-bit Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.02210.pdf)
- (arXiv 2023.03) Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction, [[Paper]](https://arxiv.org/pdf/2303.12557.pdf), [[Code]](https://github.com/Q-HyViT)
- (arXiv 2023.03) Scaled Quantization for the Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.13601.pdf)
- (arXiv 2023.03) Towards Accurate Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.14341.pdf)
- (arXiv 2023.04) Q-DETR: An Efficient Low-Bit Quantized Detection Transformer, [[Paper]](https://arxiv.org/pdf/2304.00253.pdf)
- (arXiv 2023.04) Attention Map Guided Transformer Pruning for Edge Device, [[Paper]](https://arxiv.org/pdf/2304.01452.pdf)
- (arXiv 2023.05) Patch-wise Mixed-Precision Quantization of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.06559.pdf)
- (arXiv 2023.05) Boost Vision Transformer with GPU-Friendly Sparsity and Quantization, [[Paper]](https://arxiv.org/pdf/2305.10727.pdf)
- (arXiv 2023.05) Bi-ViT: Pushing the Limit of Vision Transformer Quantization, [[Paper]](https://arxiv.org/pdf/2305.12354.pdf)
- (arXiv 2023.07) Variation-aware Vision Transformer Quantization, [[Paper]](https://arxiv.org/pdf/2307.00331.pdf), [[Code]](https://github.com/HuangOwen/VVTQ)
- (arXiv 2023.08) Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers,  [[Paper]](https://arxiv.org/pdf/2308.10814.pdf), [[Code]](https://github.com/enyac-group/evol-q)
- (arXiv 2023.08) Vision Transformer Pruning Via Matrix Decomposition,  [[Paper]](https://arxiv.org/pdf/2308.10839.pdf)
- (arXiv 2023.09) Transformer-VQ: Linear-Time Transformers via Vector Quantization,  [[Paper]](https://arxiv.org/pdf/2309.16354.pdf), [[Code]](https://github.com/transformer-vq/transformer_vq)
- (arXiv 2023.10) LLM-FP4: 4-Bit Floating-Point Quantized Transformers, [[Paper]](https://arxiv.org/pdf/2310.16836.pdf), [[Code]](https://github.com/nbasyl/LLM-FP4)
- (arXiv 2023.12) QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.02220.pdf)
- (arXiv 2024.01) LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation, [[Paper]](https://arxiv.org/pdf/2401.11243.pdf)
- (arXiv 2024.01) MPTQ-ViT: Mixed-Precision Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.14895.pdf)
- (arXiv 2024.03) Accelerating ViT Inference on FPGA through Static and Dynamic Pruning, [[Paper]](https://arxiv.org/pdf/2403.14047.pdf)
- (arXiv 2024.04) Instance-Aware Group Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.00928.pdf), [[Code]](https://cvlab.yonsei.ac.kr/projects/IGQ-ViT/)
- (arXiv 2024.04) Data-independent Module-aware Pruning for Hierarchical Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.13648.pdf), [[Code]](https://github.com/he-y/Data-independent-Module-Aware-Pruning)
- (arXiv 2024.05) Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey, [[Paper]](https://arxiv.org/pdf/2405.00314.pdf), [[Code]](https://github.com/DD-DuDa/awesome-vit-quantization-acceleration)
- (arXiv 2024.05) Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2405.03882.pdf)
- (arXiv 2024.05) MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization, [[Paper]](https://arxiv.org/pdf/2405.17873.pdf)
- (arXiv 2024.06) ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation, [[Paper]](https://arxiv.org/pdf/2406.02540.pdf), [[Code]](https://github.com/A-suozhang/ViDiT-Q)
- (arXiv 2024.06) MGRQ: Post-Training Quantization For Vision Transformer With Mixed Granularity Reconstruction, [[Paper]](https://arxiv.org/pdf/2406.09229.pdf)
- (arXiv 2024.06) An Analysis on Quantizing Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2406.11100.pdf)
- (arXiv 2024.06) ViT-1.58b: Mobile Vision Transformers in the 1-bit Era, [[Paper]](https://arxiv.org/pdf/2406.18051)
- (arXiv 2024.06) Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2406.17343), [[Code]](https://github.com/Juanerx/Q-DiT)
- (arXiv 2024.07) ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.02763)
- (arXiv 2024.07) LPViT: Low-Power Semi-structured Pruning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.02068)
- (arXiv 2024.07) Isomorphic Pruning for Vision Models, [[Paper]](https://arxiv.org/pdf/2407.04616), [[Code]](https://github.com/VainF/Isomorphic-Pruning)
- (arXiv 2024.07) LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order, [[Paper]](https://arxiv.org/pdf/2407.04513)
- (arXiv 2024.07) Fisher-aware Quantization for DETR Detectors with Critical-category Objectives, [[Paper]](https://arxiv.org/pdf/2407.03442)
- (arXiv 2024.07) PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference, [[Paper]](https://arxiv.org/pdf/2407.05010), [[Code]](https://github.com/ChildTang/PRANCE)
- (arXiv 2024.07) CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs, [[Paper]](https://arxiv.org/pdf/2407.05266), [[Code]](https://github.com/georgia-tech-synergy-lab/CLAMP-ViT.git)
- (arXiv 2024.07) Reducing Vision Transformer Latency on Edge Devices via GPU Tail Effect and Training-free Token Pruning, [[Paper]](https://arxiv.org/pdf/2407.05941)
- (arXiv 2024.07) ERQ: Error Reduction for Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.06794)
- (arXiv 2024.07) AdaLog: Post-Training Quantization for Vision Transformers with Adaptive Logarithm Quantizer, [[Paper]](https://arxiv.org/pdf/2407.12951), [[Code]](https://github.com/GoatWu/AdaLog)
- (arXiv 2024.07) Mixed Non-linear Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.18437), [[Code]](https://gitlab.com/ones-ai/mixed-non-linear-quantization)
- (arXiv 2024.07) MimiQ: Low-Bit Data-Free Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.20021)
- (arXiv 2024.08) DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2408.03291)
- (arXiv 2024.08) Task-Aware Dynamic Transformer for Efficient Arbitrary-Scale Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2408.08736)
- (arXiv 2024.08) PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications, [[Paper]](https://arxiv.org/pdf/2408.08437), [[Code]](https://github.com/kshitij11/PQV-Mobile)
- (arXiv 2024.08) Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers, [[Paper]](https://arxiv.org/pdf/2408.12568), [[Code]](https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch)
- (arXiv 2024.08) VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2408.17131)
- (arXiv 2024.10) ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge Devices, [[Paper]](https://arxiv.org/pdf/2410.11650)
- (arXiv 2024.10) Token Pruning using a Lightweight Background Aware Vision Transformer, [[Paper]](https://arxiv.org/pdf/2410.09324)
