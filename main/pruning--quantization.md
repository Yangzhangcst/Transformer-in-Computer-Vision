
### Pruning & Quantization
- (arXiv 2021.04) Visual Transformer Pruning, [[Paper]](https://arxiv.org/pdf/2104.08500.pdf)
- (arXiv 2021.06) Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.14156.pdf)
- (arXiv 2021.11) PTQ4ViT: Post-Training Quantization Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12293.pdf), [[Code]](https://github.com/hahnyuan/PTQ4ViT)
- (arXiv 2021.11) FQ-ViT: Fully Quantized Vision Transformer without Retraining, [[Paper]](https://arxiv.org/pdf/2111.15127.pdf)
- (arXiv 2022.01) Q-ViT: Fully Differentiable Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.07703.pdf)
- (arXiv 2022.01) VAQF: Fully Automatic Software-hardware Co-design Framework for Low-bit Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.06618.pdf)
- (arXiv 2022.03) Patch Similarity Aware Data-Free Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.02250.pdf)
- (arXiv 2022.03) CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction, [[Paper]](https://arxiv.org/pdf/2203.04570.pdf)
- (arXiv 2022.07) I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference, [[Paper]](https://arxiv.org/pdf/2207.01405.pdf)
- (arXiv 2022.08) Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization, [[Paper]](https://arxiv.org/pdf/2208.05163.pdf)
- (arXiv 2022.09) PSAQ-ViT V2: Towards Accurate and General Data-Free Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.05687.pdf), [[Code]](https://github.com/zkkli/PSAQ-ViT)
- (arXiv 2022.10) EAPruning: Evolutionary Pruning for Vision Transformers and CNNs, [[Paper]](https://arxiv.org/pdf/2210.00181.pdf)
- (arXiv 2022.10) SaiT: Sparse Vision Transformers through Adaptive Token Pruning, [[Paper]](https://arxiv.org/pdf/2210.05832.pdf)
- (arXiv 2022.10) Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.06707.pdf), [[Code]](https://github.com/YanjingLi0202/Q-ViT)
- (arXiv 2022.10) oViT: An Accurate Second-Order Pruning Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.09223.pdf)
- (arXiv 2022.11) CPT-V: A Contrastive Approach to Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.09643.pdf)
- (arXiv 2022.11) NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.16056.pdf)
- (arXiv 2022.12) Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis, [[Paper]](https://arxiv.org/pdf/2212.03185.pdf), [[Code]](https://github.com/TencentARC/BasicVQ-GEN)
- (arXiv 2022.12) RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.08254.pdf)
- (arXiv 2023.02) Oscillation-free Quantization for Low-bit Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.02210.pdf)
- (arXiv 2023.03) Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction, [[Paper]](https://arxiv.org/pdf/2303.12557.pdf), [[Code]](https://github.com/Q-HyViT)
- (arXiv 2023.03) Scaled Quantization for the Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.13601.pdf)
- (arXiv 2023.03) Towards Accurate Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.14341.pdf)
- (arXiv 2023.04) Q-DETR: An Efficient Low-Bit Quantized Detection Transformer, [[Paper]](https://arxiv.org/pdf/2304.00253.pdf)
- (arXiv 2023.04) Attention Map Guided Transformer Pruning for Edge Device, [[Paper]](https://arxiv.org/pdf/2304.01452.pdf)
- (arXiv 2023.05) Patch-wise Mixed-Precision Quantization of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.06559.pdf)
- (arXiv 2023.05) Boost Vision Transformer with GPU-Friendly Sparsity and Quantization, [[Paper]](https://arxiv.org/pdf/2305.10727.pdf)
- (arXiv 2023.05) Bi-ViT: Pushing the Limit of Vision Transformer Quantization, [[Paper]](https://arxiv.org/pdf/2305.12354.pdf)
- (arXiv 2023.07) Variation-aware Vision Transformer Quantization, [[Paper]](https://arxiv.org/pdf/2307.00331.pdf), [[Code]](https://github.com/HuangOwen/VVTQ)
- (arXiv 2023.08) Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers,  [[Paper]](https://arxiv.org/pdf/2308.10814.pdf), [[Code]](https://github.com/enyac-group/evol-q)
- (arXiv 2023.08) Vision Transformer Pruning Via Matrix Decomposition,  [[Paper]](https://arxiv.org/pdf/2308.10839.pdf)
- (arXiv 2023.09) Transformer-VQ: Linear-Time Transformers via Vector Quantization,  [[Paper]](https://arxiv.org/pdf/2309.16354.pdf), [[Code]](https://github.com/transformer-vq/transformer_vq)
- (arXiv 2023.10) LLM-FP4: 4-Bit Floating-Point Quantized Transformers, [[Paper]](https://arxiv.org/pdf/2310.16836.pdf), [[Code]](https://github.com/nbasyl/LLM-FP4)
- (arXiv 2023.12) QuantAttack: Exploiting Dynamic Quantization to Attack Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.02220.pdf)
- (arXiv 2024.01) LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation, [[Paper]](https://arxiv.org/pdf/2401.11243.pdf)
- (arXiv 2024.01) MPTQ-ViT: Mixed-Precision Post-Training Quantization for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.14895.pdf)
- (arXiv 2024.03) Accelerating ViT Inference on FPGA through Static and Dynamic Pruning, [[Paper]](https://arxiv.org/pdf/2403.14047.pdf)
- (arXiv 2024.04) Instance-Aware Group Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.00928.pdf), [[Code]](https://cvlab.yonsei.ac.kr/projects/IGQ-ViT/)
- (arXiv 2024.04) Data-independent Module-aware Pruning for Hierarchical Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.13648.pdf), [[Code]](https://github.com/he-y/Data-independent-Module-Aware-Pruning)
- (arXiv 2024.05) Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey, [[Paper]](https://arxiv.org/pdf/2405.00314.pdf), [[Code]](https://github.com/DD-DuDa/awesome-vit-quantization-acceleration)
- (arXiv 2024.05) Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2405.03882.pdf)
- (arXiv 2024.05) MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization, [[Paper]](https://arxiv.org/pdf/2405.17873.pdf)
- (arXiv 2024.06) ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation, [[Paper]](https://arxiv.org/pdf/2406.02540.pdf), [[Code]](https://github.com/A-suozhang/ViDiT-Q)
- (arXiv 2024.06) MGRQ: Post-Training Quantization For Vision Transformer With Mixed Granularity Reconstruction, [[Paper]](https://arxiv.org/pdf/2406.09229.pdf)
- (arXiv 2024.06) An Analysis on Quantizing Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2406.11100.pdf)
- (arXiv 2024.06) ViT-1.58b: Mobile Vision Transformers in the 1-bit Era, [[Paper]](https://arxiv.org/pdf/2406.18051)
- (arXiv 2024.06) Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2406.17343), [[Code]](https://github.com/Juanerx/Q-DiT)
- (arXiv 2024.07) ADFQ-ViT: Activation-Distribution-Friendly Post-Training Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.02763)
- (arXiv 2024.07) LPViT: Low-Power Semi-structured Pruning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.02068)
- (arXiv 2024.07) Isomorphic Pruning for Vision Models, [[Paper]](https://arxiv.org/pdf/2407.04616), [[Code]](https://github.com/VainF/Isomorphic-Pruning)
- (arXiv 2024.07) LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing Layer Execution Order, [[Paper]](https://arxiv.org/pdf/2407.04513)
- (arXiv 2024.07) Fisher-aware Quantization for DETR Detectors with Critical-category Objectives, [[Paper]](https://arxiv.org/pdf/2407.03442)
- (arXiv 2024.07) PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference, [[Paper]](https://arxiv.org/pdf/2407.05010), [[Code]](https://github.com/ChildTang/PRANCE)
- (arXiv 2024.07) CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs, [[Paper]](https://arxiv.org/pdf/2407.05266), [[Code]](https://github.com/georgia-tech-synergy-lab/CLAMP-ViT.git)
- (arXiv 2024.07) Reducing Vision Transformer Latency on Edge Devices via GPU Tail Effect and Training-free Token Pruning, [[Paper]](https://arxiv.org/pdf/2407.05941)
- (arXiv 2024.07) ERQ: Error Reduction for Post-Training Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.06794)
- (arXiv 2024.07) AdaLog: Post-Training Quantization for Vision Transformers with Adaptive Logarithm Quantizer, [[Paper]](https://arxiv.org/pdf/2407.12951), [[Code]](https://github.com/GoatWu/AdaLog)
- (arXiv 2024.07) Mixed Non-linear Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.18437), [[Code]](https://gitlab.com/ones-ai/mixed-non-linear-quantization)
- (arXiv 2024.07) MimiQ: Low-Bit Data-Free Quantization of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.20021)
- (arXiv 2024.08) DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2408.03291)
- (arXiv 2024.08) Task-Aware Dynamic Transformer for Efficient Arbitrary-Scale Image Super-Resolution, [[Paper]](https://arxiv.org/pdf/2408.08736)
- (arXiv 2024.08) PQV-Mobile: A Combined Pruning and Quantization Toolkit to Optimize Vision Transformers for Mobile Applications, [[Paper]](https://arxiv.org/pdf/2408.08437), [[Code]](https://github.com/kshitij11/PQV-Mobile)
- (arXiv 2024.08) Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune CNNs and Transformers, [[Paper]](https://arxiv.org/pdf/2408.12568), [[Code]](https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch)
- (arXiv 2024.08) VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2408.17131)
- (arXiv 2024.10) ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge Devices, [[Paper]](https://arxiv.org/pdf/2410.11650)
- (arXiv 2024.10) Token Pruning using a Lightweight Background Aware Vision Transformer, [[Paper]](https://arxiv.org/pdf/2410.09324)
- (arXiv 2024.12) Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit Post-Training Quantization in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2412.14633)
- (arXiv 2024.12) Semantics Prompting Data-Free Quantization for Low-Bit Vision Transformers, [[Paper]](https://arxiv.org/pdf/2412.16553)
- (arXiv 2024.12) MBQ: Modality-Balanced Quantization for Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2412.19509), [[Code]](https://github.com/thu-nics/MBQ)
- (arXiv 2025.01) Mix-QViT: Mixed-Precision Vision Transformer Quantization Driven by Layer Importance and Quantization Sensitivity, [[Paper]](https://arxiv.org/pdf/2501.06357)
- (arXiv 2025.02) AIQViT: Architecture-Informed Post-Training Quantization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2502.04628)
- (arXiv 2025.02) Hardware-Friendly Static Quantization Method for Video Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2502.15077)
- (arXiv 2025.03) Oscillation-Reduced MXFP4 Training for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2502.20853), [[Code]](https://github.com/thu-ml/TetraJet-MXFP4Training)
- (arXiv 2025.03) FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2503.15465), [[Code]](https://github.com/cccrrrccc/FP4DiT)
- (arXiv 2025.04) APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2504.02508), [[Code]](https://github.com/GoatWu/APHQ-ViT)
- (arXiv 2025.04) The Effects of Grouped Structural Global Pruning of Vision Transformers on Domain Generalisation, [[Paper]](https://arxiv.org/pdf/2504.04196)
- (arXiv 2025.04) NuWa: Deriving Lightweight Task-Specific Vision Transformers for Edge Devices, [[Paper]](https://arxiv.org/pdf/2504.03118), [[Code]](https://anonymous.4open.science/r/Task_Specific-3A5E/README.md)
- (arXiv 2025.06) Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2505.22167), [[Code]](https://github.com/cantbebetter2/Q-VDiT)
- (arXiv 2025.06) GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2506.11784)
- (arXiv 2025.06) FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation, [[Paper]](https://arxiv.org/pdf/2506.11543), [[Code]](https://github.com/ShiheWang/FIMA-Q)
- (arXiv 2025.06) EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices, [[Paper]](https://arxiv.org/pdf/2506.11543)
- (arXiv 2025.07) DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning, [[Paper]](https://arxiv.org/pdf/2507.14481)
- (arXiv 2025.07) Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective, [[Paper]](https://arxiv.org/pdf/2507.19131)
- (arXiv 2025.07) Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2507.19175)
- (arXiv 2025.08) LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation, [[Paper]](https://arxiv.org/pdf/2508.03485)
- (arXiv 2025.09) Quantized Visual Geometry Grounded Transformer, [[Paper]](https://arxiv.org/pdf/2509.21302), [[Code]](https://github.com/wlfeng0509/QuantVGGT)
- (arXiv 2025.09) CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers, [[Paper]](https://arxiv.org/pdf/2509.24416),[[Code]](https://github.com/Kai-Liu001/CLQ)
