### Classification (Backbone)
- (ICLR'21) MODELING LONG-RANGE INTERACTIONS WITHOUT ATTENTION, [[Paper]](https://openreview.net/pdf?id=xTJEN-ggl1b), [[Code]](https://github.com/lucidrains/lambda-networks)
- (CVPR'20) Feature Pyramid Transformer, [[Paper]](https://arxiv.org/pdf/2007.09451), [[Code]](https://github.com/ZHANGDONG-NJUST/FPT)
- (ICLR'21) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, [[Paper]](https://arxiv.org/pdf/2010.11929), [[Code]](https://github.com/google-research/vision_transformer)
- (arXiv 2020.06) Visual Transformers: Token-based Image Representation and Processing for Computer Vision, [[Paper]](https://arxiv.org/pdf/2006.03677)
- (arXiv 2020.12) Training data-efficient image transformers & distillation through attention, [[Paper]](https://arxiv.org/abs/2012.12877), [[Code]](https://github.com/facebookresearch/deit)
- (arXiv 2021.01) Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet, [[Paper]](https://arxiv.org/pdf/2101.11986.pdf), [[Code]](https://github.com/yitu-opensource/T2T-ViT)
- (arXiv 2021.01) Bottleneck Transformers for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2101.11605.pdf) , [[Code]](https://github.com/leaderj1001/BottleneckTransformers)
- (arXiv.2021.02) Conditional Positional Encodings for Vision Transformers, [[Paper]](https://arxiv.org/abs/2102.10882), [[Code]](https://github.com/Meituan-AutoML/CPVT)
- (arXiv.2021.02) Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions, [[Paper]](https://arxiv.org/pdf/2102.12122.pdf), [[Code]](https://github.com/whai362/PVT)
- (arXiv 2021.03) Transformer in Transformer, [[Paper]](https://arxiv.org/pdf/2103.00112.pdf), [[Code]](https://github.com/huawei-noah/noah-research/tree/master/TNT) 
- (arXiv 2021.03) ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases, [[Paper]](https://arxiv.org/pdf/2103.10697.pdf), [[Code]](https://github.com/facebookresearch/convit)
- (arXiv 2021.03) Scalable Visual Transformers with Hierarchical Pooling, [[Paper]](https://arxiv.org/pdf/2103.10619.pdf)
- (arXiv 2021.03) Incorporating Convolution Designs into Visual Transformers, [[Paper]](https://arxiv.org/pdf/2103.11816.pdf)
- (arXiv 2021.03) DeepViT: Towards Deeper Vision Transformer, [[Paper]](https://arxiv.org/pdf/2103.11886.pdf), [[Code]](https://github.com/zhoudaquan/dvit_repo)
- (arXiv 2021.03) Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, [[Paper]](https://arxiv.org/pdf/2103.14030.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer) 
- (arXiv 2021.03) Understanding Robustness of Transformers for Image Classification, [[Paper]](https://arxiv.org/abs/2103.14586)
- (arXiv 2021.03) Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding, [[Paper]](https://arxiv.org/abs/2103.15358)
- (arXiv 2021.03) CvT: Introducing Convolutions to Vision Transformers, [[Paper]](https://arxiv.org/abs/2103.15808), [[Code]](https://github.com/leoxiaobin/CvT)
- (arXiv 2021.03) Rethinking Spatial Dimensions of Vision Transformers, [[Paper]](https://arxiv.org/abs/2103.16302), [[Code]](https://github.com/naver-ai/pit)
- (arXiv 2021.03) Going deeper with Image Transformers, [[Paper]](https://arxiv.org/abs/2103.17239)
- (arXiv 2021.04) LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference, [[Paper]](https://arxiv.org/abs/2104.01136)
- (arXiv 2021.04) On the Robustness of Vision Transformers to Adversarial Examples, [[Paper]](https://arxiv.org/abs/2104.02610)
- (arXiv 2021.04) LocalViT: Bringing Locality to Vision Transformers, [[Paper]](https://arxiv.org/abs/2104.05704), [[Code]](https://github.com/ofsoundof/LocalViT)
- (arXiv 2021.04) Escaping the Big Data Paradigm with Compact Transformers, [[Paper]](https://arxiv.org/abs/2104.05707), [[Code]](https://github.com/SHI-Labs/Compact-Transformers)
- (arXiv 2021.04) Co-Scale Conv-Attentional Image Transformers, [[Paper]](https://arxiv.org/abs/2104.06399), [[Code]](https://github.com/mlpc-ucsd/CoaT)
- (arXiv 2021.04) Token Labeling: Training a 85.5% Top-1 Accuracy Vision Transformer with 56M Parameters on ImageNet, [[Paper]](https://arxiv.org/pdf/2104.10858.pdf), [[Code]](https://github.com/zihangJiang/TokenLabeling)
- (arXiv 2021.04) So-ViT: Mind Visual Tokens for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2104.10935.pdf)
- (arXiv 2021.04) Multiscale Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.11227.pdf), [[Code]](https://github.com/facebookresearch/SlowFast)
- (arXiv 2021.04) Visformer: The Vision-friendly Transformer, [[Paper]](https://arxiv.org/pdf/2104.12533.pdf), [[Code]](https://github.com/danczs/Visformer)
- (arXiv 2021.04) Improve Vision Transformers Training by Suppressing Over-smoothing, [[Paper]](https://arxiv.org/pdf/2104.12753.pdf), [[Code]](https://github.com/ChengyueGongR/PatchVisionTransformer)
- (arXiv 2021.04) Twins: Revisiting the Design of Spatial Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.13840.pdf), [[Code]](https://github.com/Meituan-AutoML/Twins)
- (arXiv 2021.04) ConTNet: Why not use convolution and transformer at the same time, [[Paper]](https://arxiv.org/pdf/2104.13497.pdf), [[Code]](https://github.com/yanhao-tian/ConTNet)
- (arXiv 2021.05) Rethinking the Design Principles of Robust Vision Transformer, [[Paper]](https://arxiv.org/pdf/2105.07926.pdf), [[Code]](https://github.com/vtddggg/Robust-Vision-Transformer)
- (arXiv 2021.05) Vision Transformers are Robust Learners, [[Paper]](https://arxiv.org/pdf/2105.07581.pdf), [[Code]](https://git.io/J3VO0)
- (arXiv 2021.05) Rethinking Skip Connection with Layer Normalization in Transformers and ResNets, [[Paper]](https://arxiv.org/pdf/2105.07581.pdf), [[Code]](https://git.io/J3VO0)
- (arXiv 2021.05) Single-Layer Vision Transformers for More Accurate Early Exits with Less Overhead, [[Paper]](https://arxiv.org/pdf/2105.09121.pdf)
- (arXiv 2021.05) Intriguing Properties of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2105.10497.pdf), [[Code]](https://git.io/Js15X)
- (arXiv 2021.05) Aggregating Nested Transformers, [[Paper]](https://arxiv.org/pdf/2105.12723.pdf)
- (arXiv 2021.05) ResT: An Efficient Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2105.13677.pdf), [[Code]](https://github.com/wofmanaf/ResT)
- (arXiv 2021.06) DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification, [[Paper]](https://arxiv.org/pdf/2106.02034.pdf), [[Code]](https://github.com/raoyongming/DynamicViT)
- (arXiv 2021.06) When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations, [[Paper]](https://arxiv.org/pdf/2106.01548.pdf)
- (arXiv 2021.06) Container: Context Aggregation Network, [[Paper]](https://arxiv.org/pdf/2106.01401.pdf)
- (arXiv 2021.06) TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classication, [[Paper]](https://arxiv.org/pdf/2106.00908.pdf)
- (arXiv 2021.06) KVT: k-NN Attention for Boosting Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.00515.pdf)
- (arXiv 2021.06) MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens, [[Paper]](https://arxiv.org/pdf/2105.15168.pdf), [[Code]](https://github.com/hustvl/MSG-Transformer) 
- (arXiv 2021.06) Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length, [[Paper]](https://arxiv.org/pdf/2105.15075.pdf)
- (arXiv 2021.06) Less is More: Pay Less Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2105.14217.pdf)
- (arXiv 2021.06) FoveaTer: Foveated Transformer for Image Classification, [[Paper]](https://arxiv.org/pdf/2105.14173.pdf)
- (arXiv 2021.06) An Attention Free Transformer, [[Paper]](https://arxiv.org/pdf/2105.14103.pdf)
- (arXiv 2021.06) Glance-and-Gaze Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.02277.pdf), [[Code]](https://github.com/yucornetto/GG-Transformer)
- (arXiv 2021.06) RegionViT: Regional-to-Local Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.02689.pdf)
- (arXiv 2021.06) Chasing Sparsity in Vision Transformers: An End-to-End Exploration, [[Paper]](https://arxiv.org/pdf/2106.04533.pdf), [[Code]](https://github.com/VITA-Group/SViTE)
- (arXiv 2021.06) Scaling Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.04560.pdf)
- (arXiv 2021.06) CAT: Cross Attention in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.05786.pdf), [[Code]](https://github.com/linhezheng19/CAT)
- (arXiv 2021.06) On Improving Adversarial Transferability of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.04169.pdf), [[Code]](https://git.io/JZmG3)
- (arXiv 2021.06) Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight, [[Paper]](https://arxiv.org/pdf/2106.04263.pdf)
- (arXiv 2021.06) Patch Slimming for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.02852.pdf)
- (arXiv 2021.06) Transformer in Convolutional Neural Networks, [[Paper]](https://arxiv.org/pdf/2106.03180.pdf), [[Code]](https://github.com/yun-liu/TransCNN)
- (arXiv 2021.06) ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias, [[Paper]](https://arxiv.org/pdf/2106.03348.pdf), [[Code]](https://github.com/Annbless/ViTAE)
- (arXiv 2021.06) Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.03650.pdf)
- (arXiv 2021.06) Refiner: Refining Self-attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.03714.pdf)
- (arXiv 2021.06) Reveal of Vision Transformers Robustness against Adversarial Attacks, [[Paper]](https://arxiv.org/pdf/2106.03734.pdf)
- (arXiv 2021.06) Efficient Training of Visual Transformers with Small-Size Datasets, [[Paper]](https://arxiv.org/pdf/2106.03746.pdf)
- (arXiv 2021.06) Delving Deep into the Generalization of Vision Transformers under Distribution Shifts, [[Paper]](https://arxiv.org/pdf/2106.07617.pdf)
- (arXiv 2021.06) BEIT: BERT Pre-Training of Image Transformers, [[Paper]](https://arxiv.org/pdf/2106.08254.pdf), [[Code]](https://aka.ms/beit)
- (arXiv 2021.06) XCiT: Cross-Covariance Image Transformers, [[Paper]](https://arxiv.org/pdf/2106.09681.pdf), [[Code]](https://github.com/facebookresearch/xcit)
- (arXiv 2021.06) How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.10270.pdf), [[Code1]](https://github.com/google-research/vision_transformer), [[Code2]](https://github.com/rwightman/pytorch-image-models)
- (arXiv 2021.06) Exploring Vision Transformers for Fine-grained Classification, [[Paper]](https://arxiv.org/pdf/2106.10587.pdf), [[Code]](https://github.com/mv-lab/ViT-FGVC8)
- (arXiv 2021.06) TokenLearner: What Can 8 Learned Tokens Do for Images and Videos, [[Paper]](https://arxiv.org/pdf/2106.11297.pdf)
- (arXiv 2021.06) Exploring Corruption Robustness: Inductive Biases in Vision Transformers and MLP-Mixers, [[Paper]](https://arxiv.org/pdf/2106.13122.pdf), [[Code]](https://github.com/katelyn98/CorruptionRobustness)
- (arXiv 2021.06) VOLO: Vision Outlooker for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2106.13112.pdf), [[Code]](https://github.com/sail-sg/volo)
- (arXiv 2021.06) IA-RED2: Interpretability-Aware Redundancy Reduction for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.12620.pdf), [[Project]](http://people.csail.mit.edu/bpan/ia-red/)
- (arXiv 2021.06) PVTv2: Improved Baselines with Pyramid Vision Transformer, [[Paper]](https://arxiv.org/pdf/2106.13797.pdf), [[Code]](https://github.com/whai362/PVT)
- (arXiv 2021.06) Early Convolutions Help Transformers See Better, [[Paper]](https://arxiv.org/pdf/2106.14881.pdf)
- (arXiv 2021.06) Multi-Exit Vision Transformer for Dynamic Inference, [[Paper]](https://arxiv.org/pdf/2106.15183.pdf)
- (arXiv 2021.07) Augmented Shortcuts for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2106.15941.pdf)
- (arXiv 2021.07) Improving the Efficiency of Transformers for Resource-Constrained Devices, [[Paper]](https://arxiv.org/pdf/2106.16006.pdf)
- (arXiv 2021.07) CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows, [[Paper]](https://arxiv.org/pdf/2107.00652.pdf), [[Code]](https://github.com/microsoft/CSWin-Transformer)
- (arXiv 2021.07) Focal Self-attention for Local-Global Interactions in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.00641.pdf)
- (arXiv 2021.07) Cross-view Geo-localization with Evolving Transformer, [[Paper]](https://arxiv.org/pdf/2107.00842.pdf)
- (arXiv 2021.07) What Makes for Hierarchical Vision Transformer, [[Paper]](https://arxiv.org/pdf/2107.02174.pdf)
- (arXiv 2021.07) Efficient Vision Transformers via Fine-Grained Manifold Distillation, [[Paper]](https://arxiv.org/pdf/2107.01378.pdf)
- (arXiv 2021.07) Vision Xformers: Efficient Attention for Image Classification, [[Paper]](https://arxiv.org/pdf/2107.02239.pdf)
- (arXiv 2021.07) Long-Short Transformer: Efficient Transformers for Language and Vision, [[Paper]](https://arxiv.org/pdf/2107.02192.pdf)
- (arXiv 2021.07) Feature Fusion Vision Transformer for Fine-Grained Visual Categorization, [[Paper]](https://arxiv.org/pdf/2107.02341.pdf)
- (arXiv 2021.07) Local-to-Global Self-Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.04735.pdf), [[Code]](https://github.com/ljpadam/LG-Transformer)
- (arXiv 2021.07) Visual Parser: Representing Part-whole Hierarchies with Transformers, [[Paper]](https://arxiv.org/pdf/2107.05790.pdf), [[Code]](https://github.com/kevin-ssy/ViP)
- (arXiv 2021.07) CMT: Convolutional Neural Networks Meet Vision Transformers, [[Paper]](https://arxiv.org/pdf/2107.06263.pdf)
- (arXiv 2021.07) Combiner: Full Attention Transformer with Sparse Computation Cost, [[Paper]](https://arxiv.org/pdf/2107.05768.pdf)
- (arXiv 2021.07) A Comparison of Deep Learning Classification Methods on Small-scale Image Data set: from Convolutional Neural Networks to Visual Transformers, [[Paper]](https://arxiv.org/pdf/2107.07699.pdf)
- (arXiv 2021.07) Contextual Transformer Networks for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2107.12292.pdf), [[Code]](https://github.com/JDAI-CV/CoTNet)
- (arXiv 2021.07) Rethinking and Improving Relative Position Encoding for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2107.14222.pdf), [[Code]](https://github.com/microsoft/AutoML/tree/main/iRPE)
- (arXiv 2021.08) CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention, [[Paper]](https://arxiv.org/pdf/2108.00154.pdf), [[Code]](https://github.com/cheerss/CrossFormer)
- (arXiv 2021.08) Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer, [[Paper]](https://arxiv.org/pdf/2108.01390.pdf)
- (arXiv 2021.08) Vision Transformer with Progressive Sampling, [[Paper]](https://arxiv.org/pdf/2108.01684.pdf), [[Code]](https://github.com/yuexy/PS-ViT)
- (arXiv 2021.08) Armour: Generalizable Compact Self-Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.01778.pdf)
- (arXiv 2021.08) ConvNets vs. Transformers: Whose Visual Representations are More Transferable, [[Paper]](https://arxiv.org/pdf/2108.05305.pdf)
- (arXiv 2021.08) Mobile-Former: Bridging MobileNet and Transformer, [[Paper]](https://arxiv.org/pdf/2108.05895.pdf)
- (arXiv 2021.08) Do Vision Transformers See Like Convolutional Neural Networks, [[Paper]](https://arxiv.org/pdf/2108.08810.pdf)
- (arXiv 2021.08) Exploring and Improving Mobile Level Vision Transformers, [[Paper]](https://arxiv.org/pdf/2108.13015.pdf)
- (arXiv 2021.08) A Battle of Network Structures: An Empirical Study of CNN, Transformer, and MLP, [[Paper]](https://arxiv.org/pdf/2108.13002.pdf)
- (arXiv 2021.08) Scaled ReLU Matters for Training Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.03810.pdf)
- (arXiv 2021.09) Towards Transferable Adversarial Attacks on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2109.04176.pdf)
- (arXiv 2021.09) DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and Transformers, [[Paper]](https://arxiv.org/pdf/2109.10060.pdf), [[Code]](https://github.com/changlin31/DS-Net)
- (arXiv 2021.09) Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers, [[Paper]](https://arxiv.org/pdf/2109.10686.pdf)
- (arXiv 2021.09) Fine-tuning Vision Transformers for the Prediction of State Variables in Ising Models, [[Paper]](https://arxiv.org/pdf/2109.13925.pdf)
- (arXiv 2021.09) UFO-ViT: High Performance Linear Vision Transformer without Softmax, [[Paper]](https://arxiv.org/pdf/2109.14382.pdf)
- (arXiv 2021.10) MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer, [[Paper]](https://arxiv.org/pdf/2110.02178.pdf)
- (arXiv 2021.10) Adversarial Robustness Comparison of Vision Transformer and MLP-Mixer to CNNs, [[Paper]](https://arxiv.org/pdf/2110.02797.pdf), [[Code]](https://github.com/phibenz/robustness_comparison_vit_mlp-mixer_cnn)
- (arXiv 2021.10) Token Pooling in Visual Transformers, [[Paper]](https://arxiv.org/pdf/2110.03860.pdf)
- (arXiv 2021.10) NViT: Vision Transformer Compression and Parameter Redistribution, [[Paper]](https://arxiv.org/pdf/2110.04869.pdf)
- (arXiv 2021.10) Adversarial Token Attacks on Vision Transformers, [[Paper]](https://arxiv.org/pdf/2110.04337.pdf)
- (arXiv 2021.10) Certified Patch Robustness via Smoothed Vision Transformers, [[Paper]](https://arxiv.org/pdf/2110.07719.pdf), [[Code]](https://github.com/MadryLab/smoothed-vit)
- (arXiv 2021.10) Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation, [[Paper]](https://arxiv.org/pdf/2110.07858.pdf)
- (arXiv 2021.10) SOFT: Softmax-free Transformer with Linear Complexity, [[Paper]](https://arxiv.org/pdf/2110.11945.pdf), [[Code]](https://github.com/fudan-zvg/SOFT)
- (arXiv 2021.10) Blending Anti-Aliasing into Vision Transformer, [[Paper]](https://arxiv.org/pdf/2110.15156.pdf), [[Code]](https://github.com/amazon-research/anti-aliasing-transformer)
- (arXiv 2021.11) Can Vision Transformers Perform Convolution, [[Paper]](https://arxiv.org/pdf/2111.01353.pdf)
- (arXiv 2021.11) Sliced Recursive Transformer, [[Paper]](https://arxiv.org/pdf/2111.05297.pdf), [[Code]](https://github.com/szq0214/SReT)
- (arXiv 2021.11) Hybrid BYOL-ViT: Efficient approach to deal with small Datasets, [[Paper]](https://arxiv.org/pdf/2111.04845.pdf)
- (arXiv 2021.11) Are Transformers More Robust Than CNNs, [[Paper]](https://arxiv.org/pdf/2111.05464.pdf), [[Code]](https://github.com/ytongbai/ViTs-vs-CNNs)
- (arXiv 2021.11) iBOT: Image BERT Pre-Training with Online Tokenizer, [[Paper]](https://arxiv.org/pdf/2111.07832.pdf)
- (arXiv 2021.11) Improved Robustness of Vision Transformer via PreLayerNorm in Patch Embedding, [[Paper]](https://arxiv.org/pdf/2111.08413.pdf)
- (arXiv 2021.11) TransMix: Attend to Mix for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.09833.pdf), [[Code]](https://github.com/Beckschen/TransMix)
- (arXiv 2021.11) Swin Transformer V2: Scaling Up Capacity and Resolution, [[Paper]](https://arxiv.org/pdf/2111.09883.pdf), [[Code]](https://github.com/microsoft/Swin-Transformer)
- (arXiv 2021.11) Are Vision Transformers Robust to Patch Perturbations, [[Paper]](https://arxiv.org/pdf/2111.10659.pdf)
- (arXiv 2021.11) Discrete Representations Strengthen Vision Transformer Robustness, [[Paper]](https://arxiv.org/pdf/2111.10493.pdf)
- (arXiv 2021.11) Zero-Shot Certified Defense against Adversarial Patches with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.10481.pdf)
- (arXiv 2021.11) MetaFormer is Actually What You Need for Vision, [[Paper]](https://arxiv.org/pdf/2111.11418.pdf), [[Code]](https://github.com/sail-sg/poolformer)
- (arXiv 2021.11) DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion, [[Paper]](https://arxiv.org/pdf/2111.11326.pdf), [[Code]](https://github.com/arthurdouillard/dytox)
- (arXiv 2021.11) Mesa: A Memory-saving Training Framework for Transformers, [[Paper]](https://arxiv.org/pdf/2111.11124.pdf), [[Code]](https://github.com/zhuang-group/Mesa)
- (arXiv 2021.11) Semi-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.11067.pdf)
- (arXiv 2021.11) DBIA: Data-free Backdoor Injection Attack against Transformer Networks, [[Paper]](https://arxiv.org/pdf/2111.11870.pdf), [[Code]](https://anonymous.4open.science/r/DBIA-825D)
- (arXiv 2021.11) Self-slimmed Vision Transformer, [[Paper]](https://arxiv.org/pdf/2111.12624.pdf)
- (arXiv 2021.11) PeCo: Perceptual Codebook for BERT Pre-training of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.12710.pdf), [[Code]](https://github.com/microsoft/PeCo)
- (arXiv 2021.11) SWAT: Spatial Structure Within and Among Tokens, [[Paper]](https://arxiv.org/pdf/2111.13677.pdf)
- (arXiv 2021.11) NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2111.12994.pdf), [[Code]](https://github.com/NomMer1125/NomMer)
- (arXiv 2021.11) Global Interaction Modelling in Vision Transformer via Super Tokens, [[Paper]](https://arxiv.org/pdf/2111.13156.pdf)
- (arXiv 2021.11) ATS: Adaptive Token Sampling For Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2111.15667.pdf)
- (arXiv 2021.11) Pyramid Adversarial Training Improves ViT Performance, [[Paper]](https://arxiv.org/pdf/2111.15121.pdf)
- (arXiv 2021.12) Improved Multiscale Vision Transformers for Classification and Detection, [[Paper]](https://arxiv.org/pdf/2112.01526.pdf)
- (arXiv 2021.12) Make A Long Image Short: Adaptive Token Length for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.01686.pdf)
- (arXiv 2021.12) Dynamic Token Normalization Improves Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.02624.pdf), [[Code]](https://github.com/wqshao126/DTN)
- (arXiv 2021.12) Bootstrapping ViTs: Towards Liberating Vision Transformers from Pre-training, [[Paper]](https://arxiv.org/pdf/2112.03552.pdf)
- (arXiv 2021.12) Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal, [[Paper]](https://arxiv.org/pdf/2112.03492.pdf), [[Code]](https://github.com/shiyuchengtju/par)
- (arXiv 2021.12) Visual Transformers with Primal Object Queries for Multi-Label Image Classification, [[Paper]](https://arxiv.org/pdf/2112.05485.pdf)
- (arXiv 2021.12) Couplformer:Rethinking Vision Transformer with Coupling Attention Map, [[Paper]](https://arxiv.org/pdf/2112.05425.pdf)
- (arXiv 2021.12) AdaViT: Adaptive Tokens for Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.07658.pdf)
- (arXiv 2021.12) Lite Vision Transformer with Enhanced Self-Attention, [[Paper]](https://arxiv.org/pdf/2112.10809.pdf), [[Code]](https://github.com/Chenglin-Yang/LVT)
- (arXiv 2021.12) Learned Queries for Efficient Local Attention, [[Paper]](https://arxiv.org/pdf/2112.11435.pdf), [[Code]](https://github.com/moabarar/qna)
- (arXiv 2021.12) MPViT: Multi-Path Vision Transformer for Dense Prediction, [[Paper]](https://arxiv.org/pdf/2112.11010.pdf), [[Code]](https://github.com/youngwanLEE/MPViT)
- (arXiv 2021.12) MIA-Former: Efficient and Robust Vision Transformers via Multi-grained Input-Adaptation, [[Paper]](https://arxiv.org/pdf/2112.11542.pdf)
- (arXiv 2021.12) ELSA: Enhanced Local Self-Attention for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2112.12786.pdf), [[Code]](https://github.com/damo-cv/ELSA)
- (arXiv 2021.12) SimViT: Exploring a Simple Vision Transformer with sliding windows, [[Paper]](https://arxiv.org/pdf/2112.13085.pdf), [[Code]](https://github.com/ucasligang/SimViT)
- (arXiv 2021.12) Vision Transformer for Small-Size Datasets, [[Paper]](https://arxiv.org/pdf/2112.13492.pdf)
- (arXiv 2021.12) ViR: the Vision Reservoir, [[Paper]](https://arxiv.org/pdf/2112.13545.pdf)
- (arXiv 2021.12) Augmenting Convolutional networks with attention-based aggregation, [[Paper]](https://arxiv.org/pdf/2112.13692.pdf)
- (arXiv 2021.12) Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention, [[Paper]](https://arxiv.org/pdf/2112.14000.pdf), [[Code]](https://github.com/BR-IDL/PaddleViT)
- (arXiv 2021.12) SPViT: Enabling Faster Vision Transformers via Soft Token Pruning, [[Paper]](https://arxiv.org/pdf/2112.13890.pdf)
- (arXiv 2021.12) Stochastic Layers in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2112.15111.pdf)
- (arXiv 2022.01) Vision Transformer with Deformable Attention, [[Paper]](https://arxiv.org/pdf/2201.00520.pdf), [[Code]](https://github.com/LeapLabTHU/DAT)
- (arXiv 2022.01) PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture, [[Paper]](https://arxiv.org/pdf/2201.00978.pdf), [[Code]](https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch)
- (arXiv 2022.01) QuadTree Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2201.02767.pdf), [[Code]](https://github.com/Tangshitao/QuadtreeAttention)
- (arXiv 2022.01) TerViT: An Efficient Ternary Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.08050.pdf)
- (arXiv 2022.01) UniFormer: Unifying Convolution and Self-attention for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2201.09450.pdf), [[Code]](https://github.com/Sense-X/UniFormer)
- (arXiv 2022.01) Patches Are All You Need?, [[Paper]](https://arxiv.org/pdf/2201.09792.pdf), [[Code]](https://github.com/locuslab/convmixer)
- (arXiv 2022.01) Convolutional Xformers for Vision, [[Paper]](https://arxiv.org/pdf/2201.10271.pdf), [[Code]](https://github.com/pranavphoenix/CXV)
- (arXiv 2022.01) When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism, [[Paper]](https://arxiv.org/pdf/2201.10801.pdf), [[Code]](https://github.com/microsoft/SPACH)
- (arXiv 2022.01) Training Vision Transformers with Only 2040 Images, [[Paper]](https://arxiv.org/pdf/2201.10728.pdf)
- (arXiv 2022.01) O-ViT: Orthogonal Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.12133.pdf)
- (arXiv 2022.01) Aggregating Global Features into Local Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.12903.pdf),[[Code]](https://github.com/krushi1992/MOA-transformer)
- (arXiv 2022.01) BOAT: Bilateral Local Attention Vision Transformer, [[Paper]](https://arxiv.org/pdf/2201.13027.pdf)
- (arXiv 2022.02) BViT: Broad Attention based Vision Transformer, [[Paper]](https://arxiv.org/pdf/2202.06268.pdf),[[Code]](https://github.com/DRL-CASIA/Broad_ViT)
- (arXiv 2022.02) How Do Vision Transformers Work, [[Paper]](https://arxiv.org/pdf/2202.06709.pdf),[[Code]](https://github.com/xxxnell/how-do-vits-work)
- (arXiv 2022.02) Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations, [[Paper]](https://arxiv.org/pdf/2202.07800.pdf),[[Code]](https://github.com/youweiliang/evit)
- (arXiv 2022.02) ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond, [[Paper]](https://arxiv.org/pdf/2202.10108.pdf)
- (arXiv 2022.02) Learning to Merge Tokens in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2202.12015.pdf)
- (arXiv 2022.02) Auto-scaling Vision Transformers without Training, [[Paper]](https://arxiv.org/pdf/2202.11921.pdf),[[Code]](https://github.com/VITA-Group/AsViT)
- (arXiv 2022.03) Aggregated Pyramid Vision Transformer: Split-transform-merge Strategy for Image Recognition without Convolutions, [[Paper]](https://arxiv.org/pdf/2203.00960.pdf)
- (arXiv 2022.03) D^2ETR: Decoder-Only DETR with Computationally Efficient Cross-Scale Attention, [[Paper]](https://arxiv.org/pdf/2203.00860.pdf)
- (arXiv 2022.03) BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning, [[Paper]](https://arxiv.org/pdf/2203.01522.pdf)
- (arXiv 2022.03) Multi-Tailed Vision Transformer for Efficient Inference, [[Paper]](https://arxiv.org/pdf/2203.01587.pdf)
- (arXiv 2022.03) ViT-P: Rethinking Data-efficient Vision Transformers from Locality, [[Paper]](https://arxiv.org/pdf/2203.02358.pdf)
- (arXiv 2022.03) Coarse-to-Fine Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.03821.pdf),[[Code]](https://github.com/ChenMnZ/CF-ViT)
- (arXiv 2022.03) Dynamic Group Transformer: A General Vision Transformer Backbone with Dynamic Group Attention, [[Paper]](https://arxiv.org/pdf/2203.03937.pdf)
- (arXiv 2022.03) EdgeFormer: Improving Light-weight ConvNets by Learning from Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.03952.pdf)
- (arXiv 2022.03) WaveMix: Resource-efficient Token Mixing for Images, [[Paper]](https://arxiv.org/pdf/2203.03689.pdf), [[Code]](https://github.com/pranavphoenix/WaveMix)
- (arXiv 2022.03) Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice, [[Paper]](https://arxiv.org/pdf/2203.05962.pdf), [[Code]](https://github.com/VITA-Group/ViT-Anti-Oversmoothing)
- (arXiv 2022.03) Visualizing and Understanding Patch Interactions in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.05922.pdf)
- (arXiv 2022.03) EIT: Efficiently Lead Inductive Biases to ViT, [[Paper]](https://arxiv.org/pdf/2203.07116.pdf), [[Code]](https://github.com/MrHaiPi/EIT)
- (arXiv 2022.03) The Principle of Diversity: Training Stronger Vision Transformers Calls for Reducing All Levels of Redundancy, [[Paper]](https://arxiv.org/pdf/2203.06345.pdf), [[Code]](https://github.com/VITA-Group/Diverse-ViT)
- (arXiv 2022.03) Towards Practical Certifiable Patch Defense with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.08519.pdf)
- (arXiv 2022.03) Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations, [[Paper]](https://arxiv.org/pdf/2203.08392.pdf), [[Code]](https://github.com/RICE-EIC/Patch-Fool)
- (arXiv 2022.03) Are Vision Transformers Robust to Spurious Correlations, [[Paper]](https://arxiv.org/pdf/2203.09125.pdf), [[Code]](https://github.com/deeplearning-wisc/vit-spurious-robustness)
- (arXiv 2022.03) Three things everyone should know about Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.09125.pdf)
- (arXiv 2022.03) ScalableViT: Rethinking the Context-oriented Generalization of Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.10790.pdf)
- (arXiv 2022.03) GradViT: Gradient Inversion of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.11894.pdf), [[Code]](https://gradvit.github.io/)
- (arXiv 2022.03) Learning Patch-to-Cluster Attention in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.11987.pdf)
- (arXiv 2022.03) Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization, [[Paper]](https://arxiv.org/pdf/2203.13167.pdf)
- (arXiv 2022.03) Beyond Fixation: Dynamic Window Visual Transformer, [[Paper]](https://arxiv.org/pdf/2203.12856.pdf), [[Code]](https://github.com/pzhren/DW-ViT)
- (arXiv 2022.03) Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness, [[Paper]](https://arxiv.org/pdf/2203.13639.pdf)
- (arXiv 2022.03) Automated Progressive Learning for Efficient Training of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.14509.pdf), [[Code]](https://github.com/dvlab-research/Stratified-Transformer)
- (arXiv 2022.03) Beyond Masking: Demystifying Token-Based Pre-Training for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.14313.pdf), [[Code]](https://github.com/sunsmarterjie/beyond_masking)
- (arXiv 2022.03) CaCo: Both Positive and Negative Samples are Directly Learnable via Cooperative-adversarial Contrastive Learning, [[Paper]](https://arxiv.org/pdf/2203.14370.pdf), [[Code]](https://github.com/maple-research-lab/caco)
- (arXiv 2022.03) SepViT: Separable Vision Transformer, [[Paper]](https://arxiv.org/pdf/2203.15380.pdf)
- (arXiv 2022.03) Fine-tuning Image Transformers using Learnable Memory, [[Paper]](https://arxiv.org/pdf/2203.15243.pdf)
- (arXiv 2022.03) Parameter-efficient Fine-tuning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2203.16329.pdf)
- (arXiv 2022.03) MaxViT: Multi-Axis Vision Transformer, [[Paper]](https://arxiv.org/pdf/2204.0169.pdf)
- (arXiv 2022.04) BatchFormerV2: Exploring Sample Relationships for Dense Representation Learning, [[Paper]](https://arxiv.org/pdf/2204.01254.pdf)
- (arXiv 2022.04) Improving Vision Transformers by Revisiting High-frequency Components, [[Paper]](https://arxiv.org/pdf/2204.00993.pdf)
- (arXiv 2022.04) MixFormer: Mixing Features across Windows and Dimensions, [[Paper]](https://arxiv.org/pdf/2204.02557.pdf), [[Code]](https://github.com/PaddlePaddle/PaddleClas)
- (arXiv 2022.04) DaViT: Dual Attention Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.03645.pdf), [[Code]](https://github.com/dingmyu/davit)
- (arXiv 2022.04) Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels, [[Paper]](https://arxiv.org/pdf/2204.04905.pdf)
- (arXiv 2022.04) MiniViT: Compressing Vision Transformers with Weight Multiplexing, [[Paper]](https://arxiv.org/pdf/2204.07154.pdf)
- (arXiv 2022.04) DeiT III: Revenge of the ViT, [[Paper]](https://arxiv.org/pdf/2204.07118.pdf)
- (arXiv 2022.04) Neighborhood Attention Transformer, [[Paper]](https://arxiv.org/pdf/2204.07143.pdf), [[Code]](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)
- (arXiv 2022.04) ResT V2: Simpler, Faster and Stronger, [[Paper]](https://arxiv.org/pdf/2204.07366.pdf), [[Code]](https://github.com/wofmanaf/ResT)
- (arXiv 2022.04) VSA: Learning Varied-Size Window Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.08446.pdf), [[Code]](https://github.com/ViTAE-Transformer/ViTAE-VSA)
- (arXiv 2022.04) OCFormer: One-Class Transformer Network for Image Classification, [[Paper]](https://arxiv.org/pdf/2204.11449.pdf)
- (arXiv 2022.04) Adaptive Split-Fusion Transformer, [[Paper]](https://arxiv.org/pdf/2204.11449.pdf), [[Code]](https://github.com/szx503045266/ASF-former)
- (arXiv 2022.04) Understanding The Robustness in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2204.12451.pdf), [[Code]](https://github.com/NVlabs/FAN)
- (arXiv 2022.05) Better plain ViT baselines for ImageNet-1k, [[Paper]](https://arxiv.org/pdf/2205.01580.pdf), [[Code]](https://github.com/google-research/big_vision)
- (arXiv 2022.05) EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.03436.pdf), [[Code]](https://github.com/google-research/big_vision)
- (arXiv 2022.05) ConvMAE: Masked Convolution Meets Masked Autoencoders, [[Paper]](https://arxiv.org/pdf/2205.03892.pdf), [[Code]](https://github.com/Alpha-VL/ConvMAE)
- (arXiv 2022.05) Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.08078.pdf)
- (arXiv 2022.05) TRT-ViT: TensorRT-oriented Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.09579.pdf)
- (arXiv 2022.05) Super Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.11397.pdf), [[Code]](https://github.com/lmbxmu/SuperViT)
- (arXiv 2022.05) Deeper vs Wider: A Revisit of Transformer Configuration, [[Paper]](https://arxiv.org/pdf/2205.10505.pdf)
- (arXiv 2022.05) Vision Transformers in 2022: An Update on Tiny ImageNet, [[Paper]](https://arxiv.org/pdf/2205.10660.pdf), [[Code]](https://github.com/ehuynh1106/TinyImageNet-Transformers)
- (arXiv 2022.05) Privacy-Preserving Image Classification Using Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.12041.pdf)
- (arXiv 2022.05) Inception Transformer, [[Paper]](https://arxiv.org/pdf/2205.12956.pdf), [[Code]](https://github.com/sail-sg/iFormer)
- (arXiv 2022.05) MoCoViT: Mobile Convolutional Vision Transformer, [[Paper]](https://arxiv.org/pdf/2205.12635.pdf), [[Code]](https://github.com/sail-sg/iFormer)
- (arXiv 2022.05) Breaking the Chain of Gradient Leakage in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.12551.pdf), [[Code]](https://github.com/sail-sg/iFormer)
- (arXiv 2022.05) Hierarchical Vision Transformer for Masked Image Modeling, [[Paper]](https://arxiv.org/pdf/2205.13515.pdf), [[Code]](https://github.com/LayneH/GreenMIM)
- (arXiv 2022.05) Fast Vision Transformers with HiLo Attention, [[Paper]](https://arxiv.org/pdf/2205.13213.pdf), [[Code]](https://github.com/zip-group/LITv2)
- (arXiv 2022.05) AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition, [[Paper]](https://arxiv.org/pdf/2205.13535.pdf), [[Code]](http://www.shoufachen.com/adaptformer-page)
- (arXiv 2022.05) X-ViT: High Performance Linear Vision Transformer without Softmax, [[Paper]](https://arxiv.org/pdf/2205.13805.pdf)
- (arXiv 2022.05) Architecture-Agnostic Masked Image Modeling éˆ¥? From ViT back to CNN, [[Paper]](https://arxiv.org/pdf/2205.13805.pdf), [[Code]](http://www.shoufachen.com/adaptformer-page)
- (arXiv 2022.05) HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling, [[Paper]](https://arxiv.org/pdf/2205.14949.pdf)
- (arXiv 2022.05) EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition, [[Paper]](https://arxiv.org/pdf/2205.14756.pdf), [[Code]](https://tinyml.mit.edu/)
- (arXiv 2022.06) EfficientFormer: Vision Transformers at MobileNet Speed, [[Paper]](https://arxiv.org/pdf/2206.01191.pdf), [[Code]](https://github.com/snap-research/EfficientFormer)
- (arXiv 2022.06) Optimizing Relevance Maps of Vision Transformers Improves Robustness, [[Paper]](https://arxiv.org/pdf/2206.01161.pdf), [[Code]](https://github.com/hila-chefer/RobustViT)
- (arXiv 2022.06) Separable Self-attention for Mobile Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.02680.pdf), [[Code]](https://github.com/apple/ml-cvnets)
- (arXiv 2022.06) Spatial Entropy Regularization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.04636.pdf)
- (arXiv 2022.06) Peripheral Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.06801.pdf), [[Code]](http://cvlab.postech.ac.kr/research/PerViT/)
- (arXiv 2022.06) SP-ViT: Learning 2D Spatial Priors for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.07662.pdf)
- (arXiv 2022.06) FIT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification, [[Paper]](https://arxiv.org/pdf/2206.08671.pdf), [[Code]](https://github.com/cambridge-mlg/fit)
- (arXiv 2022.06) SimA: Simple Softmax-free Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.08898.pdf), [[Code]](https://github.com/UCDvision/sima)
- (arXiv 2022.06) Vicinity Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.10552.pdf), [[Code]](https://github.com/OpenNLPLab/Vicinity-Vision-Transformer)
- (arXiv 2022.06) EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications, [[Paper]](https://arxiv.org/pdf/2206.10589.pdf), [[Code]](https://t.ly/_Vu9)
- (arXiv 2022.06) Global Context Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.09959.pdf), [[Code]](https://github.com/NVlabs/GCViT)
- (arXiv 2022.06) EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm, [[Paper]](https://arxiv.org/pdf/2206.09325.pdf), [[Code]](https://https://github.com/zhangzjn/EATFormer)
- (arXiv 2022.06) A Unified and Biologically-Plausible Relational Graph Representation of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.11073.pdf)
- (arXiv 2022.06) Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment, [[Paper]](https://arxiv.org/pdf/2206.13951.pdf), [[Code]](https://github.com/kojima-takeshi188/CFA)
- (arXiv 2022.06) Continual Learning with Transformers for Image Classification, [[Paper]](https://arxiv.org/pdf/2206.14085.pdf)
- (arXiv 2022.07) Visual Transformer Meets CutMix for Improved Accuracy, Communication Efficiency, and Data Privacy in Split Learning, [[Paper]](https://arxiv.org/pdf/2207.00234.pdf)
- (arXiv 2022.07) Rethinking Query-Key Pairwise Interactions in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.00188.pdf)
- (arXiv 2022.07) Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks, [[Paper]](https://arxiv.org/pdf/2207.01580.pdf), [[Code]](https://github.com/raoyongming/DynamicViT)
- (arXiv 2022.07) Softmax-free Linear Transformers, [[Paper]](https://arxiv.org/pdf/2207.03341.pdf), [[Code]](https://github.com/fudan-zvg/SOFT)
- (arXiv 2022.07) MaiT: Leverage Attention Masks for More Efficient Image Transformers, [[Paper]](https://arxiv.org/pdf/2207.03006.pdf)
- (arXiv 2022.07) Dual Vision Transformer, [[Paper]](https://arxiv.org/pdf/2207.04976.pdf), [[Code]](https://github.com/YehLi/ImageNetModel)
- (arXiv 2022.07) Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning, [[Paper]](https://arxiv.org/pdf/2207.04978.pdf), [[Code]](https://github.com/YehLi/ImageNetModel)
- (arXiv 2022.07) Horizontal and Vertical Attention in Transformers, [[Paper]](https://arxiv.org/pdf/2207.04399.pdf)
- (arXiv 2022.07) LightViT: Towards Light-Weight Convolution-Free Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.05557.pdf), [[Code]](https://github.com/hunto/LightViT)
- (arXiv 2022.07) Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios, [[Paper]](https://arxiv.org/pdf/2207.05501.pdf)
- (arXiv 2022.07) Image and Model Transformation with Secret Key for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2207.05366.pdf)
- (arXiv 2022.07) Convolutional Bypasses Are Better Vision Transformer Adapters, [[Paper]](https://arxiv.org/pdf/2207.07039.pdf)
- (arXiv 2022.07) Lightweight Vision Transformer with Cross Feature Attention, [[Paper]](https://arxiv.org/pdf/2207.07268.pdf)
- (arXiv 2022.07) Multi-manifold Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.08569.pdf)
- (arXiv 2022.07) TokenMix: Rethinking Image Mixing for Data Augmentation in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.08409.pdf), [[Code]](https://github.com/Sense-X/TokenMix)
- (arXiv 2022.07) Locality Guidance for Improving Vision Transformers on Tiny Datasets, [[Paper]](https://arxiv.org/pdf/2207.10026.pdf), [[Code]](https://github.com/lkhl/tiny-transformers)
- (arXiv 2022.07) TinyViT: Fast Pretraining Distillation for Small Vision Transformers, [[Paper]](https://arxiv.org/pdf/2207.10666.pdf), [[Code]](https://github.com/microsoft/Cream/tree/main/TinyViT)
- (arXiv 2022.07) Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2207.11971.pdf), [[Code]](https://yingyichen-cyy.github.io/Jigsaw-ViT/)
- (arXiv 2022.07) An Impartial Take to the CNN vs Transformer Robustness Contest, [[Paper]](https://arxiv.org/pdf/2207.11347.pdf)
- (arXiv 2022.07) Pro-tuning: Unified Prompt Tuning for Vision Tasks, [[Paper]](https://arxiv.org/pdf/2207.14381.pdf)
- (arXiv 2022.08) Semi-supervised Vision Transformers at Scale, [[Paper]](https://arxiv.org/pdf/2208.05688.pdf)
- (arXiv 2022.08) BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers, [[Paper]](https://arxiv.org/pdf/2208.06366.pdf), [[Code]](https://aka.ms/beit)
- (arXiv 2022.08) Accelerating Vision Transformer Training via a Patch Sampling Schedule, [[Paper]](https://arxiv.org/pdf/2208.09520.pdf), [[Code]](https://github.com/BradMcDanel/pss)
- (arXiv 2022.08) ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition, [[Paper]](https://arxiv.org/pdf/2208.10431.pdf), [[Code]](https://github.com/zju-vipa/ProtoPFormer)
- (arXiv 2022.08) FocusFormer: Focusing on What We Need via Architecture Sampler, [[Paper]](https://arxiv.org/pdf/2208.10861.pdf)
- (arXiv 2022.08) gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window, [[Paper]](https://arxiv.org/pdf/2208.11718.pdf)
- (arXiv 2022.08) Video Mobile-Former: Video Recognition with Efficient Global Spatial-temporal Modeling, [[Paper]](https://arxiv.org/pdf/2208.12257.pdf)
- (arXiv 2022.08) ClusTR: Exploring Efficient Self-attention via Clustering for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2208.13138.pdf)
- (arXiv 2022.09) MAFormer: A Transformer Network with Multi-scale Attention Fusion for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2209.01620.pdf)
- (arXiv 2022.09) A Light Recipe to Train Robust Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.07399.pdf), [[Code]](https://github.com/dedeswim/vits-robustness-torch)
- (arXiv 2022.09) ConvFormer: Closing the Gap Between CNN and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.07738.pdf), [[Code]](https://github.com/dedeswim/vits-robustness-torch)
- (arXiv 2022.09) Axially Expanded Windows for Local-Global Interaction in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.08726.pdf)
- (arXiv 2022.09) Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention, [[Paper]](https://arxiv.org/pdf/2209.13802.pdf)
- (arXiv 2022.09) Effective Vision Transformer Training: A Data-Centric Perspective, [[Paper]](https://arxiv.org/pdf/2209.15006.pdf)
- (arXiv 2022.09) Dilated Neighborhood Attention Transformer, [[Paper]](https://arxiv.org/pdf/2209.15001.pdf), [[Code]](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)
- (arXiv 2022.10) MobileViTv3: Mobile-Friendly Vision Transformer with Simple and Effective Fusion of Local, Global and Input Features, [[Paper]](https://arxiv.org/pdf/2209.15159.pdf), [[Code]](https://github.com/micronDLA/MobileViTv3)
- (arXiv 2022.10) Fast-ParC: Position Aware Global Kernel for ConvNets and ViTs, [[Paper]](https://arxiv.org/pdf/2210.04020.pdf)
- (arXiv 2022.10) Strong Gravitational Lensing Parameter Estimation with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.04143.pdf), [[Code]](https://github.com/kuanweih/strong_lensing_vit_resnet)
- (arXiv 2022.10) Token-Label Alignment for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.06455.pdf), [[Code]](https://github.com/Euphoria16/TL-Align)
- (arXiv 2022.10) Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets, [[Paper]](https://arxiv.org/pdf/2210.05958.pdf), [[Code]](https://github.com/ArieSeirack/DHVT)
- (arXiv 2022.10) Prompt Generation Networks for Efficient Adaptation of Frozen Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.06466.pdf), [[Code]](https://github.com/jochemloedeman/PGN)
- (arXiv 2022.10) Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers, [[Paper]](https://arxiv.org/pdf/2210.06313.pdf)
- (arXiv 2022.10) Curved Representation Space of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.05742.pdf)
- (arXiv 2022.10) How to Train Vision Transformer on Small-scale Datasets, [[Paper]](https://arxiv.org/pdf/2210.07240.pdf), [[Code]](https://github.com/hananshafi/vits-for-small-scale-datasets)
- (arXiv 2022.10) Vision Transformer Visualization: What Neurons Tell and How Neurons Behave, [[Paper]](https://arxiv.org/pdf/2210.07646.pdf), [[Code]](https://github.com/bym1902/vit_visualization)
- (arXiv 2022.10) When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture, [[Paper]](https://arxiv.org/pdf/2210.07540.pdf), [[Code]](https://github.com/mo666666/When-Adversarial-Training-Meets-Vision-Transformers)
- (arXiv 2022.10) Vision Transformers provably learn spatial structure, [[Paper]](https://arxiv.org/pdf/2210.09221.pdf)
- (arXiv 2022.10) Scratching Visual Transformer's Back with Uniform Attention, [[Paper]](https://arxiv.org/pdf/2210.08457.pdf)
- (arXiv 2022.10) Token Merging: Your ViT But Faster, [[Paper]](https://arxiv.org/pdf/2210.09461.pdf), [[Code]](https://github.com/facebookresearch/ToMe)
- (arXiv 2022.10) Accumulated Trivial Attention Matters in Vision Transformers on Small Datasets, [[Paper]](https://arxiv.org/pdf/2210.12333.pdf), [[Code]](https://github.com/xiangyu8/SATA)
- (arXiv 2022.10) MetaFormer Baselines for Vision, [[Paper]](https://arxiv.org/pdf/2210.13452.pdf)
- (arXiv 2022.10) Learning Explicit Object-Centric Representations with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.14139.pdf)
- (arXiv 2022.10) Explicitly Increasing Input Information Density for Vision Transformers on Small Datasets, [[Paper]](https://arxiv.org/pdf/2210.14319.pdf), [[Code]](https://github.com/xiangyu8/DenseVT)
- (arXiv 2022.10) Grafting Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.15943.pdf)
- (arXiv 2022.10) Differentially Private CutMix for Split Learning with Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.15986.pdf)
- (arXiv 2022.10) ViT-LSLA: Vision Transformer with Light Self-Limited-Attention, [[Paper]](https://arxiv.org/pdf/2210.17115.pdf)
- (arXiv 2022.11) Rethinking Hierarchicies in Pre-trained Plain Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.01785.pdf), [[Code]](https://github.com/ViTAE-Transformer/HPViT)
- (arXiv 2022.11) The Lottery Ticket Hypothesis for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.01484.pdf)
- (arXiv 2022.11) ViT-CX: Causal Explanation of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.03064.pdf)
- (arXiv 2022.11) ViTALiTy: Unifying Low-rank and Sparse Approximation for Vision Transformer Acceleration with a Linear Taylor Attention, [[Paper]](https://arxiv.org/pdf/2211.05109.pdf)
- (arXiv 2022.11) Training a Vision Transformer from scratch in less than 24 hours with 1 GPU, [[Paper]](https://arxiv.org/pdf/2211.05187.pdf), [[Code]](https://github.com/BorealisAI/efficient-vit-training)
- (arXiv 2022.11) Demystify Transformers & Convolutions in Modern Image Deep Networks, [[Paper]](https://arxiv.org/pdf/2211.05781.pdf), [[Code]](https://github.com/OpenGVLab/STM-Evaluation)
- (arXiv 2022.11) Token Transformer: Can class token help window-based transformer build better long-range interactions, [[Paper]](https://arxiv.org/pdf/2211.06083.pdf)
- (arXiv 2022.11) CabViT: Cross Attention among Blocks for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.07198.pdf), [[Code]](https://github.com/hkzhang91/CabViT)
- (arXiv 2022.11) BiViT: Extremely Compressed Binary Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.07091.pdf), [[Code]](https://github.com/hkzhang91/CabViT)
- (arXiv 2022.11) HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.08024.pdf)
- (arXiv 2022.11) Vision Transformer with Super Token Sampling, [[Paper]](https://arxiv.org/pdf/2211.11167.pdf), [[Code]](https://github.com/hhb072/SViT)
- (arXiv 2022.11) Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention During Vision Transformer Inference, [[Paper]](https://arxiv.org/pdf/2211.10526.pdf)
- (arXiv 2022.11) Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2211.11943.pdf)
- (arXiv 2022.11) TranViT: An Integrated Vision Transformer Framework for Discrete Transit Travel Time Range Prediction, [[Paper]](https://arxiv.org/pdf/2211.12322.pdf)
- (arXiv 2022.11) Gated Class-Attention with Cascaded Feature Drift Compensation for Exemplar-free Continual Learning of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.12292.pdf), [[Code]](https://github.com/OcraM17/GCAB-CFDC)
- (arXiv 2022.11) Data Augmentation Vision Transformer for Fine-grained Image Classification, [[Paper]](https://arxiv.org/pdf/2211.12879.pdf)
- (arXiv 2022.11) Integrally Pre-Trained Transformer Pyramid Networks, [[Paper]](https://arxiv.org/pdf/2211.12735.pdf), [[Code]](https://github.com/sunsmarterjie/iTPN)
- (arXiv 2022.11) Explanation on Pretraining Bias of Finetuned Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.15428.pdf)
- (arXiv 2022.11) Adaptive Attention Link-based Regularization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2211.13852.pdf)
- (arXiv 2022.11) Semantic-Aware Local-Global Vision Transformer, [[Paper]](https://arxiv.org/pdf/2211.14705.pdf)
- (arXiv 2022.11) Pattern Attention Transformer with Doughnut Kernel, [[Paper]](https://arxiv.org/pdf/2211.16961.pdf)
- (arXiv 2022.11) ResFormer: Scaling ViTs with Multi-Resolution Training, [[Paper]](https://arxiv.org/pdf/2212.00776.pdf)
- (arXiv 2022.12) Teaching Matters: Investigating the Role of Supervision in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.03862.pdf), [[Code]](https://github.com/mwalmer-umd/vit_analysis)
- (arXiv 2022.12) Group Generalized Mean Pooling for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2212.04114.pdf)
- (arXiv 2022.12) OAMixer: Object-aware Mixing Layer for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.06595.pdf), [[Code]](https://github.com/alinlab/OAMixer)
- (arXiv 2022.12) What do Vision Transformers Learn? A Visual Exploration, [[Paper]](https://arxiv.org/pdf/2212.06727.pdf)
- (arXiv 2022.12) GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation, [[Paper]](https://arxiv.org/pdf/2212.06795.pdf), [[Code]](https://github.com/ChenhongyiYang/GPViT)
- (arXiv 2022.12) FlexiViT: One Model for All Patch Sizes, [[Paper]](https://arxiv.org/pdf/2212.08013.pdf), [[Code]](https://github.com/google-research/big_vision)
- (arXiv 2022.12) Rethinking Vision Transformers for MobileNet Size and Speed, [[Paper]](https://arxiv.org/pdf/2212.08059.pdf), [[Code]](https://github.com/snap-research/EfficientFormer)
- (arXiv 2022.12) Rethinking Cooking State Recognition with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2212.08586.pdf)
- (arXiv 2022.12) What Makes for Good Tokenizers in Vision Transformer, [[Paper]](https://arxiv.org/pdf/2212.11115.pdf)
- (arXiv 2022.12) Local Learning on Transformers via Feature Reconstruction, [[Paper]](https://arxiv.org/pdf/2212.14215.pdf)
- (arXiv 2022.12) Exploring Transformer Backbones for Image Diffusion Models, [[Paper]](https://arxiv.org/pdf/2212.14678.pdf)
- (arXiv 2023.01) TinyMIM: An Empirical Study of Distilling MIM Pre-trained Models, [[Paper]](https://arxiv.org/pdf/2301.01296.pdf), [[Code]](https://github.com/OliverRensu/TinyMIM)
- (arXiv 2023.01) Semi-MAE: Masked Autoencoders for Semi-supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.01431.pdf)
- (arXiv 2023.01) Skip-Attention: Improving Vision Transformers by Paying Less Attention, [[Paper]](https://arxiv.org/pdf/2301.02240.pdf)
- (arXiv 2023.01) Dynamic Grained Encoder for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.03831.pdf), [[Code]](https://github.com/StevenGrove/vtpack)
- (arXiv 2023.01) Image Memorability Prediction with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.08647.pdf)
- (arXiv 2023.01) Holistically Explainable Vision Transformers, [[Paper]](https://arxiv.org/pdf/2301.08669.pdf)
- (arXiv 2023.02) DilateFormer: Multi-Scale Dilated Transformer for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2302.01791.pdf), [[Code]](https://github.com/JIAOJIAYUASD/dilateformer)
- (arXiv 2023.02) KDEformer: Accelerating Transformers via Kernel Density Estimation, [[Paper]](https://arxiv.org/pdf/2302.02451.pdf), [[Code]](https://github.com/majid-daliri/kdeformer)
- (arXiv 2023.02) Reversible Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.04869.pdf), [[Code]](https://github.com/facebookresearch/slowfast)
- (arXiv 2023.02) TFormer: A Transmission-Friendly ViT Model for IoT Devices, [[Paper]](https://arxiv.org/pdf/2302.07734.pdf)
- (arXiv 2023.02) Efficiency 360: Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.08374.pdf)
- (arXiv 2023.02) ViTA: A Vision Transformer Inference Accelerator for Edge Applications, [[Paper]](https://arxiv.org/pdf/2302.09108.pdf)
- (arXiv 2023.02) CertViT: Certified Robustness of Pre-Trained Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.10287.pdf), [[Code]](https://github.com/sagarverma/transformer-lipschitz)
- (arXiv 2023.03) Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves, [[Paper]](https://arxiv.org/pdf/2303.01112.pdf), [[Code]](https://masora1030.github.io/Visual-Atoms-Pre-training-Vision-Transformers-with-Sinusoidal-Waves/)
- (arXiv 2023.03) Data-Efficient Training of CNNs and Transformers with Coresets: A Stability Perspective, [[Paper]](https://arxiv.org/pdf/2303.02095.pdf), [[Code]](https://github.com/transmuteAI/Data-Efficient-Transformers)
- (arXiv 2023.03) A Fast Training-Free Compression Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.02331.pdf), [[Code]](https://github.com/johnheo/fast-compress-vit)
- (arXiv 2023.03) FFT-based Dynamic Token Mixer for Vision, [[Paper]](https://arxiv.org/pdf/2303.03932.pdf), [[Code]](https://github.com/okojoalg/dfformer)
- (arXiv 2023.03) Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models, [[Paper]](https://arxiv.org/pdf/2303.04143.pdf), [[Code]](https://github.com/SamsungSAILMontreal/ghn3)
- (arXiv 2023.03) X-Pruner: eXplainable Pruning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.04935.pdf)
- (arXiv 2023.03) CrossFormer++: A Versatile Vision Transformer Hinging on Cross-scale Attention, [[Paper]](https://arxiv.org/pdf/2303.06908.pdf), [[Code]](https://github.com/cheerss/CrossFormer)
- (arXiv 2023.03) Stabilizing Transformer Training by Preventing Attention Entropy Collapse, [[Paper]](https://arxiv.org/pdf/2303.06296.pdf)
- (arXiv 2023.03) Making Vision Transformers Efficient from A Token Sparsification View, [[Paper]](https://arxiv.org/pdf/2303.08685.pdf)
- (arXiv 2023.03) BiFormer: Vision Transformer with Bi-Level Routing Attention, [[Paper]](https://arxiv.org/pdf/2303.08810.pdf), [[Code]](https://github.com/rayleizhu/BiFormer)
- (arXiv 2023.03) ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices, [[Paper]](https://arxiv.org/pdf/2303.09730.pdf)
- (arXiv 2023.03) Robustifying Token Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.11126.pdf)
- (arXiv 2023.03) FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization, [[Paper]](https://arxiv.org/pdf/2303.14189.pdf)
- (arXiv 2023.03) Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.13755.pdf)
- (arXiv 2023.03) How Does Attention Work in Vision Transformers? A Visual Analytics Attempt, [[Paper]](https://arxiv.org/pdf/2303.13731.pdf)
- (arXiv 2023.03) SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications, [[Paper]](https://arxiv.org/pdf/2303.15446.pdf), [[Code]](https://tinyurl.com/5ft8v46w)
- (arXiv 2023.03) Vision Transformer with Quadrangle Attention, [[Paper]](https://arxiv.org/pdf/2303.15105.pdf), [[Code]](https://github.com/ViTAE-Transformer/QFormer)
- (arXiv 2023.04) LaCViT: A Label-aware Contrastive Training Framework for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2303.18013.pdf)
- (arXiv 2023.04) Rethinking Local Perception in Lightweight Vision Transformer, [[Paper]](https://arxiv.org/pdf/2303.17803.pdf)
- (arXiv 2023.04) Vision Transformers with Mixed-Resolution Tokenization, [[Paper]](https://arxiv.org/pdf/2304.00287.pdf), [[Code]](https://github.com/TomerRonen34/mixed-resolution-vit)
- (arXiv 2023.04) Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention, [[Paper]](https://arxiv.org/pdf/2304.03282.pdf), [[Code]](https://github.com/dingmyu/DependencyViT)
- (arXiv 2023.04) PSLT: A Light-weight Vision Transformer with Ladder Self-Attention and Progressive Shift, [[Paper]](https://arxiv.org/pdf/2304.03481.pdf), [[Code]](https://isee-ai.cn/wugaojie/PSLT.html)
- (arXiv 2023.04) SparseFormer: Sparse Visual Recognition via Limited Latent Tokens, [[Paper]](https://arxiv.org/pdf/2304.03768.pdf), [[Code]](https://github.com/showlab/sparseformer)
- (arXiv 2023.04) ViT-Calibrator: Decision Stream Calibration for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2304.04354.pdf)
- (arXiv 2023.04) Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention, [[Paper]](https://arxiv.org/pdf/2304.04237.pdf), [[Code]](https://github.com/LeapLabTHU/Slide-Transformer)
- (arXiv 2023.04) Life Regression based Patch Slimming for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2304.04926.pdf)
- (arXiv 2023.04) RIFormer: Keep Your Vision Backbone Effective While Removing Token Mixer, [[Paper]](https://arxiv.org/pdf/2304.05659.pdf), [[Code]](https://techmonsterwang.github.io/RIFormer/)
- (arXiv 2023.04) SpectFormer: Frequency and Attention is what you need in a Vision Transformer, [[Paper]](https://arxiv.org/pdf/2304.06446.pdf), [[Code]](https://badripatro.github.io/SpectFormers/)
- (arXiv 2023.04) VISION DIFFMASK: Faithful Interpretation of Vision Transformers with Differentiable Patch Masking, [[Paper]](https://arxiv.org/pdf/2304.06391.pdf), [[Code]](https://github.com/AngelosNal/Vision-DiffMask)
- (arXiv 2023.04) RSIR Transformer: Hierarchical Vision Transformer using Random Sampling Windows and Important Region Windows, [[Paper]](https://arxiv.org/pdf/2304.06250.pdf)
- (arXiv 2023.04) Dynamic Mobile-Former: Strengthening Dynamic Convolution with Attention and Residual Connection in Kernel Space, [[Paper]](https://arxiv.org/pdf/2304.07254.pdf), [[Code]](https://github.com/ysj9909/DMF)
- (arXiv 2023.04) LipsFormer: Introducing Lipschitz Continuity to Vision Transformers, [[Paper]](https://arxiv.org/pdf/2304.09856.pdf), [[Code]](https://github.com/IDEA-Research/LipsFormer)
- (arXiv 2023.04) Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2304.10716.pdf), [[Code]](https://github.com/megvii-research/TPS-CVPR2023)
- (arXiv 2023.04) MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2304.12043.pdf), [[Code]](https://github.com/fistyee/MixPro)
- (arXiv 2023.04) Vision Conformer: Incorporating Convolutions into Vision Transformer Layers, [[Paper]](https://arxiv.org/pdf/2304.13991.pdf)
- (arXiv 2023.05) Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT, [[Paper]](https://arxiv.org/pdf/2305.00201.pdf)
- (arXiv 2023.05) MMViT: Multiscale Multiview Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.00104.pdf)
- (arXiv 2023.05) AxWin Transformer: A Context-Aware Vision Transformer Backbone with Axial Windows, [[Paper]](https://arxiv.org/pdf/2305.01280.pdf)
- (arXiv 2023.05) Understanding Gaussian Attention Bias of Vision Transformers Using Effective Receptive Fields, [[Paper]](https://arxiv.org/pdf/2305.04722.pdf)
- (arXiv 2023.05) EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention, [[Paper]](https://arxiv.org/pdf/2305.07027.pdf), [[Code]](https://github.com/microsoft/Cream/tree/main/EfficientViT)
- (arXiv 2023.05) GSB: Group Superposition Binarization for Vision Transformer with Limited Training Samples, [[Paper]](https://arxiv.org/pdf/2305.07931.pdf)
- (arXiv 2023.05) Enhancing Performance of Vision Transformers on Small Datasets through Local Inductive Bias Incorporation, [[Paper]](https://arxiv.org/pdf/2305.08551.pdf)
- (arXiv 2023.05) CageViT: Convolutional Activation Guided Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.09924.pdf)
- (arXiv 2023.05) Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design, [[Paper]](https://arxiv.org/pdf/2305.13035.pdf)
- (arXiv 2023.05) Predicting Token Impact Towards Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2305.14840.pdf)
- (arXiv 2023.05) Dual Path Transformer with Partition Attention, [[Paper]](https://arxiv.org/pdf/2305.14768.pdf)
- (arXiv 2023.05) BinaryViT: Towards Efficient and Accurate Binary Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.14730.pdf)
- (arXiv 2023.05) Making Vision Transformers Truly Shift-Equivariant, [[Paper]](https://arxiv.org/pdf/2305.16316.pdf)
- (arXiv 2023.05) Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability, [[Paper]](https://arxiv.org/pdf/2305.15775.pdf), [[Code]](https://github.com/jyhong0304/concept_centric_transformers)
- (arXiv 2023.05) DiffRate : Differentiable Compression Rate for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2305.17997.pdf), [[Code]](https://github.com/OpenGVLab/DiffRate)
- (arXiv 2023.06) Lightweight Vision Transformer with Bidirectional Interaction, [[Paper]](https://arxiv.org/pdf/2306.00396.pdf), [[Code]](https://github.com/qhfan/FAT)
- (arXiv 2023.06) Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles, [[Paper]](https://arxiv.org/pdf/2306.00989.pdf), [[Code]](https://github.com/facebookresearch/hiera)
- (arXiv 2023.06) Bytes Are All You Need: Transformers Operating Directly On File Bytes, [[Paper]](https://arxiv.org/pdf/2306.00238.pdf), [[Code]](https://github.com/apple/ml-cvnets/tree/main/examples/byteformer)
- (arXiv 2023.06) Muti-Scale And Token Mergence: Make Your ViT More Efficient, [[Paper]](https://arxiv.org/pdf/2306.04897.pdf)
- (arXiv 2023.06) FasterViT: Fast Vision Transformers with Hierarchical Attention, [[Paper]](https://arxiv.org/pdf/2306.06189.pdf), [[Code]](https://github.com/NVlabs/FasterViT)
- (arXiv 2023.06) E(2)-Equivariant Vision Transformer, [[Paper]](https://arxiv.org/pdf/2306.06722.pdf), [[Code]](https://github.com/ZJUCDSYangKaifan/GEVit)
- (arXiv 2023.06) 2-D SSM: A General Spatial Layer for Visual Transformers, [[Paper]](https://arxiv.org/pdf/2306.06635.pdf), [[Code]](https://github.com/ethanbar11/ssm_2d)
- (arXiv 2023.06) Mitigating Transformer Overconfidence via Lipschitz Regularization, [[Paper]](https://arxiv.org/pdf/2306.06849.pdf), [[Code]](https://github.com/SZCHAI/LRFormer)
- (arXiv 2023.06) Reviving Shift Equivariance in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.07470.pdf)
- (arXiv 2023.06) Learning to Mask and Permute Visual Tokens for Vision Transformer Pre-Training, [[Paper]](https://arxiv.org/pdf/2306.07346.pdf), [[Code]](https://github.com/aimagelab/MaPeT)
- (arXiv 2023.06) Fast Training of Diffusion Models with Masked Transformers, [[Paper]](https://arxiv.org/pdf/2306.09305.pdf), [[Code]](https://github.com/Anima-Lab/MaskDiT)
- (arXiv 2023.06) RaViTT: Random Vision Transformer Tokens, [[Paper]](https://arxiv.org/pdf/2306.10959.pdf)
- (arXiv 2023.06) Vision Transformer with Attention Map Hallucination and FFN Compaction, [[Paper]](https://arxiv.org/pdf/2306.10875.pdf)
- (arXiv 2023.06) Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing, [[Paper]](https://arxiv.org/pdf/2306.12929.pdf)
- (arXiv 2023.06) Swin-Free: Achieving Better Cross-Window Attention and Efficiency with Size-varying Window, [[Paper]](https://arxiv.org/pdf/2306.13776.pdf)
- (arXiv 2023.06) BinaryViT: Pushing Binary Vision Transformers Towards Convolutional Models, [[Paper]](https://arxiv.org/pdf/2306.16678.pdf)
- (arXiv 2023.06) Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing, [[Paper]](https://arxiv.org/pdf/2306.17848.pdf), [[Code]](https://arielnlee.github.io/PatchMixing/)
- (arXiv 2023.07) Stitched ViTs are Flexible Vision Backbones, [[Paper]](https://arxiv.org/pdf/2307.00154.pdf), [[Code]](https://github.com/ziplab/SN-Netv2)
- (arXiv 2023.07) MSViT: Dynamic Mixed-Scale Tokenization for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.02321.pdf), [[Code]](https://github.com/ziplab/SN-Netv2)
- (arXiv 2023.07) Make A Long Image Short: Adaptive Token Length for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.02092.pdf)
- (arXiv 2023.07) Art Authentication with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.03039.pdf)
- (arXiv 2023.07) Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution, [[Paper]](https://arxiv.org/pdf/2307.06304.pdf)
- (arXiv 2023.07) What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation, [[Paper]](https://arxiv.org/pdf/2307.06006.pdf)
- (arXiv 2023.07) Scale-Aware Modulation Meet Transformer, [[Paper]](https://arxiv.org/pdf/2307.08579.pdf), [[Code]](https://github.com/AFeng-x/SMT)
- (arXiv 2023.07) RepViT: Revisiting Mobile CNN From ViT Perspective, [[Paper]](https://arxiv.org/pdf/2307.09283.pdf), [[Code]](https://github.com/jameslahm/RepViT)
- (arXiv 2023.07) R-Cut: Enhancing Explainability in Vision Transformers with Relationship Weighted Out and Cut, [[Paper]](https://arxiv.org/pdf/2307.09050.pdf)
- (arXiv 2023.07) Learned Thresholds Token Merging and Pruning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.10780.pdf)
- (arXiv 2023.07) Sparse then Prune: Toward Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.11988.pdf), [[Code]](https://github.com/yogiprsty/Sparse-ViT)
- (arXiv 2023.07) Sparse Double Descent in Vision Transformers: real or phantom threat?, [[Paper]](https://arxiv.org/pdf/2307.14253.pdf)
- (arXiv 2023.07) Adaptive Frequency Filters As Efficient Global Token Mixers, [[Paper]](https://arxiv.org/pdf/2307.14008.pdf)
- (arXiv 2023.07) E2VPT: An Effective and Efficient Approach for Visual Prompt Tuning, [[Paper]](https://arxiv.org/pdf/2307.13770.pdf), [[Code]](https://github.com/ChengHan111/E2VPT)
- (arXiv 2023.07) Pre-training Vision Transformers with Very Limited Synthesized Images, [[Paper]](https://arxiv.org/pdf/2307.14710.pdf), [[Code]](https://github.com/ryoo-nakamura/OFDB/)
- (arXiv 2023.08) LGViT: Dynamic Early Exiting for Accelerating Vision Transformer, [[Paper]](https://arxiv.org/abs/2308.00255)
- (arXiv 2023.08) Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique, [[Paper]](https://arxiv.org/pdf/2308.00197.pdf)
- (arXiv 2023.08) FLatten Transformer: Vision Transformer using Focused Linear Attention, [[Paper]](https://arxiv.org/pdf/2308.00442.pdf), [[Code]](https://github.com/LeapLabTHU/FLatten-Transformer)
- (arXiv 2023.08) A Multidimensional Analysis of Social Biases in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.01948.pdf)
- (arXiv 2023.08) Which Tokens to Use? Investigating Token Reduction in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.04657.pdf), [[Code]](https://vap.aau.dk/tokens)
- (arXiv 2023.08) DiT: Efficient Vision Transformers with Dynamic Token Routing, [[Paper]](https://arxiv.org/pdf/2308.03409.pdf), [[Code]](https://github.com/Maycbj/DiT)
- (arXiv 2023.08) Revisiting Vision Transformer from the View of Path Ensemble, [[Paper]](https://arxiv.org/pdf/2308.06548.pdf)
- (arXiv 2023.08) Patch Is Not All You Need, [[Paper]](https://arxiv.org/pdf/2308.10729.pdf)
- (arXiv 2023.08) ConcatPlexer: Additional Dim1 Batching for Faster ViTs, [[Paper]](https://arxiv.org/pdf/2308.11199.pdf), [[Code]](https://github.com/Maycbj/DiT)
- (arXiv 2023.08) SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation, [[Paper]](https://arxiv.org/pdf/2308.11568.pdf), [[Code]](https://doranlyong.github.io/projects/spanet/)
- (arXiv 2023.08) SG-Former: Self-guided Transformer with Evolving Token Reallocation, [[Paper]](https://arxiv.org/pdf/2308.12216.pdf), [[Code]](https://github.com/OliverRensu/SG-Former)
- (arXiv 2023.08) Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2308.13494.pdf)
- (arXiv 2023.08) Learning Diverse Features in Vision Transformers for Improved Generalization, [[Paper]](https://arxiv.org/pdf/2308.16274.pdf)
- (arXiv 2023.09) DAT++: Spatially Dynamic Vision Transformer with Deformable Attention, [[Paper]](https://arxiv.org/pdf/2309.01430.pdf), [[Code]](https://github.com/LeapLabTHU/DAT)
- (arXiv 2023.09) ExMobileViT: Lightweight Classifier Extension for Mobile Vision Transformer, [[Paper]](https://arxiv.org/pdf/2309.01310.pdf)
- (arXiv 2023.09) Mobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts, [[Paper]](https://arxiv.org/pdf/2309.04354.pdf)
- (arXiv 2023.09) CNN or ViT? Revisiting Vision Transformers Through the Lens of Convolution, [[Paper]](https://arxiv.org/pdf/2309.05375.pdf), [[Code]](https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention)
- (arXiv 2023.09) SparseSwin: Swin Transformer with Sparse Transformer Block, [[Paper]](https://arxiv.org/pdf/2309.05224.pdf), [[Code]](https://github.com/KrisnaPinasthika/SparseSwin)
- (arXiv 2023.09) DeViT: Decomposing Vision Transformers for Collaborative Inference in Edge Devices, [[Paper]](https://arxiv.org/pdf/2309.05015.pdf)
- (arXiv 2023.09) Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit, [[Paper]](https://arxiv.org/pdf/2309.06891.pdf), [[Code]](https://github.com/billpsomas/simpool)
- (arXiv 2023.09) Interpretability-Aware Vision Transformer, [[Paper]](https://arxiv.org/pdf/2309.08035.pdf)
- (arXiv 2023.09) Replacing softmax with ReLU in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2309.08586.pdf)
- (arXiv 2023.09) Interpret Vision Transformers as ConvNets with Dynamic Convolutions, [[Paper]](https://arxiv.org/pdf/2309.10713.pdf)
- (arXiv 2023.09) RMT: Retentive Networks Meet Vision Transformers, [[Paper]](https://arxiv.org/pdf/2309.11523.pdf)
- (arXiv 2023.09) DualToken-ViT: Position-aware Efficient Vision Transformer with Dual Token Fusion, [[Paper]](https://arxiv.org/pdf/2309.12424.pdf)
- (arXiv 2023.09) Associative Transformer Is A Sparse Representation Learner, [[Paper]](https://arxiv.org/pdf/2309.12862.pdf)
- (arXiv 2023.09) Masked Image Residual Learning for Scaling Deeper Vision Transformers, [[Paper]](https://arxiv.org/pdf/2309.14136.pdf)
- (arXiv 2023.09) Efficient Low-rank Backpropagation for Vision Transformer Adaptation, [[Paper]](https://arxiv.org/pdf/2309.15275.pdf)
- (arXiv 2023.09) Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words, [[Paper]](https://arxiv.org/pdf/2309.16108.pdf)
- (arXiv 2023.09) Vision Transformers Need Registers, [[Paper]](https://arxiv.org/pdf/2309.16588.pdf)
- (arXiv 2023.10) PPT: Token Pruning and Pooling for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.01812.pdf)
- (arXiv 2023.10) Selective Feature Adapter for Dense Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.01843.pdf)
- (arXiv 2023.10) GET: Group Event Transformer for Event-Based Vision, [[Paper]](https://arxiv.org/pdf/2310.02642.pdf), [[Code]](https://github.com/Peterande/GET-Group-Event-Transformer)
- (arXiv 2023.10) ViT-ReciproCAM: Gradient and Attention-Free Visual Explanations for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2310.02588.pdf)
- (arXiv 2023.10) SlowFormer: Universal Adversarial Patch for Attack on Compute and Energy Efficiency of Inference Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.02544.pdf), [[Code]](https://github.com/UCDvision/SlowFormer)
- (arXiv 2023.10) TiC: Exploring Vision Transformer in Convolution, [[Paper]](https://arxiv.org/pdf/2310.04134.pdf), [[Code]](https://github.com/zs670980918/MSA-Conv)
- (arXiv 2023.10) Sub-token ViT Embedding via Stochastic Resonance Transformers, [[Paper]](https://arxiv.org/pdf/2310.03967.pdf)
- (arXiv 2023.10) No Token Left Behind: Efficient Vision Transformer via Dynamic Token Idling, [[Paper]](https://arxiv.org/pdf/2310.05654.pdf)
- (arXiv 2023.10) Plug n' Play: Channel Shuffle Module for Enhancing Tiny Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.05642.pdf)
- (arXiv 2023.10) Hierarchical Side-Tuning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2310.05393.pdf), [[Code]](https://github.com/AFeng-x/HST)
- (arXiv 2023.10) EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention, [[Paper]](https://arxiv.org/pdf/2310.06629.pdf), [[Code]](https://github.com/nkusyl)
- (arXiv 2023.10) Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing, [[Paper]](https://arxiv.org/pdf/2310.06234.pdf), [[Code]](https://github.com/DavidYanAnDe/ARC)
- (arXiv 2023.10) Accelerating Vision Transformers Based on Heterogeneous Attention Patterns, [[Paper]](https://arxiv.org/pdf/2310.07664.pdf)
- (arXiv 2023.10) MatFormer: Nested Transformer for Elastic Inference, [[Paper]](https://arxiv.org/pdf/2310.07707.pdf)
- (arXiv 2023.10) Eureka-Moments in Transformers: Multi-Step Tasks Reveal Softmax Induced Optimization Problems, [[Paper]](https://arxiv.org/pdf/2310.12956.pdf)
- (arXiv 2023.10) ConvNets Match Vision Transformers at Scale, [[Paper]](https://arxiv.org/pdf/2310.16764.pdf)
- (arXiv 2023.10) MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory, [[Paper]](https://arxiv.org/pdf/2310.16898.pdf), [[Code]](https://github.com/liangyn22/MCUFormer)
- (arXiv 2023.10) Analyzing Vision Transformers for Image Classification in Class Embedding Space, [[Paper]](https://arxiv.org/pdf/2310.18969.pdf)
- (arXiv 2023.11) Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation, [[Paper]](https://arxiv.org/pdf/2311.00441.pdf)
- (arXiv 2023.11) Scattering Vision Transformer: Spectral Mixing Matters, [[Paper]](https://arxiv.org/pdf/2311.01310.pdf), [[Project]](https://badripatro.github.io/svt/)
- (arXiv 2023.11) GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values, [[Paper]](https://arxiv.org/pdf/2311.03426.pdf)
- (arXiv 2023.11) Mini but Mighty: Finetuning ViTs with Mini Adapters, [[Paper]](https://arxiv.org/pdf/2311.03873.pdf), [[Code]](https://github.com/IemProg/MiMi)
- (arXiv 2023.11) A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis, [[Paper]](https://arxiv.org/pdf/2311.04157.pdf), [[Code]](https://github.com/Imageomics/INTR)
- (arXiv 2023.11) SBCFormer: Lightweight Network Capable of Full-size ImageNet Classification at 1 FPS on Single Board Computers, [[Paper]](https://arxiv.org/pdf/2311.03747.pdf), [[Code]](https://github.com/xyongLu/SBCFormer)
- (arXiv 2023.11) FMViT: A multiple-frequency mixing Vision Transformer, [[Paper]](https://arxiv.org/pdf/2311.05707.pdf), [[Code]](https://github.com/tany0699/FMViT)
- (arXiv 2023.11) Cross-Axis Transformer with 2D Rotary Embeddings, [[Paper]](https://arxiv.org/pdf/2311.07184.pdf)
- (arXiv 2023.11) Aggregate, Decompose, and Fine-Tune: A Simple Yet Effective Factor-Tuning Method for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2311.06749.pdf), [[Code]](https://github.com/Dongping-Chen/EFFT-EFfective-Factor-Tuning)
- (arXiv 2023.11) Advancing Vision Transformers with Group-Mix Attention, [[Paper]](https://arxiv.org/pdf/2311.15157.pdf), [[Code]](https://github.com/AILab-CVC/GroupMixFormer)
- (arXiv 2023.11) Token Recycling for Efficient Sequential Inference with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.15335.pdf)
- (arXiv 2023.11) TransNeXt: Robust Foveal Visual Perception for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.17132.pdf)
- (arXiv 2023.11) Stochastic Vision Transformers with Wasserstein Distance-Aware Attention, [[Paper]](https://arxiv.org/pdf/2311.18645.pdf)
- (arXiv 2023.11) Improving Faithfulness for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2311.17983.pdf)
- (arXiv 2023.11) SCHEME: Scalable Channer Mixer for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.00412.pdf)
- (arXiv 2023.12) MABViT -- Modified Attention Block Enhances Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.01324.pdf)
- (arXiv 2023.12) Class-Discriminative Attention Maps for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.02364.pdf), [[Code]](https://github.com/lenbrocki/CDAM)
- (arXiv 2023.12) Factorization Vision Transformer: Modeling Long Range Dependency with Local Window Cost, [[Paper]](https://arxiv.org/pdf/2312.08614.pdf), [[Code]](https://github.com/q2479036243/FaViT)
- (arXiv 2023.12) Weight Subcloning: Direct Initialization of Transformers Using Larger Pretrained Ones, [[Paper]](https://arxiv.org/pdf/2312.09299.pdf)
- (arXiv 2023.12) Cached Transformers: Improving Transformers with Differentiable Memory Cache, [[Paper]](https://arxiv.org/pdf/2312.12742.pdf)
- (arXiv 2023.12) Partial Fine-Tuning: A Successor to Full Fine-Tuning for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2312.15681.pdf)
- (arXiv 2023.12) Merging Vision Transformers from Different Tasks and Domains, [[Paper]](https://arxiv.org/pdf/2312.16240.pdf)
- (arXiv 2023.12) Universal Pyramid Adversarial Training for Improved ViT Performance, [[Paper]](https://arxiv.org/pdf/2312.16339.pdf)
- (arXiv 2024.01) Token Propagation Controller for Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2401.01470.pdf)
- (arXiv 2024.01) Intriguing Equivalence Structures of the Embedding Space of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.15568.pdf)
- (arXiv 2024.01) SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design, [[Paper]](https://arxiv.org/pdf/2401.16456.pdf)
- (arXiv 2024.02) LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition, [[Paper]](https://arxiv.org/pdf/2402.00033.pdf), [[Code]](https://github.com/edgeai1/LF-ViT.git)
- (arXiv 2024.02) A Manifold Representation of the Key in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2402.00534.pdf)
- (arXiv 2024.02) Faster Inference of Integer SWIN Transformer by Removing the GELU Activation, [[Paper]](https://arxiv.org/pdf/2402.01169.pdf)
- (arXiv 2024.02) SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization, [[Paper]](https://arxiv.org/pdf/2402.03317.pdf)
- (arXiv 2024.02) Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images, [[Paper]](https://arxiv.org/pdf/2402.03752.pdf)
- (arXiv 2024.02) Multi-Attribute Vision Transformers are Efficient and Robust Learners, [[Paper]](https://arxiv.org/abs/2402.08070), [[Code]](https://github.com/hananshafi/MTL-ViT)
- (arXiv 2024.02) FViT: A Focal Vision Transformer with Gabor Filter, [[Paper]](https://arxiv.org/pdf/2402.11303.pdf)
- (arXiv 2024.02) ReViT: Enhancing Vision Transformers with Attention Residual Connections for Visual Recognition, [[Paper]](https://arxiv.org/pdf/2402.11301.pdf)
- (arXiv 2024.02) Perceiving Longer Sequences With Bi-Directional Cross-Attention Transformers, [[Paper]](https://arxiv.org/pdf/2402.12138.pdf)
- (arXiv 2024.03) LUM-ViT: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition, [[Paper]](https://arxiv.org/pdf/2403.01412.pdf), [[Code]](https://github.com/MaxLLF/LUM-ViT)
- (arXiv 2024.03) NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function, [[Paper]](https://arxiv.org/pdf/2403.02411.pdf)
- (arXiv 2024.03) T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.04523.pdf)
- (arXiv 2024.03) ACC-ViT : Atrous Convolution's Comeback in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.04200.pdf)
- (arXiv 2024.03) Scalable and Robust Transformer Decoders for Interpretable Image Classification with Foundation Models, [[Paper]](https://arxiv.org/pdf/2403.04125.pdf)
- (arXiv 2024.03) ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions, [[Paper]](https://arxiv.org/pdf/2403.07392.pdf), [[Code]](https://github.com/Traffic-X/ViT-CoMer)
- (arXiv 2024.03) Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.10030.pdf), [[Code]](https://github.com/mlvlab/MCTF)
- (arXiv 2024.03) HIRI-ViT: Scaling Vision Transformer with High Resolution Inputs, [[Paper]](https://arxiv.org/pdf/2403.11999.pdf)
- (arXiv 2024.03) Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation, [[Paper]](https://arxiv.org/pdf/2403.11808.pdf), [[Code]](https://github.com/NUS-HPC-AI-Lab/Dynamic-Tuning)
- (arXiv 2024.03) Rotary Position Embedding for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2403.13298.pdf), [[Code]](https://github.com/naver-ai/rope-vit)
- (arXiv 2024.03) Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers, [[Paper]](https://arxiv.org/pdf/2403.13677.pdf)
- (arXiv 2024.03) ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding, [[Paper]](https://arxiv.org/pdf/2403.15004.pdf), [[Code]](https://github.com/novendrastywn/ParFormer-CAPE-2024)
- (arXiv 2024.03) Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2403.18063.pdf), [[Code]](https://github.com/badripatro/sct)
- (arXiv 2024.03) Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach, [[Paper]](https://arxiv.org/pdf/2403.19067.pdf), [[Code]](https://github.com/zstarN70/RLRR.git)
- (arXiv 2024.03) DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs, [[Paper]](https://arxiv.org/pdf/2403.19588.pdf), [[Code]](https://github.com/naver-ai/rdnet)
- (arXiv 2024.03) Sine Activated Low-Rank Matrices for Parameter Efficient Learning, [[Paper]](https://arxiv.org/pdf/2403.19243.pdf)
- (arXiv 2024.03) Efficient Modulation for Vision Networks, [[Paper]](https://arxiv.org/pdf/2403.19963.pdf), [[Code]](https://github.com/ma-xu/EfficientMod)
- (arXiv 2024.04) Structured Initialization for Attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.01139.pdf), [[Code]](https://github.com/osiriszjq/structured_init)
- (arXiv 2024.04) A General and Efficient Training for Transformer via Token Expansion, [[Paper]](https://arxiv.org/pdf/2404.00672.pdf), [[Code]](https://github.com/Osilly/TokenExpansion)
- (arXiv 2024.04) DeiT-LT: Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets, [[Paper]](https://arxiv.org/pdf/2404.02900.pdf), [[Code]](https://github.com/val-iisc/DeiT-LT/tree/main)
- (arXiv 2024.04) LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity, [[Paper]](https://arxiv.org/pdf/2404.03214.pdf), [[Code]](https://github.com/WalBouss/LeGrad)
- (arXiv 2024.04) Learning Correlation Structures for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2404.03924.pdf), [[Code]](http://cvlab.postech.ac.kr/research/StructViT/)
- (arXiv 2024.04) HSViT: Horizontally Scalable Vision Transformer, [[Paper]](https://arxiv.org/pdf/2404.03924.pdf), [[Code]](https://github.com/xuchenhao001/HSViT)
- (arXiv 2024.04) MLP Can Be A Good Transformer Learner, [[Paper]](https://arxiv.org/pdf/2404.05657.pdf), [[Code]](https://github.com/sihaoevery/lambda_vit)
- (arXiv 2024.04) Observation, Analysis, and Solution: Exploring Strong Lightweight Vision Transformers via Masked Image Modeling Pre-Training, [[Paper]](https://arxiv.org/pdf/2404.12210.pdf), [[Code]](https://github.com/wangsr126/mae-lite)
- (arXiv 2024.04) Nested-TNT: Hierarchical Vision Transformers with Multi-Scale Feature Processing, [[Paper]](https://arxiv.org/pdf/2404.13434.pdf)
- (arXiv 2024.04) SPARO: Selective Attention for Robust and Compositional Transformer Encodings for Vision, [[Paper]](https://arxiv.org/pdf/2404.15721.pdf), [[Code]](https://github.com/ankitkv/sparo-clip)
- (arXiv 2024.05) SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization, [[Paper]](https://arxiv.org/pdf/2405.11582.pdf), [[Code]](https://github.com/xinghaochen/SLAB), [[Code]](https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB)
- (arXiv 2024.05) Block Selective Reprogramming for On-device Training of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2405.10951.pdf)
- (arXiv 2024.05) Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference, [[Paper]](https://arxiv.org/pdf/2405.14700.pdf), [[Code]](https://github.com/liuting20/Sparse-Tuning)
- (arXiv 2024.05) TerDiT: Ternary Diffusion Models with Transformers, [[Paper]](https://arxiv.org/pdf/2405.14854.pdf), [[Code]](https://github.com/Lucky-Lance/TerDiT)
- (arXiv 2024.05) LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate, [[Paper]](https://arxiv.org/pdf/2405.13985.pdf), [[Code]](https://github.com/GreenCUBIC/lookhere)
- (arXiv 2024.05) DCT-Based Decorrelated Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2405.13901.pdf)
- (arXiv 2024.05) Configuring Data Augmentations to Reduce Variance Shift in Positional Embedding of Vision Transformers, [[Paper]](https://arxiv.org/pdf/2405.14115.pdf)
- (arXiv 2024.05) Vision Transformer with Sparse Scan Prior, [[Paper]](https://arxiv.org/pdf/2405.13335.pdf), [[Code]](https://github.com/qhfan/SSViT)
- (arXiv 2024.05) Semantic Equitable Clustering: A Simple, Fast and Effective Strategy for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2405.13337.pdf), [[Code]](https://github.com/qhfan/SecViT)
- (arXiv 2024.05) Dissecting Query-Key Interaction in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2405.14880.pdf)
- (arXiv 2024.05) MSPE: Multi-Scale Patch Embedding Prompts Vision Transformers to Any Resolution, [[Paper]](https://arxiv.org/pdf/2405.18240.pdf)
- (arXiv 2024.05) Wavelet-Based Image Tokenizer for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2405.18616v1.pdf)
- (arXiv 2024.06) You Only Need Less Attention at Each Stage in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2406.00427.pdf)
- (arXiv 2024.06) An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels, [[Paper]](https://arxiv.org/pdf/2406.09415.pdf)
- (arXiv 2024.06) Fusion of regional and sparse attention in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2406.08859.pdf)
- (arXiv 2024.06) Inpainting the Gaps: A Novel Framework for Evaluating Explanation Methods in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2406.11534.pdf)
- (arXiv 2024.06) PEANO-ViT: Power-Efficient Approximations of Non-Linearities in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2406.14854.pdf)
- (arXiv 2024.06) A Primal-Dual Framework for Transformers and Neural Networks, [[Paper]](https://arxiv.org/pdf/2406.13781.pdf)
- (arXiv 2024.06) CTRL-F: Pairing Convolution with Transformer for Image Classification via Multi-Level Feature Cross-Attention and Representation Learning Fusion, [[Paper]](https://arxiv.org/pdf/2407.06673.pdf),[[Code]](https://github.com/hosamsherif/CTRL-F)
- (arXiv 2024.07) MambaVision: A Hybrid Mamba-Transformer Vision Backbone, [[Paper]](https://arxiv.org/pdf/2407.08083v1.pdf),[[Code]](https://github.com/NVlabs/MambaVision)
- (arXiv 2024.07) Global-Local Similarity for Efficient Fine-Grained Image Recognition with Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.12891.pdf),[[Code]](https://github.com/arkel23/GLSim)
- (arXiv 2024.07) LookupViT: Compressing visual information to a limited number of tokens, [[Paper]](https://arxiv.org/pdf/2407.12753.pdf)
- (arXiv 2024.07) PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2407.11306.pdf)
- (arXiv 2024.07) DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention, [[Paper]](https://arxiv.org/pdf/2407.13920.pdf)
- (arXiv 2024.07) Towards Robust Vision Transformer via Masked Adaptive Ensemble, [[Paper]](https://arxiv.org/pdf/2407.15385.pdf)
- (arXiv 2024.07) Efficient Visual Transformer by Learnable Token Merging, [[Paper]](https://arxiv.org/pdf/2407.15219.pdf),[[Code]](https://github.com/Statistical-Deep-Learning/LTM)
- (arXiv 2024.07) Depth-Wise Convolutions in Vision Transformers for Efficient Training on Small Datasets, [[Paper]](https://arxiv.org/pdf/2407.19394.pdf),[[Code]](https://github.com/ZTX-100/Efficient_ViT_with_DW)
- (arXiv 2024.08) CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications, [[Paper]](https://arxiv.org/pdf/2407.03703.pdf),[[Code]](https://github.com/Tianfang-Zhang/CAS-ViT)
- (arXiv 2024.08) A Spitting Image: Modular Superpixel Tokenization in Vision Transformers, [[Paper]](https://arxiv.org/pdf/2408.07680.pdf),[[Code]](https://github.com/dsb-ifi/SPiT)
- (arXiv 2024.08) Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning, [[Paper]](https://arxiv.org/pdf/2408.06798.pdf),[[Code]](https://github.com/JieShibo/ToCom)
- (arXiv 2024.09) Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer, [[Paper]](https://arxiv.org/pdf/2408.17062.pdf)
- (arXiv 2024.09) LowFormer: Hardware Efficient Design for Convolutional Transformer Backbones, [[Paper]](https://arxiv.org/pdf/2409.03460.pdf),[[Code]](https://github.com/altair199797/LowFormer)
- (arXiv 2024.09) Brain-Inspired Stepwise Patch Merging for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2409.06963.pdf)
- (arXiv 2024.09) SparX: A Sparse Cross-Layer Connection Mechanism for Hierarchical Vision Mamba and Transformer Networks, [[Paper]](https://arxiv.org/pdf/2409.09649.pdf),[[Code]](https://github.com/LMMMEng/SparX)
- (arXiv 2024.09) ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer Acceleration, [[Paper]](https://arxiv.org/pdf/2409.09708.pdf)
- (arXiv 2024.09) Kolmogorovâ€“Arnold Transformer, [[Paper]](https://arxiv.org/pdf/2409.10594.pdf),[[Code]](https://github.com/Adamdad/kat)
- (arXiv 2024.09) Multiple-Exit Tuning: Towards Inference-Efficient Adaptation for Vision Transformer, [[Paper]](https://arxiv.org/pdf/2409.13999.pdf)
- (arXiv 2024.09) HydraViT: Stacking Heads for a Scalable ViT, [[Paper]](https://arxiv.org/pdf/2409.17978.pdf),[[Code]](https://github.com/ds-kiel/HydraViT)
- (arXiv 2024.10) MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential with Masked Autoregressive Pretraining, [[Paper]](https://arxiv.org/pdf/2410.00871.pdf)
- (arXiv 2024.10) PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners, [[Paper]](https://arxiv.org/pdf/2410.04733.pdf),[[Code]](https://github.com/yyyujintang/PredFormer)
- (arXiv 2024.10) DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention, [[Paper]](https://arxiv.org/pdf/2410.08582.pdf),[[Code]](https://github.com/maclong01/DeBiFormer)
- (arXiv 2024.10) Improving Vision Transformers by Overlapping Heads in Multi-Head Self-Attention, [[Paper]](https://arxiv.org/pdf/2410.14874.pdf)
- (arXiv 2024.10) Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation, [[Paper]](https://arxiv.org/pdf/2410.22952.pdf)
- (arXiv 2024.10) Context-Aware Token Selection and Packing for Enhanced Vision Transformer, [[Paper]](https://arxiv.org/pdf/2410.23608.pdf)
- (arXiv 2024.11) IO Transformer: Evaluating SwinV2-Based Reward Models for Computer Vision, [[Paper]](https://arxiv.org/pdf/2411.00252.pdf)
- (arXiv 2024.11) SAG-ViT: A Scale-Aware, High-Fidelity Patching Approach with Graph Attention for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2411.09420.pdf)
- (arXiv 2024.11) On the Surprising Effectiveness of Attention Transfer for Vision Transformers, [[Paper]](https://arxiv.org/pdf/2411.09702.pdf),[[Code]](https://github.com/alexlioralexli/attention-transfer)
- (arXiv 2024.11) CARE Transformer: Mobile-Friendly Linear Visual Transformer via Decoupled Dual Interaction, [[Paper]](https://arxiv.org/pdf/2411.16170.pdf)
- (arXiv 2024.11) UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision Transformers on Edge Devices, [[Paper]](https://arxiv.org/pdf/2412.02344.pdf)
- (arXiv 2024.12) Token Cropr: Faster ViTs for Quite a Few Tasks, [[Paper]](https://arxiv.org/pdf/2412.00965.pdf)
- (arXiv 2024.12) Enhancing Parameter-Efficient Fine-Tuning of Vision Transformers through Frequency-Based Adaptation, [[Paper]](https://arxiv.org/pdf/2411.19297.pdf),[[Code]](https://github.com/tsly123/FreqFiT)
- (arXiv 2024.12) Slicing Vision Transformer for Flexible Inference, [[Paper]](https://arxiv.org/pdf/2412.04786.pdf),[[Code]](https://github.com/BeSpontaneous/Scala-pytorch)
- (arXiv 2024.12) Superpixel Tokenization for Vision Transformers: Preserving Semantic Integrity in Visual Tokens, [[Paper]](https://arxiv.org/pdf/2412.04680.pdf),[[Code]](https://github.com/jangsoohyuk/SuiT)
