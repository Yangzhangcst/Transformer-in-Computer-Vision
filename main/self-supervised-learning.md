### Self-supervised Learning
- (arXiv 2021.03) Can Vision Transformers Learn without Natural Images? [[Paper]](https://arxiv.org/abs/2103.13023), [[Code]](https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/)
- (arXiv 2021.04) An Empirical Study of Training Self-Supervised Visual Transformers, [[Paper]](https://arxiv.org/abs/2104.02057)
- (arXiv 2021.04) SiT: Self-supervised vIsion Transformer, [[Paper]](https://arxiv.org/abs/2104.03602)], [[Code]](https://github.com/Sara-Ahmed/SiT)
- (arXiv 2021.04) VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text, [[Paper]](https://arxiv.org/abs/2104.11178), [[Code]](https://github.com/Sara-Ahmed/SiT)
- (arXiv 2021.04) Emerging Properties in Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2104.14294.pdf), [[Code]](https://github.com/facebookresearch/dino)
- (arXiv 2021.05) Self-Supervised Learning with Swin Transformers, [[Paper]](https://arxiv.org/pdf/2105.04553.pdf), [[Code]](https://github.com/SwinTransformer/Transformer-SSL)
- (arXiv 2021.06) MST: Masked Self-Supervised Transformer for Visual Representation, [[Paper]](https://arxiv.org/pdf/2106.05656.pdf)
- (arXiv 2021.06) Efficient Self-supervised Vision Transformers for Representation Learning, [[Paper]](https://arxiv.org/pdf/2106.09785.pdf)
- (arXiv 2021.09) Localizing Objects with Self-Supervised Transformers and no Labels, [[Paper]](https://arxiv.org/pdf/2109.14279.pdf)
- (arXiv 2021.10) Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning, [[Paper]](https://arxiv.org/pdf/2110.05340.pdf), [[Code]](https://github.com/ChongjianGE/CARE)
- (arXiv 2022.01) RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training, [[Paper]](https://arxiv.org/pdf/2201.06857.pdf), [[Code]](https://github.com/ChongjianGE/CARE)
- (arXiv 2022.02) Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut, [[Paper]](https://arxiv.org/pdf/2202.11539.pdf), [[Project]](https://www.m-psi.fr/Papers/TokenCut2022/)
- (arXiv 2022.03) Mugs: A Multi-Granular Self-Supervised Learning Framework, [[Paper]](https://arxiv.org/pdf/2203.14415.pdf), [[Code]](https://github.com/sail-sg/mugs)
- (arXiv 2022.04) A Transformer-Based Contrastive Learning Approach for Few-Shot Sign Language Recognition, [[Paper]](https://arxiv.org/pdf/2204.02803.pdf)
- (arXiv 2022.04) DILEMMA: Self-Supervised Shape and Texture Learning with Transformers, [[Paper]](https://arxiv.org/pdf/2204.04788.pdf)
- (arXiv 2022.04) Self-supervised Vision Transformers for Joint SAR-optical Representation Learning, [[Paper]](https://arxiv.org/pdf/2204.05381.pdf)
- (arXiv 2022.05) UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog, [[Paper]](https://arxiv.org/pdf/2205.00423.pdf)
- (arXiv 2022.05) Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality, [[Paper]](https://arxiv.org/pdf/2205.10063.pdf), [[Code]](https://github.com/implus/UM-MAE)
- (arXiv 2022.05) Self-Supervised Pre-training of Vision Transformers for Dense Prediction Tasks, [[Paper]](https://arxiv.org/pdf/2205.15173.pdf), [[Code]](https://github.com/implus/UM-MAE)
- (arXiv 2022.05) A Closer Look at Self-supervised Lightweight Vision Transformers, [[Paper]](https://arxiv.org/pdf/2205.14443.pdf)
- (arXiv 2022.06) Where are my Neighbors? Exploiting Patches Relations in Self-Supervised Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.00481.pdf), [[Code]](https://github.com/guglielmocamporese/relvit)
- (arXiv 2022.06) Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning, [[Paper]](https://arxiv.org/pdf/2206.02647.pdf), [[Code]](https://github.com/mahmoodlab/HIPT)
- (arXiv 2022.06) Exploring Feature Self-relation for Self-supervised Transformer, [[Paper]](https://arxiv.org/pdf/2206.05184.pdf)
- (arXiv 2022.06) Position Labels for Self-Supervised Vision Transformer, [[Paper]](https://arxiv.org/pdf/2206.04981.pdf)
- (arXiv 2022.06) Adapting Self-Supervised Vision Transformers by Probing Attention-Conditioned Masking Consistency, [[Paper]](https://arxiv.org/pdf/2206.08222.pdf), [[Code]](https://github.com/virajprabhu/PACMAC)
- (arXiv 2022.06) Patch-level Representation Learning for Self-supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2206.07990.pdf), [[Code]](https://github.com/alinlab/SelfPatch)
- (arXiv 2022.07) Hierarchically Self-Supervised Transformer for Human Skeleton Representation Learning, [[Paper]](https://arxiv.org/pdf/2207.09644.pdf), [[Code]](https://github.com/yuxiaochen1103/Hi-TRS)
- (arXiv 2022.08) Self-Supervised Vision Transformers for Malware Detection, [[Paper]](https://arxiv.org/pdf/2208.07049.pdf)
- (arXiv 2022.09) Prior Knowledge-Guided Attention in Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2209.03745.pdf)
- (arXiv 2022.10) Attention Distillation: self-supervised vision transformer students need more guidance, [[Paper]](https://arxiv.org/pdf/2210.00944.pdf), [[Code]](https://github.com/wangkai930418/attndistill)
- (arXiv 2022.10) Histopathological Image Classification based on Self-Supervised Vision Transformer and Weak Labels, [[Paper]](https://arxiv.org/pdf/2210.09021.pdf), [[Code]](https://github.com/gokberkgul/selflearning-transformer-mil)
- (arXiv 2022.10) Learning Self-Regularized Adversarial Views for Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.08458.pdf), [[Code]](https://github.com/Trent-tangtao/AutoView)
- (arXiv 2022.10) SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic Retinopathy Grading, [[Paper]](https://arxiv.org/pdf/2210.10969.pdf), [[Code]](https://github.com/YijinHuang/SSiT)
- (arXiv 2022.10) PatchRot: A Self-Supervised Technique for Training Vision Transformers, [[Paper]](https://arxiv.org/pdf/2210.15722.pdf), [[Code]](https://github.com/s-chh/patchrot)
- (arXiv 2022.10) Foreign Object Debris Detection for Airport Pavement Images based on Self-supervised Localization and Vision Transformer, [[Paper]](https://arxiv.org/pdf/2210.16901.pdf)
- (arXiv 2022.12) Location-Aware Self-Supervised Transformers, [[Paper]](https://arxiv.org/pdf/2212.02400.pdf), [[Code]](https://github.com/google-research/scenic/tree/main/scenic/projects/loca)
- (arXiv 2023.02) Real Estate Property Valuation using Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2302.00117.pdf)
- (arXiv 2023.02) Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations, [[Paper]](https://arxiv.org/pdf/2302.14138.pdf), [[Code]](https://github.com/VITA-Group/layerGraftedPretraining_ICLR23.git)
- (arXiv 2023.03) ST-KeyS: Self-Supervised Transformer for Keyword Spotting in Historical Handwritten Documents, [[Paper]](https://arxiv.org/pdf/2303.03127.pdf)
- (arXiv 2023.03) AdPE: Adversarial Positional Embeddings for Pretraining Vision Transformers via MAE+, [[Paper]](https://arxiv.org/pdf/2303.07598.pdf), [[Code]](https://github.com/maple-research-lab/AdPE)
- (arXiv 2023.03) Contrastive Transformer: Contrastive Learning Scheme with Transformer innate Patches, [[Paper]](https://arxiv.org/pdf/2303.14806.pdf)
- (arXiv 2023.04) Token Boosting for Robust Self-Supervised Visual Transformer Pre-training, [[Paper]](https://arxiv.org/pdf/2304.04175.pdf)
- (arXiv 2023.04) MOST: Multiple Object localization with Self-supervised Transformers for object discovery, [[Paper]](https://arxiv.org/pdf/2304.05387.pdf)
- (arXiv 2023.05) LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input, [[Paper]](https://arxiv.org/pdf/2304.14765.pdf)
- (arXiv 2023.05) What Do Self-Supervised Vision Transformers Learn, [[Paper]](https://arxiv.org/pdf/2305.00729.pdf), [[Code]](https://github.com/naver-ai/cl-vs-mim)
- (arXiv 2023.06) Improving Visual Prompt Tuning for Self-supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2306.05067.pdf), [[Code]](https://github.com/ryongithub/GatedPromptTuning)
- (arXiv 2023.06) DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency, [[Paper]](https://arxiv.org/pdf/2306.04654.pdf)
- (arXiv 2023.07) Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation, [[Paper]](https://arxiv.org/pdf/2307.03407.pdf)
- (arXiv 2023.07) Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments, [[Paper]](https://arxiv.org/pdf/2307.09361.pdf)
- (arXiv 2023.08) Emergence of Segmentation with Minimalistic White-Box Transformers, [[Paper]](https://arxiv.org/pdf/2308.16271.pdf), [[Code]](https://ma-lab-berkeley.github.io/CRATE)
- (arXiv 2023.10) Limited Data, Unlimited Potential:A Study on ViTs Augmented by Masked Autoencoders, [[Paper]](https://arxiv.org/pdf/2310.20704.pdf), [[Code]](https://github.com/dominickrei/Limited-data-vits)
- (arXiv 2023.11) LISBET: a self-supervised Transformer model for the automatic segmentation of social behavior motifs, [[Paper]](https://arxiv.org/pdf/2311.04069.pdf), [[Code]](https://github.com/dominickrei/Limited-data-vits)
- (arXiv 2024.01) Analyzing Local Representations of Self-supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2401.00463.pdf)
- (arXiv 2024.04) Parameter Efficient Fine-tuning of Self-supervised ViTs without Catastrophic Forgetting, [[Paper]](https://arxiv.org/pdf/2404.17245.pdf), [[Code]](https://github.com/rezaakb/peft-vit)
- (arXiv 2024.05) S3Former: Self-supervised High-resolution Transformer for Solar PV Profiling, [[Paper]](https://arxiv.org/pdf/2405.04489.pdf)
- (arXiv 2024.05) Visualizing the loss landscape of Self-supervised Vision Transformer, [[Paper]](https://arxiv.org/pdf/2405.18042.pdf)
- (arXiv 2024.06) Self-Supervised Vision Transformer for Enhanced Virtual Clothes Try-On, [[Paper]](https://arxiv.org/pdf/2406.10539.pdf)
- (arXiv 2024.07) Self-supervised Vision Transformer are Scalable Generative Models for Domain Generalization, [[Paper]](https://arxiv.org/pdf/2407.02900.pdf), [[Code]](https://github.com/sdoerrich97/vits-are-generative-models)
- (arXiv 2024.07) Multi-Label Plant Species Classification with Self-Supervised Vision Transformers, [[Paper]](https://arxiv.org/pdf/2407.06298.pdf), [[Code]](https://github.com/dsgt-kaggle-clef/plantclef-2024)
- (arXiv 2024.07) TreeSBA: Tree-Transformer for Self-Supervised Sequential Brick Assembly, [[Paper]](https://arxiv.org/pdf/2407.15648.pdf), [[Code]](https://dreamguo.github.io/projects/TreeSBA/)
- (arXiv 2024.07) Self-supervised transformer-based pre-training method with General Plant Infection dataset, [[Paper]](https://arxiv.org/pdf/2407.14911.pdf), [[Code]](https://github.com/WASSER2545/GPID-22)
- (arXiv 2025.06) Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features, [[Paper]](https://arxiv.org/pdf/2506.21046.pdf), [[Code]](https://github.com/spencerwooo/dSVA)
- (arXiv 2025.07) Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding,  [[Paper]](https://arxiv.org/pdf/2507.09513.pdf)
- (arXiv 2025.09) Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks,  [[Paper]](https://arxiv.org/pdf/2509.15272.pdf)
