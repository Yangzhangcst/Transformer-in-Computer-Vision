### LLM/LVM
- (arXiv 2023.07) INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers, [[Paper]](https://arxiv.org/pdf/2307.03712.pdf)
- (arXiv 2023.11) NExT-Chat: An LMM for Chat, Detection and Segmentation, [[Paper]](https://arxiv.org/pdf/2311.04498.pdf), [[Code]](https://next-chatv.github.io/)
- (arXiv 2023.11) u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model, [[Paper]](https://arxiv.org/pdf/2311.05348.pdf)
- (arXiv 2023.11) Towards Open-Ended Visual Recognition with Large Language Model, [[Paper]](https://arxiv.org/pdf/2311.08400.pdf), [[Code]](https://github.com/bytedance/OmniScient-Model)
- (arXiv 2023.11) Stable Segment Anything Model, [[Paper]](https://arxiv.org/pdf/2311.15776.pdf), [[Code]](https://github.com/fanq15/Stable-SAM)
- (arXiv 2023.11) Adapter is All You Need for Tuning Visual Tasks, [[Paper]](https://arxiv.org/pdf/2311.15010.pdf), [[Code]](https://github.com/Leiyi-Hu/mona)
- (arXiv 2023.11) LLaFS: When Large-Language Models Meet Few-Shot Segmentation, [[Paper]](https://arxiv.org/pdf/2311.16926.pdf), [[Code]](https://github.com/lanyunzhu99/LLaFS)
- (arXiv 2023.11) Efficient In-Context Learning in Vision-Language Models for Egocentric Videos, [[Paper]](https://arxiv.org/pdf/2311.17041.pdf), [[Code]](https://github.com/yukw777/EILEV)
- (arXiv 2023.11) Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model, [[Paper]](https://arxiv.org/pdf/2311.17112.pdf)
- (arXiv 2023.11) PoseGPT: Chatting about 3D Human Pose, [[Paper]](https://arxiv.org/pdf/2311.18836.pdf), [[Code]](https://yfeng95.github.io/posegpt)
- (arXiv 2023.11) InstructSeq: Unifying Vision Tasks with Instruction-conditioned Multi-modal Sequence Generation, [[Paper]](https://arxiv.org/pdf/2311.18835.pdf), [[Code]](https://github.com/rongyaofang/InstructSeq)
- (arXiv 2023.11) Semantic-Aware Frame-Event Fusion based Pattern Recognition via Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2311.18592.pdf), [[Code]](https://github.com/Event-AHU/SAFE_LargeVLM)
- (arXiv 2023.11) Contrastive Vision-Language Alignment Makes Efficient Instruction Learner, [[Paper]](https://arxiv.org/pdf/2311.17945.pdf), [[Code]](https://github.com/lizhaoliu-Lec/CG-VLM)
- (arXiv 2023.12) Bootstrapping SparseFormers from Vision Foundation Models, [[Paper]](https://arxiv.org/pdf/2312.01987.pdf), [[Code]](https://github.com/showlab/sparseformer)
- (arXiv 2023.12) IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks, [[Paper]](https://arxiv.org/pdf/2312.01771.pdf), [[Code]](https://jerryxu.net/IMProv)
- (arXiv 2023.12) Segment and Caption Anything, [[Paper]](https://arxiv.org/pdf/2312.00869.pdf), [[Code]](https://xk-huang.github.io/segment-caption-anything/)
- (arXiv 2023.12) EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything, [[Paper]](https://arxiv.org/pdf/2312.00863.pdf)
- (arXiv 2023.12) Segment Any 3D Gaussians, [[Paper]](https://arxiv.org/pdf/2312.00860.pdf), [[Code]](https://github.com/Jumpat/SegAnyGAussians)
- (arXiv 2023.12) Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts, [[Paper]](https://arxiv.org/pdf/2312.00968.pdf)
- (arXiv 2023.12) PixelLM: Pixel Reasoning with Large Multimodal Model, [[Paper]](https://arxiv.org/pdf/2312.02228.pdf), [[Code]](https://github.com/MaverickRen/PixelLM)
- (arXiv 2023.12) Foundation Model Assisted Weakly Supervised Semantic Segmentation, [[Paper]](https://arxiv.org/pdf/2312.03585.pdf)
- (arXiv 2023.12) AI-SAM: Automatic and Interactive Segment Anything Model, [[Paper]](https://arxiv.org/pdf/2312.03119.pdf), [[Code]](https://github.com/ymp5078/AI-SAM)
- (arXiv 2023.12) MobileSAMv2: Faster Segment Anything to Everything, [[Paper]](https://arxiv.org/abs/2312.09579),[[Code]](https://github.com/ChaoningZhang/MobileSAM)
- (arXiv 2023.12) MobileVLM : A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices, [[Paper]](https://arxiv.org/abs/2312.16886),[[Code]](https://github.com/Meituan-AutoML/MobileVLM)
- (arXiv 2024.01) One for All: Toward Unified Foundation Models for Earth Vision, [[Paper]](https://arxiv.org/pdf/2401.07527.pdf)
- (arXiv 2024.01) RAP-SAM: Towards Real-Time All-Purpose Segment Anything, [[Paper]](https://arxiv.org/pdf/2401.10228.pdf), [[Code]](https://github.com/xushilin1/RAP-SAM/)
- (arXiv 2024.02) MobileVLM V2: Faster and Stronger Baseline for Vision Language Model, [[Paper]](https://arxiv.org/pdf/2402.03766.pdf), [[Code]](https://github.com/Meituan-AutoML/MobileVLM)
- (arXiv 2024.02) Data-efficient Large Vision Models through Sequential Autoregression, [[Paper]](https://arxiv.org/pdf/2402.04841.pdf), [[Code]](https://github.com/ggjy/DeLVM)
- (arXiv 2024.02) EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss, [[Paper]](https://arxiv.org/pdf/2402.05008.pdf), [[Code]](https://github.com/mit-han-lab/efficientvit)
- (arXiv 2024.02) Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models, [[Paper]](https://arxiv.org/pdf/2402.08473.pdf)
- (arXiv 2024.02) PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter, [[Paper]](https://arxiv.org/pdf/2402.10896.pdf)
- (arXiv 2024.02) GROUNDHOG : Grounding Large Language Models to Holistic Segmentation, [[Paper]](https://arxiv.org/pdf/2402.16846.pdf)
- (arXiv 2024.03) VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model, [[Paper]](https://arxiv.org/pdf/2403.05346.pdf)
- (arXiv 2024.03) Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models, [[Paper]](https://arxiv.org/pdf/2403.09635.pdf), [[Code]](https://github.com/akhilkedia/TranformersGetStable)
- (arXiv 2024.04) Adapting LLaMA Decoder to Vision Transformer, [[Paper]](https://arxiv.org/pdf/2404.06773.pdf), [[Code]](https://github.com/techmonsterwang/iLLaMA)
- (arXiv 2024.04) Surgical-DeSAM: Decoupling SAM for Instrument Segmentation in Robotic Surgery, [[Paper]](https://arxiv.org/pdf/2404.14040.pdf), [[Code]](https://github.com/YuyangSheng/Surgical-DeSAM)
- (arXiv 2024.04) Dense Connector for MLLMs, [[Paper]](https://arxiv.org/pdf/2405.13800.pdf), [[Code]](https://github.com/HJYao00/DenseConnector)
- (arXiv 2024.05) Do Vision-Language Transformers Exhibit Visual Commonsense? An Empirical Study of VCR, [[Paper]](https://arxiv.org/pdf/2405.16934.pdf)
- (arXiv 2024.05) Matryoshka Query Transformer for Large Vision-Language Models, [[Paper]](https://arxiv.org/pdf/2405.19315.pdf), [[Code]](https://github.com/gordonhu608/MQT-LLaVA)
- (arXiv 2024.07) A Single Transformer for Scalable Vision-Language Modeling, [[Paper]](https://arxiv.org/pdf/2407.06438.pdf), [[Code]](https://github.com/Yangyi-Chen/SOLO)
- (arXiv 2024.07) X-Former: Unifying Contrastive and Reconstruction Learning for MLLMs, [[Paper]](https://arxiv.org/pdf/2407.13851.pdf)
- (arXiv 2024.07) EVLM: An Efficient Vision-Language Model for Visual Understanding, [[Paper]](https://arxiv.org/pdf/2407.14177.pdf)
- (arXiv 2024.07) Hierarchical Generation for Coherent Long Visual Sequences, [[Paper]](https://arxiv.org/pdf/2407.16655.pdf), [[Code]](https://aim-uofa.github.io/MovieDreamer/)
- (arXiv 2024.08) ARPA: A Novel Hybrid Model for Advancing Visual Word Disambiguation Using Large Language Models and Transformers, [[Paper]](https://arxiv.org/pdf/2408.0640.pdf)
- (arXiv 2024.09) VLM-KD: Knowledge Distillation from VLM for Long-Tail Visual Recognition, [[Paper]](https://arxiv.org/pdf/2408.16930.pdf)
- (arXiv 2024.09) Shaking Up VLMs: Comparing Transformers and Structured State Space Models for Vision & Language Modeling, [[Paper]](https://arxiv.org/pdf/2409.05395.pdf), [[Code]](https://github.com/gpantaz/vl_mamba)
- (arXiv 2024.09) TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models, [[Paper]](https://arxiv.org/pdf/2410.05239.pdf), [[Code]](https://github.com/naamiinepal/tunevlseg)
- (arXiv 2024.10) OMCAT: Omni Context Aware Transformer, [[Paper]](https://arxiv.org/pdf/2410.12109.pdf), [[Code]](https://om-cat.github.io/)
- (arXiv 2024.11) MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding, [[Paper]](https://arxiv.org/pdf/2411.17762.pdf)
